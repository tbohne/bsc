\section{Stacking-Probleme}
\label{sec:stacking_problems}

In diesem Kapitel werden die einleitend allgemein und aus praktischer Perspektive beschriebenen Stacking-Probleme
formal definiert. Diese formale Definition stellt die Grundlage der in den Folgekapiteln vorgestellten Lösungsmethoden dar.

Eintreffende Items müssen sogenannten Stacks in einer Weise zugeordnet werden, welche bestimmte Nebenbedingungen respektiert.
Diese Nebenbedingungen sorgen zum Beispiel dafür, dass nicht jedes Item auf jedem anderen Item platziert werden
darf. Außerdem existieren Items, welche auf bestimmte Positionen beschränkt sind, weil sie zum Beispiel eine Energiequelle benötigen. Es wird dabei angenommen, dass die im Folgenden als Storage-Area bezeichnete Lagerfläche in fixierten
Stacks organisiert ist, welche eine limitierte gemeinsame Höhe besitzen, die im Folgenden als Stack-Kapazität bezeichnet wird
und der Maximalanzahl an Items, welche pro Stack aufeinander gestapelt werden dürfen, entspricht.
Die Items werden dementsprechend in eine Storage-Area verladen, welche aus Stacks mit einer festen Position besteht,
d.h. es ist nicht möglich, die Stacks zu positionieren, sondern lediglich den Items eine Position innerhalb eines solchen
Stacks zuweisen. In dieser Arbeit wird dabei ausschließlich der Loading-Prozess betrachtet, währenddem keine
Item-Entnahme stattfindet. Dies entspricht im Wesentlichen der von \citet{Bruns2015} betrachteten Problemstellung.

In der Literatur existieren diverse Ansätze für das Konzept der Storage-Area.
\citet{Jaehn2013} betrachtet beispielsweise eine Lagerfläche, welche nicht aus fixierten Stacks besteht,
sondern sich in parallele \textquote{Lanes}
aufteilt, in welchen ein Stack prinzipiell an jeder Position eröffnet werden kann, sofern bestimmte Sicherheitsabstände eingehalten werden.
Das im Folgenden verwendete Konzept der Storage-Area mit fixierten Stacks, welches auf viele praktische
Szenarien anwendbar ist, basiert auf der Beschreibung von \citet{Lehnfeld2014}.

Abbildung \ref{fig:storage_area} zeigt den Aufbau der Storage-Area bestehend aus $m$ fixierten
Stacks, welche jeweils $b$ Level enthalten. Jedes Level innerhalb eines Stacks entspricht dabei einer Position,
welcher ein Item zugeordnet werden kann.

\begin{figure}[H]
\centering

\begin{subfigure}[b]{\textwidth}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\cline{1-5}
$\boldsymbol{S_1}$ & $\boldsymbol{S_2}$ & $\boldsymbol{S_3}$ & ... & $\boldsymbol{S_m}$\\ \cline{1-5}
\end{tabular}}
\caption{\textsc{Draufsicht}}
\label{fig:topview}
\end{subfigure}
\par\bigskip
\begin{subfigure}[b]{\textwidth}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|}
\cline{2-6}
$\boldsymbol{L_b}$ & $pos_{1, b}$ & $pos_{2, b}$ & $pos_{3, b}$ & ... & $pos_{m, b}$ \\ \cline{2-6}
... & ... & ... & ... & ... & ...\\ \cline{2-6}
$\boldsymbol{L_2}$ & $pos_{1, 2}$ & $pos_{2, 2}$ & $pos_{3, 2}$ & ... & $pos_{m, 2}$\\ \cline{2-6}
$\boldsymbol{L_1}$ & $pos_{1, 1}$ & $pos_{2, 1}$ & $pos_{3, 1}$ & ... & $pos_{m, 1}$\\ \cline{2-6}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{$\boldsymbol{S_1}$} & \multicolumn{1}{c}{$\boldsymbol{S_2}$}
& \multicolumn{1}{c}{$\boldsymbol{S_3}$} & \multicolumn{1}{c}{...} & \multicolumn{1}{c}{$\boldsymbol{S_m}$} \\
\end{tabular}}
\caption{\textsc{Seitenansicht}}
\label{fig:sideview}
\end{subfigure}

\caption{\textsc{Aufbau der Storage-Area}.}
\label{fig:storage_area}
\end{figure}

\pagebreak

Obwohl die Storage-Area in der Realität häufig mehr als eine Reihe von Stacks beinhaltet und die in
Abb. \ref{fig:topview} dargestellte Draufsicht dementsprechend dreidimensional sein kann,
wird in den Folgekapiteln stets von der in Abb. \ref{fig:storage_area} dargestellten Reihe von Stacks ausgegangen.
Lediglich für die in Abschnitt \ref{sec:transport_costs} beschriebene Ermittlung der Transportkosten ist es entscheidend,
an welcher Stelle innerhalb der Storage-Area sich ein Stack befindet. Nachdem die Transportkosten basierend auf der
Stack-Position für jede Item-Stack-Kombination ermittelt wurden, kann für die eigentliche Lösung des Problems von
der in Abb. \ref{fig:storage_area} dargestellten zweidimensionalen Reihe von Stacks ausgegangen werden.
Es ist jedoch auch bei der Betrachtung der Transportkosten eine zweidimensionale Struktur ausreichend, denn dort sind
die Level innerhalb der Stacks irrelevant, weil die Zuweisung eines Items zu den unterschiedlichen Leveln
desselben Stacks stets dieselben Kosten verursacht. Daher ist dort einzig die von oben betrachtete zweidimensionale
Position der Stacks von Bedeutung.

Das Ziel von Stacking-Problemen ist es, jedes ankommende Item einer zulässigen Position in einem
Stack zuzuordnen, sodass eine gegebene Zielfunktion optimiert wird. Es gibt eine Vielzahl praktisch relevanter Zielfunktionen,
in dieser Arbeit geht es allerdings ausschließlich um die Minimierung der Transportkosten, welche bei der Verladung der Items von ihrer Originalposition zur zugewiesenen Stackposition entstehen. Wobei das primäre Ziel im Gegensatz zu einer bedingungslosen Kostenminimierung darin besteht, für möglichst viele Probleminstanzen eine zulässige Lösung zu finden. Im Detail wird dies in
Abschnitt \ref{sec:objective} erläutert.

\subsection{Formale Definition}
\label{sec:formal_definition}

Die in Abb. \ref{fig:parameters} dargestellte Liste der Parameter, welche im weiteren Verlauf der Arbeit verwendet
werden, um Stacking-Probleme zu definieren, enthält zusätzlich zur bereits erwähnten Anzahl der zur Verfügung stehenden
Stacks $m$ und der Stack-Kapazität $b$ die Menge der eintreffenden Items $I$, deren Kardinalität $n$, welche der Anzahl
der zuzuweisenden Items entspricht sowie die Menge der zur Verfügung stehenden Stacks $Q$. Diese und auch im Folgenden
eingeführte Notationen, welche mit Konzepten aus der Arbeit von \citet{Bruns2015} übereinstimmen,
werden in dieser Arbeit übernommen.

\begin{figure}[H]
\centering
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{ | l | l |}
    \hline
    \textbf{Parameter} & \textbf{Semantik} \\ \hline
    $n$ & Anzahl der Items \\ \hline
    $m$ & Anzahl der Stacks \\ \hline
    $b$ & Stack-Kapazität \\ \hline
    $I$ & Menge der Items $ I := \{1, 2, ..., n\}$ \\ \hline
    $Q$ & Menge der Stacks $ Q := \{1, 2, ..., m\}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zur Definition verwendete Parameter}.}
\label{fig:parameters}
\end{figure}
In der Regel gilt $m < n$, d.h. es müssen Items gestapelt werden.
Außerdem muss die Annahme $n \leq bm$ gelten, denn sonst ist die Instanz des Problems unzulässig,
weil die Anzahl der zuzuweisenden Items die Anzahl der verfügbaren Positionen in der Storage-Area übersteigt.

\pagebreak

Für jedes Item $i \in I$ ist dessen Originalposition $O_i$ auf dem Fahrzeug, mit welchem es geliefert wird,
in $x$- und $y$-Koordinaten gegeben. Ebenso ist für jeden Stack $q \in Q$ dessen fixierte Position $F_q$ in der von oben betrachteten Storage-Area in zweidimensionalen Koordinaten gegeben.

Neben den in Abb. \ref{fig:parameters} aufgeführten Parametern existieren Nebenbedingungen, welche beim Lösen der Stacking-Probleme
berücksichtigt werden müssen. Zum einen die in Abschnitt \ref{sec:stacking_restrictions} vorgestellten Stacking-Constraints $s_{ij}$, bei denen es sich um Restriktionen handelt, welche angeben, ob ein Item $i \in I$ direkt auf einem anderen Item $j \in I$ platziert werden darf. Dies ist der Fall, wenn $s_{ij} = 1$ gilt.
Zum anderen die Placement-Constraints $t_{iq}$, welche in Kapitel \ref{sec:placement_restrictions} thematisiert werden und bei denen es darum geht, dass manche Items Restriktionen bezüglich ihrer Positionierung besitzen. Ein Item $i \in I$ ist nur dann mit einem Stack $q \in Q$ kompatibel, wenn $t_{iq} = 1$ gilt.

Items, welche in einem Stack platziert werden, sind durch ein Tupel $(i_k, ..., i_1)$ definiert, wobei
$i_\lambda$ das Item auf Level $\lambda$ beschreibt. $\lambda = 1$ entspricht dabei dem
Ground-Level\footnote{Unterste Position (Level) innerhalb eines Stacks.}.
Ein solches Tupel ist bei \citet{Bruns2015} zulässig, wenn $k \leq b$ und $s_{i_{\lambda + 1} i_\lambda} = 1
\thinspace \forall \thinspace \lambda = 1, ... , k - 1$ gilt.
Demzufolge ist ein Tupel dann zulässig, wenn die Stack-Kapazität eingehalten wird und sämtliche Items,
welche aufeinander gestapelt sind, nicht den Stacking-Constraints widersprechen. Darüber hinaus müssen bei den in dieser
Arbeit betrachteten Problemstellungen für die Zulässigkeit eines Tupels sämtliche Item-Stack-Zuweisungen
den Placement-Constraints genügen, d.h. für alle $i_\lambda$ eines Tupels bzw. Stacks $q \in Q$ muss
$t_{i_\lambda q} = 1$ gelten. Jedes Item muss demgemäß mit dem Stack, welchem es zugewiesen wird, kompatibel sein.
\newline

\textbf{Formulierung des Problems}

Gegeben sei eine Menge $I = \{1, ..., n\}$ von Items und eine Menge $Q = \{1, ..., m\}$ von Stacks der Kapazität $b$.
Außerdem seien Stacking-Constraints $s_{ij}$ und Placement-Constraints $t_{iq}$ gegeben.
Das Ziel ist nun, jedes Item $i \in I$ genau einem Stack $q \in Q$ zuzuweisen, wobei die Stacking-Constraints $s_{ij}$,
die Placement-Constraints $t_{iq}$ und die Stack-Kapazität $b$ respektiert werden und gegebenenfalls eine Zielfunktion
optimiert wird.

\vfill

\pagebreak

\subsection{Transportkosten}
\label{sec:transport_costs}
Die Transportkosten beziehen sich auf jene Kosten, welche bei der Verladung der Items in die Storage-Area entstehen
und sind praktisch zum Beispiel durch Kranbetriebskosten, Wartezeiten oder Arbeitszeiten motiviert.\newline
Jeder Stack $q \in Q$ besitzt eine fixierte Position $F_q$ in der Storage-Area.
Außerdem besitzt jedes Item $i \in I$ eine gegebene Originalposition $O_i$ auf dem Fahrzeug, mit welchem es geliefert wird.
Die Kosten der Zuweisung eines Items zu einem Stack ergeben sich aus der Manhattan-Distanz zwischen Itemposition $O_i$ und
zugewiesener Stack-Position $F_q$, da diese der Kranbewegung entspricht.

In Abb. \ref{fig:costs} ist ein Beispielszenario visualisiert, in welchem die Items auf Zügen geliefert und durch Kräne auf die ihnen zugewiesenen Stackpositionen in der Storage-Area verladen werden.
Jedes Item besitzt in diesem Szenario eine gegebene Position auf einem der in der Darstellung als \textquote{Freight Train} bezeichneten Güterzüge. Außerdem besitzt jeder Stack, also jede Zelle der als Grid dargestellten Storage-Area,
eine feste Position. Dieses Grid entspricht der in Abschnitt \ref{sec:stacking_problems} einleitend erwähnten
von oben betrachteten Anordnung der Stacks in der Storage-Area.
Die in der Darstellung abgebildeten \textquote{Gantry Cranes}, zu Deutsch
Portalkräne\footnote{Kräne, welche die Be- und Entladefläche wie ein Portal umschließen und häufig an
Containerterminals zum Einsatz kommen.}, verladen die Items von den Güterzügen in die
Storage-Area und können dabei in horizontaler und vertikaler Richtung bewegt werden. Dementsprechend bietet sich die Manhattan-Metrik zur Definition der Distanzen zwischen Items und Stacks und somit zur Bestimmung der Transportkosten an.

\begin{figure}[H]
\includegraphics[width=0.9\textwidth]{img/costs.png}
\caption{Schematische Darstellung eines Bahn-Terminals \cite{Briskorn2018}.}
\label{fig:costs}
\end{figure}
Die Transportkosten werden in einer Matrix $C = (c_{iq})_{n \times m}$ kodiert,
wobei $c_{iq} \coloneqq d_{man}(O_i, F_q)$. D.h. der Eintrag an der Stelle $c_{iq}$ entspricht
den Kosten für den Transport des Items $i$ in den Stack $q$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Nebenbedingungen}
\label{sec:constraints}

Bei der Lösung der in dieser Arbeit betrachteten Problemstellungen müssen, wie einleitend erwähnt,
neben der Kapazität der Stacks zwei weitere Kategorien von Nebenbedingungen berücksichtigt werden, welche in diesem Abschnitt
präzisiert werden. Zunächst geht es in Abschnitt \ref{sec:stacking_restrictions} um die Stacking-Constraints
und anschließend in Abschnitt \ref{sec:placement_restrictions} um die Placement-Constraints.

\subsubsection{Stacking-Constraints}
\label{sec:stacking_restrictions}

Es gibt in der Praxis vielfältige Gründe dafür, dass nicht jedes Item auf jedes andere Item gestapelt werden darf.
Aus diesen resultieren bestimmte Restriktionen, welche auch Teil der von \citet{Bruns2015} betrachteten Problemstellungen
sind und z.B. wie folgt lauten:
\begin{itemize}
  \item schwerere Items dürfen nicht auf leichteren Items platziert werden
  \item größere Items dürfen nicht auf kleineren Items platziert werden
  \item Items best. Materialien oder Zielorte dürfen nicht aufeinander gestapelt werden
\end{itemize}

All jene Restriktionen, welche unter der Bezeichnung Stacking-Constraints zusammengefasst werden,
werden in einer Binärmatrix $S = (s_{ij})_{n \times n}$ kodiert, wobei:
\[
    s_{ij} =
\begin{cases}
    1, & \text{wenn Item $i$ direkt auf Item $j$ gestapelt werden darf }\\
    0, & \text{sonst}
\end{cases}
\]

Diese Matrix lässt sich, wie in Abb. \ref{fig:matrix_to_graph} beispielhaft dargestellt, auch durch einen
gerichteten \textquote{Stacking-Graphen} $G^S = (I, A^S)$ repräsentieren, welcher eine Kante $(i, j) \in A^S$ enthält,
wenn $s_{ij} = 1$ gilt.
\begin{figure}[H]
  \begin{subfigure}[b]{0.5\textwidth}
  \centering
    $S =$
    $\left(
    \begin{array}{rrrr}
    0 & 1 & 1 & 0 \\
    1 & 0 & 1 & 0 \\
    1 & 0 & 0 & 0 \\
    1 & 1 & 1 & 0 \\
    \end{array} \right) $
    \caption{\textsc{Stacking-Matrix}}
    \label{fig:constraint_matrix}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
  \centering
    \begin{tikzpicture}[->, scale=0.65, transform shape, node distance=2.5cm]
    \node[state] (A) {0};
    \node[state] (B) [right of=A] {1};
    \node[state] (C) [below of=A] {2};
    \node[state] (D) [right of=C] {3};

    \path (A) edge node {} (B)
          (A) edge node {} (C)

          (B) edge [bend right] node {} (A)
          (B) edge [bend left=10] node {} (C)

          (C) edge [bend left] node {} (A)

          (D) edge [bend left=10] node {} (A)
          (D) edge node {} (B)
          (D) edge node {} (C);
  \end{tikzpicture}
    \caption{\textsc{Stacking-Graph}}
    \label{fig:resulting_graph}
  \end{subfigure}
  \caption{\textsc{Repräsentation der Stacking-Constraints.}}
  \label{fig:matrix_to_graph}
\end{figure}
In dieser Arbeit werden ausschließlich transitive Stacking-Constraints betrachtet, d.h.
wenn $(i, j), (j, h) \in A^S$, dann gilt ebenfalls $(i, h) \in A^S$.
Diese Einschränkung ist auch in der Praxis zumeist gegeben, da die Stacking-Constraints häufig auf Eigenschaften wie dem Gewicht oder der Länge der Items basieren. Restriktionen dieser Art haben die besondere Eigenschaft, dass alle Items vergleichbar sind, d.h. für alle $i \neq j$ gilt $s_{ij} = 1$ oder $s_{ji} = 1$, was zur Folge hat,
dass eine totale Ordnung auf allen Items definiert wird (vgl. \citet{Bruns2015}).

\pagebreak

Die Stacking-Constraints, welche in dieser Arbeit primär betrachtet werden, definieren jeweils partielle
Ordnungen auf der Menge der Items, weil die Relation zweier Items statt auf nur einer Eigenschaft,
z.B. auf dem Gewicht und der Länge der Items basiert und somit die totale Vergleichbarkeit,
also $\forall i, j \in I : i \leq j \lor j \leq i$, welche Voraussetzung einer totalen Ordnung ist, nicht gilt.
In Kapitel \ref{sec:test_data} wird das ebenfalls praktisch motivierte Verfahren, mit welchem die
Stacking-Constraints generiert werden, genau beschrieben.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Placement-Constraints}
\label{sec:placement_restrictions}

Die unter der Bezeichnung Placement-Constraints zusammengefassten Einschränkungen bezüglich der Positionierung eines
Items resultieren z.B. aus der Länge oder dem Gewicht der Items und den entsprechenden Eigenschaften der Stacks. Es gibt allerdings auch Items mit speziellen Anforderungen, beispielsweise Kühlcontainer\footnote{Container, welche über eine elektrisch betriebene Kühleinheit verfügen.}, welche eine Energiequelle benötigen. Solche Items können nur Stacks mit entsprechender Konfiguration zugewiesen werden.

% Die Placement-Constraints sind durch Mengen $Q^i$ für die Items $i \in I$ gegeben, wobei $Q^i$
% der Menge der Stacks entspricht, denen Item $i$ zugewiesen werden kann, zu denen Item $i$ also kompatibel ist.
% Dieser Sachverhalt kann durch einen bipartiten Graph $G^P = (I \cup Q, E^P)$ modelliert werden, wobei
% eine Kante $\{i, q\} \in E^P$ angibt, dass Item $i \in I$ Stack $q \in Q$ zugewiesen werden kann.
% Diese Kanten können zusätzlich ein Gewicht erhalten, welches den Transportkosten $c_{iq}$ entspricht.
% Kanten, welche nicht existieren erhalten das Gewicht $\infty$.

Die Placement-Constraints werden indirekt über hohe Kostenwerte implementiert. Die Zuweisung eines Items zu einem
Stack, mit welchem es nicht kompatibel ist, verursacht folglich höhere Kosten als sämtliche zulässigen Zuweisungen.
Dies hat den Vorteil, dass es sich dabei initial nur um weiche Nebenbedingungen handelt,
d.h. es wird auch dann eine Lösung gefunden, wenn gegen diese Nebenbedingungen verstoßen wird.
Eine solche Lösung wird allerdings aufgrund des Kostenminimierungsansatzes nur in Fällen generiert,
in denen keine Lösung existiert, welche sämtliche Placement-Constraints respektiert.
Schließlich kann entschieden werden, ob solche Lösungen als zulässig gelten,
oder, ob die Placement-Constraints dessen ungeachtet als harte Nebenbedingungen und solche
Lösungen dementsprechend als unzulässig betrachtet werden.
Der wesentliche Grund der Entscheidung für diese Art der Implementation ist allerdings eher pragmatischer Natur, denn der in den konstruktiven Heuristiken, welche in Kapitel \ref{sec:constructive_heuristics} vorgestellt werden, verwendete Algorithmus zur Berechnung eines Minimum-Weight-Perfect-Matchings, erwartet einen vollständig bipartiten Graphen als Eingabe. Bei diesem Graphen handelt es sich um einen Kompatibilitätsgraphen, welcher Item-Tupel durch Kanten mit kompatiblen Stacks verbindet. Insofern muss jedes Item zunächst mit jedem Stack kompatibel sein muss. Da die Placement-Constraints genau dies jedoch verhindern, können diese nicht direkt durch ein Fehlen der entsprechenden Kanten umgesetzt werden.

Zunächst werden die Placement-Constraints in einer Binärmatrix $T = (t_{iq})_{n \times m}$ kodiert, wobei:
\[
    t_{iq} =
\begin{cases}
    1, & \text{wenn Item $i$ in Stack $q$ platziert werden darf }\\
    0, & \text{sonst}
\end{cases}
\]
Diese Matrix wird jedoch nicht direkt verwendet, um die Restriktionen zu implementieren. Stattdessen wird
die Transportkosten-Matrix $C = (c_{iq})_{n \times m}$, welche in Abschnitt \ref{sec:transport_costs} eingeführt wurde, nun wie folgt modifiziert.
\[
    c_{iq} =
\begin{cases}
    d_{man}(O_i, F_q), & \text{wenn $t_{iq} = 1$}\\
    \infty, & \text{sonst}
\end{cases}
\]
Der Eintrag an der Stelle $c_{iq}$ enthält also nur noch die tatsächlichen Transportkosten, wenn die
entsprechende Item-Stack-Zuweisung basierend auf den Placement-Constraints zulässig ist und sonst $\infty$.
Die Placement-Constraints gelten als verletzt, wenn eine Lösung eine Item-Stack-Zuweisung
der Kosten $\infty$ enthält.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Komplexität}
\label{sec:complexity}

In den folgenden Kapiteln werden Instanzen von Stacking-Problemen mit Stacks der Kapazitäten $b = 2$
und $b = 3 $ betrachtet, welche besonders in Szenarien aus der Praxis, in denen es darum geht,
Container zu stapeln, von Interesse sind.

Wie \citet{Bruns2015} zeigten, handelt es sich bei dem Zulässigkeitsproblem mit einer Stack-Kapazität von $b=3$ und gegebenen transitiven Stacking-Constraints $s_{ij}$ um ein stark NP-vollständiges Problem. Erst kürzlich wurde zudem gezeigt, dass auch das Zulässigkeitsproblem mit einer Stack-Kapazität von $b=2$, gegebenen transitiven
Stacking-Constraints $s_{ij}$ und Placement-Constraints $t_{iq}$, NP-vollständig ist. Der entsprechende Beweis
wurde allerdings noch nicht veröffentlicht.

Somit sind die in dieser Arbeit betrachteten Stacking-Probleme, bei welchen zusätzlich die Transportkosten minimiert
und jeweils Stacking- sowie Placement-Constraints berücksichtigt werden, ebenfalls NP-vollständig.
Deshalb werden im Folgenden effiziente heuristische Lösungsverfahren für diese Probleme präsentiert,
welche zum Ziel haben, bei möglichst geringer Laufzeit zu möglichst günstigen Ergebnissen zu gelangen.

\subsection{Zielfunktion}
\label{sec:objective}

Das Ziel, welches im Folgenden für sämtliche Problemstellungen angestrebt wird, ist eine möglichst günstige
Einlagerung sämtlicher Items, d.h. die Zielfunktion entspricht der Minimierung der Transportkosten.
Dabei haben Zulässigkeit und geringe Laufzeit Priorität. Eine Lösung wird als zulässig betrachtet,
wenn sämtliche Items unter Berücksichtigung der Stacking- und Placement-Constraints sowie der Stack-Kapazität, eingelagert werden.

Der Fokus auf Zulässigkeit äußert sich darin, dass zunächst stets versucht wird, möglichst wenige der zur Verfügung stehenden Stacks zu verwenden, weshalb die Zielfunktion im Grunde zunächst darin besteht, die Anzahl der verwendeten Stacks zu minimieren bzw. die Anzahl der vollständig gefüllten Stacks zu maximieren, damit möglichst jede Instanz zumindest zulässig gelöst wird.
Erst im Anschluss daran werden die Transportkosten minimiert.
Im Wesentlichen handelt es sich daher um eine Kombination zweier Zielfunktionen,
welche im Sinne der lexikographischen Optimierung mit eine Präferenzordnung ausgestattet sind.
Zu Beginn geht es darum, den zur Verfügung stehenden Platz in der Storage-Area durch initiales Generieren
möglichst vieler $b$-Tupel von Items, ökonomisch zu nutzen. Anschließend werden diese $b$-Tupel
zusammen mit den verbleibenden Items möglichst günstig den zur Verfügung stehenden Stacks zugewiesen.

Ein ausschließlicher Fokus auf Kostenminimierung nachdem die Zulässigkeit erreicht ist, ist durch
einen Postprocessing-Schritt umsetzbar, in welchem zuletzt ausgewählte Items von den ihnen zugewiesenen Positionen
an andere, freie Positionen verschoben werden, sofern sich infolgedessen Kosten einsparen lassen.
Dafür stehen auch die bisher ungenutzten Stacks zur Verfügung.

\pagebreak

Es kann in der Praxis durchaus sinnvoll sein, den zur Verfügung stehenden Platz möglichst ökonomisch zu nutzen
und nicht bedingungslos Kosten zu minimieren und dabei viele lediglich partiell gefüllte Stacks zu generieren.
Ebenso kann allerdings auch ein reiner Fokus auf Kostenminimierung erwünscht sein. Da sich für beide Varianten
entsprechende Szenarien aus der Praxis finden, stellen die konstruktiven Heuristiken,
welche in Kapitel \ref{sec:constructive_heuristics} vorgestellt werden, beide Varianten bereit.
Angesichts der primären Betrachtung der Transportkostenminimierung in dieser Arbeit,
wird der Postprocessing-Schritt in den Experimenten stets durchgeführt.

\section{Testdaten-Generierung}
\label{sec:test_data}

Zu Testzwecken werden durch einen im Zuge dieser Arbeit implementierten \textsc{TestDataGenerator}
unterschiedlich große Instanzen von Stacking-Problemen generiert. Die Größe einer
Instanz bezieht sich dabei stets auf die Anzahl der betrachteten Items. Die Testinstanzen werden
in drei Kategorien unterteilt:
\begin{itemize}
  \item klein (\textbf{s}) ($\leq 100$ Items)
  \item mittelgroß (\textbf{m}) ($\approx 300$ Items)
  \item groß (\textbf{l}) ($\approx 500$ Items)\newline
\end{itemize}
Dabei handelt es sich um einen groben Rahmen, in welchem sich praktisch motivierte Stacking-Probleme
typischerweise bewegen. Dieser findet sich auch in der Literatur wieder (vgl. \citet{Le2016}).

Um die entwickelten Lösungsverfahren sinnvoll testen und miteinander vergleichen zu können,
werden Instanzen generiert, welche im Idealfall nicht allzu weit von realistischen Szenarien entfernt
sein sollten. Dazu stellt der \textsc{TestDataGenerator} eine Reihe von Konfigurationsmöglichkeiten
bereit, welche im Folgenden vorgestellt werden. Um den Vergleichen eine gewisse Aussagekraft zu verleihen,
werden pro Konfiguration $20$ Instanzen generiert.

\textbf{Anzahl zur Verfügung stehender Stacks}

Die erste vom \textsc{TestDataGenerator} bereitgestellte Konfigurationsmöglichkeit ist der Spielraum $a$ in der Storage-Area, welcher der Anzahl der zusätzlichen Stacks entspricht, die über die Minimalanzahl hinausgeht.
Die Anzahl der Items $n$ sowie die Stack-Kapazität $b$ wird stets spezifiziert und in der Berechnung der Anzahl zur Verfügung stehender Stacks $m$ verwendet. Es werden mindestens $\ceil{\frac{n}{b}}$ Stacks benötigt, um sämtliche $n$ Items zuzuweisen.
Aufgrund der in Abschnitt \ref{sec:constraints} geschilderten Nebenbedingungen wären die meisten Instanzen bei dieser
Anzahl an Stacks jedoch unlösbar. Daher wird die Minimalanzahl der Stacks um $a$ erhöht und es gilt
$m = \ceil{\ceil{\frac{n}{b}} + a}$. In den durchgeführten Experimenten wird ein durchaus geringer Spielraum von
zusätzlichen $20 \%$ betrachtet, also $a = 0.2 \cdot \ceil{\frac{n}{b}}$, welcher in weiteren Betrachtungen erhöht werden könnte.

\pagebreak

\textbf{Stacking-Constraints}

Die zweite Möglichkeit der Konfiguration besteht in der Stacking-Constraint-Generierung,
von welcher zwei Varianten implementiert wurden, die jeweils spezifische Vor- und Nachteile aufweisen
und im \textsc{TestDataGenerator} ausgewählt werden können.

\textbf{Variante 1}

Die Stacking-Constraint-Matrix $S$ wird anhand einer definierten Wahrscheinlichkeit mit $1$- bzw. $0$-Einträgen gefüllt. Anschließend werden jene $1$-Einträge, welche aus Transitivität folgen, ergänzt.

\textbf{Variante 2}

Für alle $n$ Items wird eine zufällige Länge und Breite generiert. Item $i$ kann auf Item $j$
platziert werden, wenn für dessen Länge $l_i$ und Breite $w_i$ gilt $l_i \leq l_j$ und $w_i \leq w_j$.
Zwei Items $i, j$ sind in beide Richtungen stapelbar, wenn $l_i = l_j$ und $w_i = w_j$ gilt.

Die zweite Variante erscheint realitätsnäher und wird deshalb in den Experimenten verwendet.
Aufgrund der Tatsache, dass diese jedoch bei zufälligen Längen und Breiten der Items stets zu einem Anteil
von $1$-Einträgen in der Stacking-Constraint-Matrix $S$ von ca. $25\%$ führt und jener mit Variante 1 sehr gut
konfigurierbar ist, wird Variante 1 nicht verworfen und kann ggf. in weiteren Experimenten, in welchen eine
solche Konfiguration erwünscht ist, zum Einstaz kommen.

Die zu ungefähr einem Viertel mit $1$-Einträgen gefüllte Matrix $S$ ergibt sich bei partiellen Ordnungen der Dimension 2,
die hier aufgrund der zwei Werte (Länge, Breite), welche pro Item generiert werden, gegeben sind.
Diese partiellen Ordnungen lassen sich auch als Permutationen darstellen, wobei eine Kante zwischen
zwei Knoten im Vergleichbarkeitsgraphen existiert, wenn diese in der Permutation
vertauscht sind. Ein solches ungeordnetes Paar von Elementen einer Permutation wird auch Inversion
der Permutation genannt. Die erwartete Anzahl von Inversionen in zufälligen Permutationen
beträgt $\frac{n \cdot (n - 1)}{4}$ bzw. $\frac{1}{2} \binom{n}{2}$ (vgl. \citet{Heuberger2012}).
Eine Kante im Vergleichbarkeitsgraphen bedeutet in diesem Fall, dass die verbundenen Items
basierend auf Variante 2 der Stacking-Constraint-Generierung in mindestens einer Richtung stapelbar sind,
d.h. die erwartete Anzahl der Inversionen entspricht der erwarteten Anzahl an $1$-Einträgen in $S$.
Dies gilt im Allgemeinen für zufällige Längen und Breiten der Items und ist nicht zwingend der Fall,
wenn diese Werte jeweils aus kleineren Intervallen gewählt werden.
Die in den Experimenten betrachteten Intervalle, welche im Folgenden definiert werden,
führen zu ca. $36\%$ $1$-Einträgen in $S$. Bei Vergrößerung der Intervalle nähert sich dieser
erwartete Wert den zuvor erwähnten $25\%$ an.

% sind ausreichend groß, um zumindest tendenziell ein Viertel $1$-Einträge in $S$ zu erhalten.

\textbf{Placement-Constraints}

Die Placement-Constraints werden zufällig generiert, wobei die Wahrscheinlichkeit für $t_{iq} = 1$, also dafür,
dass ein Item $i$ in einem Stack $q$ platziert werden darf, im \textsc{TestDataGenerator} konfiguriert werden kann.
Die Konfiguration, welche den Experimenten zugrunde liegt, definiert diese Wahrscheinlichkeit
als $70 \%$, da in der Praxis tendenziell mehr Zuweisungen erlaubt als verboten sind.

\pagebreak

\textbf{Item- und Stackpositionierung}

Im Wesentlichen liegen der Item- und Stackpositionierung zwei Grids zugrunde. Zum einen die Storage-Area,
welche die Stacks beinhaltet und zum anderen die Fahrzeuge, auf welchen die Items geliefert werden.
Die Positionierung der Items und Stacks einer Instanz ist somit an die schematische Darstellung in Abb. \ref{fig:costs}
angelehnt. In der Storage-Area existiert eine im \textsc{TestDataGenerator} konfigurierbare Anzahl von Stackreihen.
Da die Grids nicht notwendigerweise direkt aneinander grenzen, sondern z.B. wie in Abb. \ref{fig:costs}
durch \textquote{Driving-} und \textquote{Parking-Lane} voneinander getrennt sind, kann zudem eine Distanz
zwischen der Storage-Area und dem zu entladenen Fahrzeug spezifiziert werden.
Das Grid, welches die initialen Item-Positionen $O_i$ repräsentiert, besteht in den in dieser Arbeit betrachteten
Instanzen stets aus zwei Reihen.

Die Positionen der Items und Stacks sind jeweils als $x$- und $y$-Koordinaten gegeben.
Diese ergeben sich aus der Länge und Breite der Zellen der beiden Grids, in welchen die Items platziert werden
bzw. bei ihrer Ankunft platziert sind. Für diese Zellen wird eine einheitliche Größe angenommen,
welche sowohl für die Storage-Area, als auch für die Fahrzeuge gilt.
Neben der Konfiguration der Anzahl der Reihen, ist daher außerdem die Definition dieser Zellengröße der beiden
Grids von Relevanz. Der als erstes weltweit standardisierte Seecontainer ist der 20-Fuß-Container TEU\footnote{Twenty Foot Equivalent Unit (Standardcontainer).}, welcher bis heute international als Maßeinheit gebräuchlich ist. Der TEU ist
$6.06 \thinspace m$ lang, $2.44 \thinspace m$ breit und $2.59 \thinspace m$ hoch (vgl. \citet{ContainerBasis}).
In Orientierung am TEU betragen Länge und Breite einer Zelle in den Experimenten $6.06 \thinspace m$ bzw.
$2.44 \thinspace m$. Außerdem lässt sich die erwähnte Distanz zwischen den beiden Grids konfigurieren.
Bei den Längen und Breiten der Items selbst handelt es sich um zufällige Werte, welche nicht die Zellengröße überschreiten.
Die Länge eines Items wird dementsprechend zufällig aus dem Intervall $[1.0, 6.0]$ generiert während die Breite aus dem
Intervall $[1.0, 2.4]$ stammt. Da es typischerweise nur eine begrenzte Anzahl unterschiedlicher Item-Typen im selben Lager
gibt, werden nicht sämtliche Werte aus diesen Intervallen betrachtet, sondern lediglich in Schritten von $0.5$.
Folglich sind die Item-Längen $\{1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0\}$ und
die Item-Breiten $\{1.0, 1.5, 2.0\}$ möglich.
Die konkreten Maße der Items sind für deren Positionierung allerdings irrelevant und lediglich für
die zweite Variante der Stacking-Constraint-Generierung von Bedeutung.
Die Itemdimensionen haben dort direkten Einfluss auf die Stacking-Constraints,
weil beispielsweise die Wahrscheinlichkeit für Item-Paarungen, welche in beide Richtungen stapelbar sind,
mit steigenden Intervallgrößen abnimmt.

Für eine Instanz mit $40$ Items und $17$ Stacks der Kapazität $b = 3$ ergibt sich bei den genannten
Zellendimensionen, mit einer konfigurierten Anzahl von drei Stackreihen sowie einer Distanz zwischen
Storage-Area und der ersten Fahrzeugspur vom Fünffachen der Zellenbreite,
die in Abb. \ref{fig:positioning_example} dargestellte Item- und Stackpositionierung.
Die Positionen entsprechen in der von oben betrachteten Darstellung
in der Abbildung stets der unteren linken Ecke eines Items bzw. Stacks.

\vfill
\pagebreak

% TDODO: add plot

\textbf{Transportkosten}

Die Kosten der Verladung eines Items in einen Stack entsprechen, wie in Abschnitt \ref{sec:transport_costs}
erläutert, der Distanz zwischen der Originalposition dieses Items und der Position des jeweiligen Stacks.
Zur Ermittlung der Distanzen zwischen Items und Stacks, werden deren Positionen bestehend aus
$x$- und $y$-Koordinaten, welche in zuvor beschriebener Weise generiert werden, herangezogen.
In den im Zuge dieser Arbeit durchgeführten Experimenten basieren die Distanzen zwischen Item- und
Stackpositionen stets auf der Manhattan-Metrik, welche die Kranbewegung repräsentiert,
in anderen Szenarien können jedoch andere Metriken geeigneter sein und dementsprechend im \textsc{TestDataGenerator}
definiert werden.

\vfill

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{MIP-Formulierungen}
\label{sec:mip_formulations}

In diesem Kapitel werden MIP-Formulierungen eingeführt, welche dem experimentellen Vergleich dienen.
Diese werden verwendet, um die optimalen Zielfunktionswerte der Test-Instanzen zu ermitteln und somit eine Grundlage zu schaffen, mit welcher die Qualität der von den Heuristiken generierten Lösungen beurteilt werden kann. Des Weiteren geht es um einen Vergleich der im Folgenden vorgestellten MIP-Formulierungen untereinander.
Es handelt sich dabei um auf die in dieser Arbeit betrachteten Problemstellungen angepasste Formulierungen,
welche in ihrer ursprünglichen Form von \citet{Le2016} vorgestellt wurden.
Zusätzlich zu eher marginalen Änderungen an den Nebenbedingungen, welche der etwas anderen Ausgangssituation geschuldet sind,
wird, wie bereits einleitend erwähnt, mit der Transportkostenminimierung jeweils eine andere Zielfunktion betrachtet.
In den ursprünglichen Formulierungen entspricht die Zielfunktion jeweils der Minimierung der Anzahl der verwendeten Stacks.

Die Zielfunktion der Heuristiken stellt, wie in Abschnitt \ref{sec:objective} geschildert, im Grunde die Kombination
zweier Zielfunktionen dar, welche initial die Anzahl der verwendeten Stacks minimiert, indem möglichst viele
$b-$Tupel von Items generiert werden, bevor diese möglichst günstig den Stacks zugewiesen werden.
Insofern ist zu erwarten, dass die MIP-Formulierungen verglichen mit den konstruktiven Heuristiken
aus Abschnitt \ref{sec:constructive_heuristics} prinzipiell, vorausgesetzt beide Lösungsverfahren lösen eine
Instanz zulässig, zu etwas günstigeren Zielfunktionswerten gelangen, dafür jedoch mehr Stacks verwenden.
Tatsächlich vergleichbar sind die Ergebnisse der MIP-Formulierungen mit jenen der Heuristiken allerdings nur,
wenn beide Lösungsverfahren dieselbe Zielfunktion betrachten. Dies wird durch ein Postprocessing-Verfahren
ermöglicht, welches beide konstruktiven Heuristiken bereitstellen und welches einen reinen Fokus auf die
Transportkostenminimierung ermöglicht, nachdem die Zulässigkeit erreicht ist.

Zunächst wird in Abschnitt \ref{sec:mip_definition} kurz die Idee des MIP im Allgemeinen
eingeführt, um dann im Anschluss in den Abschnitten \ref{sec:bin_packing_formulation} und \ref{sec:three_idx_formulation} die beiden in dieser Arbeit betrachteten MIP-Formulierungen im Detail vorzustellen.

\subsection{Mixed Integer Linear Programming (MIP)}
\label{sec:mip_definition}

Viele kombinatorische Optimierungsprobleme lassen sich als lineare Programme formulieren, die sich dadurch
auszeichnen, dass die Zielfunktion eine lineare Funktion in Abhängigkeit von den Entscheidungsvariablen ist
und alle Nebenbedingungen in Form von linearen Gleichungen und Ungleichungen vorliegen. Für diese Probleme gibt es mit dem Simplexverfahren ein sehr effizientes Lösungsverfahren.\cite{Knust2017}

Ein gemischt-ganzzahliges lineares Programm (MIP) ist ein lineares Programm, welches kontinuierliche und ganzzahligen Variablen
enthält. Im Gegensatz zur kontinuierlichen Variante, ist die Lösung von ganzzahligen linearen Programmen NP-schwer.
Während sogar sehr große kontinuierliche lineare Programme durch kommerzielle Solver in wenigen Minuten gelöst werden können, gilt dies nicht für ganzzahlige lineare Programme. Diese können nur mit deutlich zeitaufwändigeren Algorithmen wie Branch \& Bound gelöst werden.\cite{Brucker2006}

In dieser Arbeit wird das Java-Interface von CPLEX, einem Solver für lineare Optimierungsprobleme verwendet,
um die MIP-Formulierungen, die in Abschnitt \ref{sec:bin_packing_formulation} und Abschnitt \ref{sec:three_idx_formulation} vorgestellt werden,
zu lösen. Es handelt sich dabei um eine unter anderem auf dem Simplexverfahren basierende Implementation von IBM. \cite{CPLEX2015}

\subsection{Bin-Packing-Formulierung}
\label{sec:bin_packing_formulation}

Die Bin-Packing-Formulierung orientiert sich dem Namen nach am klassichen \textsc{Bin-Packing} Problem, bei dem es darum geht,
eine Menge von Objekten so auf eine Menge von Behältern zu verteilen, dass die Behälterkapazitäten eingehalten werden.
Dies beschreibt auch direkt den Ansatz der Bin-Packing-Formulierung. Die Stacks werden als Behälter betrachtet und die Items als Objekte,
die diesen Behältern zugeordnet werden, wobei die Behälter- bzw. Stack-Kapazität jeweils eingehalten wird. Im Unterschied zum klassischen \textsc{Bin-Packing} müssen zusätzlich die Stacking- und Placement-Constraints berücksichtigt werden.

Es sei eine Menge von Items $I = \{1, ..., n\}$ und eine Menge von Stacks $Q = \{1, ..., m\}$ mit einer Kapazität von $b$ gegeben.
Außerdem sei $G = (I, A)$ ein transitiver Stacking-Constraint-Graph, bei dem eine Kante $(i, j) \in A$ bedeutet,
dass die Items $i$ und $j$ in mindestens einer Richtung stackbar sind, also $s_{ij} = 1$ oder $s_{ji} = 1$.\newline
Bei den Variablen $x_{iq}$ handelt es sich um Binärvariablen, wobei $x_{iq} = 1$ bedeutet, dass Item $i$ Stack $q$ zugewiesen wird.
Die Kosten werden über die Variablen $c_{iq}$ modelliert, wobei $c_{iq}$ den Kosten für den Transport des Items $i$ in den Stack $q$
entspricht.

\begin{gather}
\boldsymbol{min} \quad \sum_{i \in I} \sum_{q \in Q} c_{iq} x_{iq} \label{bin_packing_line_one} \\
\thinspace\thinspace \boldsymbol{s.t.} \thinspace\quad\thinspace\thinspace\thinspace\quad\thinspace \quad \sum_{q \in Q} x_{iq} = 1 \quad\quad\quad\quad\quad\quad\quad \forall i \in I \quad\quad\quad\quad\thinspace\thinspace \label{bin_packing_line_two} \\
\quad \sum_{i \in I} x_{iq} \leq b \quad\quad\quad\quad\quad\quad\quad \forall q \in Q \label{bin_packing_line_three} \\
\thinspace\thinspace\quad\quad x_{iq} + x_{jq} \leq 1 \quad\quad\thinspace\thinspace\quad\quad\quad\quad \forall \{i, j\} \notin A \label{bin_packing_line_four} \\
\thinspace\thinspace\quad\quad\quad x_{iq} \in \{0, 1\} \thinspace\thinspace\thinspace\thinspace\thinspace\thinspace\quad\quad\quad\quad\quad\quad \forall i \in I, q \in Q \thinspace \label{bin_packing_line_five}
\end{gather}

In (\ref{bin_packing_line_one}) wird die betrachtete Zielfunktion definiert, welche die Summe der Kosten sämtlicher
Item-Stack-Zuweisungen minimiert, was der Minimierung der Transportkosten entspricht.
Nebenbedingung (\ref{bin_packing_line_two}) fordert, dass jedes Item genau einem Stack zugewiesen wird. Die Ungleichung in (\ref{bin_packing_line_three}) entspricht der Forderung, dass für sämtliche Stacks die Stack-Kapazität eingehalten werden muss, d.h.
die Summe der eines Stacks zugewiesenen Items darf dessen Kapazität nicht überschreiten. (\ref{bin_packing_line_four}) entspricht schließlich der Forderung, dass alle Items, die nicht stackbar sind, für die also keine Kante im Stacking-Constraint-Graph existiert, nicht Teil desselben Stacks sind. (\ref{bin_packing_line_five}) definiert letztendlich die Domäne der Variablen $x_{iq}$.

\subsection{3-Index-Formulierung}
\label{sec:three_idx_formulation}

Die 3-Index-Formulierung enthält drei Indizes - Item, Stack und Level.
Im Gegensatz zur Bin-Packing-Formulierung, die Item-Stack-Zuweisungen vornimmt, weist die 3-Index-Formulierung einem
Item auch eine konkrete Position innerhalb eines Stacks zu, sie berücksichtigt also den Level.

Gegeben sei erneut eine Item-Menge $I = \{1, ..., n\}$ und eine Stack-Menge $Q = \{1, ..., m\}$, die Stacks der Kapazität $b$ enthält.
Jeder Stack enthält dementsprechend die Level $L = \{1, ..., b\}$. Des Weiteren sei ein transitiver Stacking-Constraint-Graph
$G = (I, A)$ gegeben, bei dem eine Kante $i \rightarrow j \in A$ bedeutet, dass das Item $i$ direkt auf das Item $j$ gestapelt werden darf.
Die Kosten seien durch die gegebene Transportkostenmatrix $C$ definiert, in der an der Stelle $c_{iq}$ die Kosten für den
Transport von Item $i$ in Stack $q$ stehen. Bei den Variablen $x_{iql}$ handelt es sich um Binärvariablen,
bei denen $x_{iql} = 1$ bedeutet, dass Item $i$ in Stack $q$ auf Level $l$ platziert wird.

\begin{gather}
\boldsymbol{min} \quad \sum_{i \in I} \sum_{q \in Q} \sum_{l \in L} c_{iq} x_{iql} \label{three_idx_line_one} \\
\boldsymbol{s.t.} \quad \sum_{q \in Q} \sum_{l \in L} x_{iql} = 1 \quad\thinspace\thinspace\quad\quad\quad\quad \forall i \in I \thinspace\thinspace\quad\quad\quad\quad\quad\quad\quad\quad\thinspace\thinspace \label{three_idx_line_two} \\
\sum_{i \in I} x_{iql} \leq 1 \thinspace\quad\quad\quad\quad\quad\quad\quad \forall q \in Q, l \in L \thinspace\thinspace\thinspace\thinspace\quad\quad\quad\thinspace \label{three_idx_line_three} \\
\sum_{j \in I | i \rightarrow j \in A} x_{jq(l-1)} - x_{iql} \geq 0 \thinspace\quad\quad \forall i \in I, q \in Q, l \in L \textbackslash \{1\}
\label{three_idx_line_four} \\
\quad\quad x_{iql} \in \{0, 1\} \quad\quad\quad\quad\quad\quad\quad \forall i \in I, q \in Q, l \in L \quad\thinspace\thinspace\thinspace\thinspace\thinspace \label{three_idx_line_five}
\end{gather}

In (\ref{three_idx_line_one}) wird die Zielfunktion definiert, die erneut die Summe der Kosten sämtlicher Item-Zuweisungen
und somit die Transportkosten minimiert.
Nebenbedingung (\ref{three_idx_line_two}) fordert, dass sich jedes Item in genau einem Stack an genau einer Position (Level) befindet,
also im Grunde, dass jedes Item einer Position in einem Stack zugewiesen wird.
Anschließend wird in (\ref{three_idx_line_three}) gefordert, dass sich an jeder Position in jedem Stack höchstens ein Item befindet und
(\ref{three_idx_line_four}) formuliert die Einschränkung, dass jedes Item, welches oberhalb des Ground-Levels platziert wird,
ein Item unter sich hat, welches sich basierend auf den Stacking-Constraints dort befinden darf.
Es setzt also die Stacking-Constraints um und sorgt zusätzlich dafür, dass kein Item \textquote{in der Luft} gestapelt wird.
Schließlich wird in (\ref{three_idx_line_five}) die Domäne der Variablen $x_{iql}$ definiert.

Beide Formulierungen weisen jedem Item einen Stack zu und minimieren dabei die Transportkosten.
Die Bin-Packing-Formulierung nimmt dabei lediglich Item-Stack-Zuweisungen vor, während die 3-Index-Formulierung die
Items konkreten Positionen innerhalb der Stacks zuweist. Dabei ist allerdings auch bei der Bin-Packing-Formulierung
sichergestellt, dass eine Item-Stack-Zuweisung nur dann vorgenommen wird, wenn die Items innerhalb des Stacks anschließend
in einer Weise umsortiert werden können, dass sie den Stacking Constraints entsprechen. Diese Tatsache ergibt sich aus Nebenbedingung (\ref{bin_packing_line_four}), welche fordert, dass Items sich nur dann im selben Stack befinden, wenn sie in mindestens einer Richtung stackbar sind.
Da die Stacking Constraints außerdem transitiv sind, ist sichergestellt, dass die Reihenfolge innerhalb der Stacks stets so korrigiert werden kann, dass eine valide Lösung, die die Stacking Constraints respektiert, dabei herauskommt.

Die 3-Index-Formulierung liefert folglich direkt eine zulässige Lösung, während die Item-Stack-Zuweisungen, die die Bin-Packing-Formulierung liefert, zunächst noch sortiert werden müssen, damit sie eine zulässige Lösung ergeben.
Dazu werden sämtliche Stacks, denen mehr als ein Item zugewiesen wurde, betrachtet. Für jeden
dieser Stacks wird geprüft, ob die aufeinander gestapelten Items jeweils paarweise aufeinander gestapelt sein dürfen.
Werden zwei Items innerhalb eines Stacks gefunden, welche die Stacking-Constraints verletzen, so werden diese
vertauscht und der Prozess startet für diesen Stack erneut. Ein vollständig gefüllter Stack mit $b = 3$ ist mit dieser
Strategie nach spätestens drei Tausch-Operationen korrekt sortiert.

% \begin{algorithm}
% \fontsize{10pt}{10pt}

% \SetAlgoLined

%   \ForEach{stack in storageArea}{

%     somethingChanged = true\;

%     \While{somethingChanged}{

%       somethingChanged = false\;

%       \For{position = 0; position < stack.capacity - 1; position++}{
%           \BlankLine\BlankLine
%           \uIf{stack[position].\Call{isOccupied()}{} \&\& stack[position + 1].\Call{isOccupied()}{{}}}{
%             \uIf{itemAt(stack[position]).\Call{isStackableOn}{itemAt(stack[position + 1])}}{

%                 \Call{exchangeItems}{stack[position], stack[position + 1]}\;
%                 somethingChanged = true\;
%                 break\;
%             }
%           }
%       }
%     }
%   }

% \caption{Korrektur der Item Reihenfolge innerhalb der Stacks.}
% \label{alg:algo1}
% \end{algorithm}

\section{Untere Schranken}
\label{sec:lower_bounds}

Zur Abschätzung der Güte von heuristischen Lösungen berechnet man bei Minimierungsproblemen häufig untere Schranken,
d.h. Werte $LB$, sodass $c(S) \geq LB$ für alle Lösungen $S$ gilt, wobei $c(S)$ dem Zielfunktionswert von $S$ entspricht.
Zur Berechnung solcher Schranken betrachtet man sogenannte Relaxationen\footnote{Einfachere Probleme, bei denen bestimmte Nebenbedingungen vernachlässigt werden.} des ursprünglichen Problems.
Da die Lösungsmenge des ursprünglichen Problems eine Teilmenge der Lösungsmenge der Relaxation ist,
definiert der optimale Zielfunktionswert der Relaxation eine untere Schranke für die optimale Lösung des Ausgangsproblems
\cite{Knust2017}.

Eine Abschätzung der Güte heuristischer Lösungen durch eine untere Schranke ist z.B. bei großen Instanzen hilfreich,
welche nicht bzw. nicht effizient durch die in Kapitel \ref{sec:mip_formulations} vorgestellten MIP-Formulierungen gelöst werden können.
Der optimale Lösungswert ist in solchen Fällen nicht bekannt, weshalb sich eine untere Schranke als Referenzwert
zur Beurteilung der Lösungsqualität heranziehen lässt. Eine untere Schranke für die Transportkosten einer Instanz eines Stacking-Problems lässt sich durch eine Relaxation des Problems bezüglich der Stacking-Constraints $s_{ij}$ generieren.
Die Placement-Constraints $t_{iq}$ sowie die Stack-Kapazität $b$ müssen weiterhin erfüllt werden.

Sei $P$ die Menge aller $mb$ Positionen innerhalb der Stacks und $q(p)$ jener Stack, zu welchem die Position $p$ gehört.
Ferner sei $G^c = (I \cup P, E^c)$ ein bipartiter Graph, in welchem eine Kante $\{i, p\} \in E^c$ existiert,
wenn $t_{iq(p)} = 1$, d.h. wenn ein Item $i$ kompatibel zum Stack ist, zu welchem die Position $p$ gehört.
Das Gewicht der Kante $\{i, p\}$ entspricht den Kosten $c_{iq(p)}$.

Berechnet man nun ein Minimum-Weight-Perfect-Matching zwischen Items und Stack-Positionen, so entspricht
das Gewicht des Matchings einer unteren Schranke für die Gesamtkosten. Da die Kosten der Zuweisung eines Items
zu einem Stack unabhängig vom Level innerhalb des Stacks sind, werden die Items jeweils den untersten Positionen
innerhalb eines Stacks zugewiesen.

\vfill

\pagebreak

\section{Konstruktive Heuristiken}
\label{sec:constructive_heuristics}

Bei den entwickelten konstruktiven Heuristiken handelt es sich um effiziente Lösungsverfaren für die in Kapitel \ref{sec:stacking_problems}
beschriebenen Problemstellungen. Zunächst werden vier wichtige Konzepte eingeführt, welche in den Heuristiken Verwendung finden.
In \ref{sec:digression_bipartite_graph} bipartite Graphen, in \ref{sec:digression_mwpm} Minimum-Weight-Perfect-Matchings, in \ref{sec:digression_mcm} Maximum-Cardinality-Matchings und in \ref{sec:digression_mwbm} Maximum-Weight-Bipartite-Matchings. Anschließend geht es in \ref{sec:two_cap_heuristic} um die konstruktive Heuristik
für eine Stack-Kapazität von $b=2$ und darauffolgend in \ref{sec:solver_comp_b=2} um einen experimentellen Vergleich der Solver für
eben diese Stack-Kapazität in Bezug auf die benötigten Laufzeiten und die Qualität der generierten Lösungen.
Abschnitt \ref{sec:three_cap_heuristic} behandelt die konstruktive Heuristik für eine Stack-Kapazität von $b=3$, worauf erneut ein Vergleich der entsprechenden Solver in \ref{sec:solver_comp_b=3} folgt.

\subsection{Hintergrund}

\subsubsection{Bipartiter Graph}
\label{sec:digression_bipartite_graph}

Ein Graph $G = (V, E)$ heißt bipartit (vgl. Abb. \ref{fig:digression_bipartite_graph}), wenn die Knotenmenge in zwei disjunkte Teilmengen zerfällt
$(V = S \cup T$ mit $S \cap T = \emptyset$), sodass jede Kante einen Knoten aus $S$ mit einem Knoten aus $T$ verbindet. \cite{HochschuleDarmstadt}\newline
Es lassen sich folgende Äquivalenzen festhalten \cite{Leighton2010}:
\begin{itemize}
  \item $G$ bipartit $\iff$ $G$ 2-färbbar
  \item $G$ bipartit $\iff$ keine Kreise ungerader Länge in $G$
\end{itemize}
In Abb. \ref{fig:digression_bipartite_graph} ist die 2-Färbbarkeit eines bipartiten Graphen, bei welcher die eine Teilmenge
in grün und die andere Teilmenge in rot gefärbt ist, visualisiert. Wie man sieht, existieren in solchen Graphen
ausschließlich Kanten, welche einen Knoten der einen Teilmenge (rot) mit einem Knoten der anderen Teilmenge (grün) verbinden,
jedoch keine Kanten zwischen Knoten derselben Farbe bzw. Teilmenge.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.6, transform shape, node distance=3cm]
  \node[state] [green] (A) {};
  \node[state] [red] (B) [above right of=A] {};
  \node[state] [green] (C) [below right of=B] {};
  \node[state] [red] (D) [below of=B] {};
  \node[state] [red] (E) [below of=A] {};
  \node[state] [green] (F) [right of=E] {};

  \path (A) edge node {} (B)
        (B) edge node {} (C)
        (C) edge node {} (D)
        (A) edge node {} (D)
        (A) edge node {} (E)
        (E) edge node {} (F)
        (D) edge node {} (F);

\end{tikzpicture}
\caption{\textsc{Bipartiter Graph.}}
\label{fig:digression_bipartite_graph}
\end{figure}

Ein Graph wird als vollständig bipartit bezeichnet, wenn jeder Knoten der einen Teilmenge mit jedem Knoten
der anderen Teilmenge durch eine Kante verbunden ist. Der in Abb. \ref{fig:digression_bipartite_graph}
dargestellte Graph ist dementsprechend nicht vollständig bipartit.

\subsubsection{Minimum-Weight-Perfect-Matching (\textsc{MWPM})}
\label{sec:digression_mwpm}

Gegeben sei ein ungerichteter Graph $G = (V, E)$. Eine Menge $M \subseteq E$ heißt Matching,
wenn keine zwei Kanten aus $M$ einen Knoten gemeinsam haben. Bei einem Matching handelt es sich also
um eine Menge unabhängiger Kanten. Ein Perfect-Matching ist wiederum ein Matching, welches sämtliche Knoten des Graphen enthält.
Jeder Knoten des Graphen ist dabei inzident zu genau einer Kante des Matchings (vgl. Abb. \ref{fig:perfect_matching}).
Daher ist ein Perfect-Matching nur für Graphen mit einer geraden Anzahl an Knoten möglich.
Ein Minimum-Weight-Perfect-Matching ist ein günstigstes Perfect-Matching basierend auf den Kantenkosten.
\begin{figure}[H]
\centering
\begin{tikzpicture}[-, scale=0.6, transform shape, node distance=3cm]
    \node[state] (A) {};
    \node[state] (B) [above right of=A] {};
    \node[state] (C) [below right of=A] {};
    \node[state] (D) [right of=B] {};
    \node[state] (E) [right of=D] {};
    \node[state] (F) [right of=C] {};

    \path (A) edge [red] node {} (B)
          (A) edge node {} (C)
          (B) edge node {} (D)
          (B) edge node {} (C)
          (D) edge [red] node {} (E)
          (D) edge node {} (F)
          (C) edge [red] node {} (F);
\end{tikzpicture}
\caption{\textsc{Perfect-Matching (dargestellt in rot).}}
\label{fig:perfect_matching}
\end{figure}

Der Algorithmus zur Berechnung von Minimum-Weight-Perfect-Matchings, welcher in den im Folgenden
vorgestellten Heuristiken zum Einsatz kommt, stammt aus der Java-Bibliothek \textit{JGraphT} \cite{JGraphT}, trägt
den Namen \textit{KuhnMunkresMinimalWeightBipartitePerfectMatching} und ist eine Implementation der
sogenannten Ungarischen Methode zum Lösen von Zuordnungsproblemen in bipartiten Graphen.
Die Laufzeitkomplexität der Implementation beträgt $O(n^3)$, wobei $n$ der Anzahl der Knoten im Graphen entspricht.
Als Eingabe wird ein vollständig bipartiter Graph $G$ erwartet, d.h. $G= (S \cup T, E)$, wobei $S \cap T = \emptyset$ und
$\forall s \in S, t \in T \thinspace \exists \thinspace \{s, t\} \in E$. Außerdem wird erwartet, dass beide Partitionen gleich groß sind,
also $|S| = |T|$. Bei der Rückgabe des Algorithmus handelt es sich um ein Perfect-Matching der geringsten Kosten.

\subsubsection{Maximum-Cardinality-Matching (MCM)}
\label{sec:digression_mcm}

Sei $M$ ein Matching in einem Graphen $G$. Falls $M$ als Menge eine maximale Kardinalität unter
allen Matchings von $G$ hat, wird dies als Maximum-Cardinality-Matching bezeichnet. \cite{WikiMatching}
Es kann mehrere unterschiedliche MCMs in einem Graphen geben.
In Abb. \ref{fig:mcm_examples} sind beispielhaft zwei MCMs dargestellt.
\begin{figure}[H]
  \begin{subfigure}[b]{0.4\textwidth}
  \centering
    \begin{tikzpicture}[scale=0.55, transform shape, node distance=2cm]
    \node[state] (A) {};
    \node[state] (B) [above right of=A] {};
    \node[state] (C) [below left of=A] {};
    \node[state] (D) [below right of=C] {};
    \node[state] (F) [below right of=B] {};
    \node[state] (E) [below of=F] {};

    \path (A) edge [red] node {} (B)
          (A) edge node {} (C)
          (A) edge node {} (D)
          (A) edge node {} (E)
          (A) edge node {} (F)
          (C) edge [red] node {} (D);
  \end{tikzpicture}
  \caption{\textsc{$|MCM| = 2$}}
  \label{fig:mcm1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.4\textwidth}
  \centering
    \begin{tikzpicture}[scale=0.55, transform shape, node distance=2cm]
    \node[state] (A) {};
    \node[state] (B) [right of=A] {};
    \node[state] (C) [right of=B] {};
    \node[state] (D) [below left of=A] {};
    \node[state] (E) [below right of=D] {};
    \node[state] (F) [right of=E] {};

    \path (A) edge node {} (B)
          (B) edge [red] node {} (C)
          (B) edge node {} (F)
          (A) edge node {} (E)
          (E) edge [red] node {} (F)
          (A) edge [red] node {} (D)
          (D) edge node {} (E);
  \end{tikzpicture}
    \caption{\textsc{$|MCM| = 3$}}
    \label{fig:mcm_2}
  \end{subfigure}
  \caption{\textsc{MCMs (dargestellt in rot).}}
  \label{fig:mcm_examples}
\end{figure}

Der Algorithmus zur Berechnung von Maximum-Cardinality-Matchings, welcher in den im Folgenden
vorgestellten Heuristiken Anwendung findet, stammt ebenfalls aus der Java-Bibliothek \textit{JGraphT} \cite{JGraphT} und trägt
den Namen \textit{EdmondsMaximumCardinalityMatching}. Es handelt sich dabei um eine Implementation von Edmonds' Blossom-Algorithmus,
welcher MCMs in ungerichteten Graphen berechnet. Der Algorithmus liefert ein Perfect-Matching, sofern eines existiert.
Andernfalls wird das größte Non-Perfect-Matching zurückgegeben.
Der Algorithmus besitzt eine Laufzeitkomplexität von $O(nm \thinspace \alpha(m, n))$. Wobei $n$ der Anzahl der Knoten
und $m$ der Anzahl der Kanten im Graphen entspricht. Im \textquote{Average-Case}, also im Regelfall, ist der Algorithmus jedoch effizienter.

\subsubsection{Maximum-Weight-Bipartite-Matching (MWBM)}
\label{sec:digression_mwbm}

Bei einem Maximum-Weight-Bipartite-Matching handelt es sich um ein Matching in einem bipartiten Graphen,
in welchem die Summe der Kantengewichte maximal ist. Auch diesen Algorithmus stellt \textit{JGraphT} \cite{JGraphT}
unter dem Namen \textit{MaximumWeightBipartiteMatching} bereit.
Die Implementation besitzt eine Laufzeitkomplexität von $O(n (m + n \thinspace log \thinspace n))$, wobei $n$ der Knotenanzahl
und $m$ der Anzahl der Kanten im Graphen entspricht.

\subsection{Konstruktive Heuristik ($b = 2$)}
\label{sec:two_cap_heuristic}

Die konstruktive Heuristik zur Lösung von Stacking-Problemen mit Stacks der Kapazität $b=2$ erweitert den Ansatz aus dem Beweis
zu Theorem 1 aus der einleitend erwähnten Arbeit von \citet{Bruns2015}.

Zunächst wird ein Stacking-Constraint-Graph $G = (V, E)$ generiert, welcher die Items als Knoten enthält, d.h. $V = I$. Dieser Graph
enthält eine Kante $\{i, j\} \in E$ zwischen zwei Knoten, wenn die entsprechenden Items $i, j \in I$ basierend auf den Stacking-Constraints
in mindestens einer Richtung stackbar sind, d.h. wenn $s_{ij} + s_{ji} \geq 1$ und wenn mindestens ein Stack existiert, in welchem die beiden Items gemeinsam platziert werden können ohne die Placement-Constraints zu verletzen, also wenn $c_{iq} < \infty$ und $c_{jq} < \infty$ für
mindestens einen Stack $q \in Q$ gilt.

Berechnet man nun ein \textsc{MCM} in $G$ und interpretiert dessen Kanten als Stacks, welche die beiden Knoten, die durch
die Kanten verbunden werden, als Items enthalten, so erhält man eine größtmögliche Anzahl an vollständig gefüllten Stacks.
Items, welche nicht inzident zu einer Kante des Matchings sind, werden auf dem Ground-Level eines eigenen Stacks platziert.
Insgesamt werden auf diese Weise so wenig Stacks wie möglich verwendet, denn auch wenn der eigentliche Gegenstand der Optimierung die Transportkosten sind, steht zunächst stets im Vordergrund, überhaupt eine zulässige Lösung zu finden, weshalb die Maximierung der Anzahl der vollständig gefüllten Stacks initial eine gute Strategie darstellt.
Eine zulässige Lösung mit maximal $m$ Stacks kann nur dann existieren, wenn die Anzahl der Kanten im Matching und die Anzahl der nicht gematchten Knoten in der Summe nicht größer ist als $m$.

Aufgrund der Placement-Constraints kann allerdings nicht jedes Item in jedem Stack platziert werden, weshalb noch die bestmögliche Zuweisung von
Items und Item-Paaren zu Stacks gefunden werden muss. Diese entspricht einer Zuweisung, in welcher sämtliche Items möglichst günstig einem
kompatiblen Stack zugeordnet werden. Dazu wird ein bipartiter Graph $G_1 = (V_1 \cup V_2, A)$ eingeführt, dessen erste Partition $V_1$ die Items bestehend aus Item-Paaren und Unmatched-Items enthält während die zweite Partition $V_2$ die zur Verfügung stehenden Stacks beinhaltet.
Eine Kante $\{i, q\} \in A$ zwischen einem Knoten $i \in V_1$ und einem Knoten $q \in V_2$ gibt an, dass das entsprechende Item oder Item-Paar $i$ dem jeweiligen Stack $q$ zugeordnet werden kann. Da die Placement-Constraints nicht direkt durch ein Fehlen der entsprechenden Kante in $G_1$, sondern indirekt durch ein hohes Gewicht ebendieser Kante umgesetzt werden, existieren Kanten zwischen sämtlichen Knoten
aus $V_1$ und denen aus $V_2$, also $\forall i \in V_1, q \in V_2 \thinspace \exists \thinspace \{i, q\} \in A$.
Das Gewicht einer solchen Kante ist definiert als die Kosten $c_{iq}$ des Transports des Items $i$ von seiner Originalposition $O_i$ auf dem Fahrzeug, mit welchem es geliefert wird, zum Stack $q$
in der Storage-Area. Im Fall eines Item-Paars $(i, j) \in V_1$ entspricht das
Gewicht der Kante $\{(i, j), q\} \in A$ der Summe der Kosten des Transports beider Items, also $c_{(i, j)q} = c_{iq} + c_{jq}$.

Da der von \textit{JGraphT} bereitgestellte Algorithmus, welcher zur Berechnung eines \textsc{MWPM} in $G_1$ verwendet wird,
einen vollständig bipartiten Graphen, dessen Partitionen dieselbe Größe besitzen, als Eingabe erwartet und die Summe der Items und Item-Paare
nicht notwendigerweise der Anzahl der verfügbaren Stacks entspricht, werden Dummy-Items eingeführt.
Diese Dummy-Items füllen $V_1$ für den Fall, dass diese Partition kleiner ist, auf.
Übersteigt die Anzahl der Items und Item-Paare die Anzahl der zur Verfügung stehenden Stacks, so existiert,
wie bereits erwähnt, ohnehin keine zulässige Lösung.

Die Dummy-Items haben keinen Einfluss auf die Lösung, da sämtliche Kanten, die ein Dummy-Item mit einem Stack verbinden,
das Gewicht $0$ besitzen und somit keine günstigere Zuweisung eines echten Items oder Item-Paars durch ein Dummy-Item
verhindert wird. Diese Tatsache ergibt sich daraus, dass die Dummy-Items jedem Stack zu den Kosten $0$ zugewiesen
werden können, während eine Zuweisung echter Items und Item-Paare stets Kosten verursacht. Dementsprechend werden
echte Items bei der Zuweisung gewissermaßen bevorzugt während Dummy-Items schlicht den verbleibenden Stacks zugewiesen werden.

Nachdem beide Partitionen dieselbe Größe besitzen, wird ein \textsc{MWPM} in $G_1$ berechnet, dessen Kanten als
Stackzuweisungen interpretiert werden. Da es sich um ein Perfect-Matching handelt, werden sämtliche Items und Item-Paare einem Stack zugewiesen. Die Minimum-Weight-Variante eines Perfect-Matchings sorgt dafür, dass es sich um eine günstigste Zuweisung sämtlicher Items und Item-Paare zu Stacks handelt, da die Kantenkosten jeweils den Transportkosten der entsprechenden Items entsprechen. Sollte das \textsc{MWPM} eine Kante der Kosten $\infty$ enthalten, so existiert keine zulässige Zuweisung der generierten Item-Paare und Unmatched Items zu den Stacks, welche die Placement-Constraints respektiert.

Abschließend muss ggf. die Reihenfolge der Items innerhalb der Stacks angepasst werden, da für die Item-Paare bisher nur sichergestellt ist, dass sie kompatibel und somit in mindestens einer Richtung stackbar sind. Die Richtung, in welcher sie stackbar sind, wurde bei der Zuweisung jedoch noch nicht berücksichtigt. Dies ist trivial möglich, indem für jene Stacks, welche ein Item-Paar beinhalten, überprüft wird, ob die Stacking-Constraints respektiert werden. Ist dies nicht der Fall, so werden die Items vertauscht. Anschließend ist garantiert, dass sämtliche Paare den Stacking-Constraints genügen.

Wie bereits in Abschnitt \ref{sec:objective} erwähnt, bietet die Heuristik über den Parameter
\textit{postProcessing} die Möglichkeit, einen Postprocessing-Schritt zu aktivieren, welcher einen reinen Fokus
auf die Transportkostenminimierung ermöglicht. Wird dieser Wert auf \textit{true} gesetzt, so werden
ausgewählte Items von den ihnen zugewiesenen Positionen an andere, freie Positionen verschoben,
sofern sich dadurch eine Kostenersparnis ergibt. Diese Option sollte in Betracht gezogen werden,
wenn die Anzahl der verwendeten Stacks keine Rolle spielt und lediglich die entstehenden Transportkosten
von Relevanz sind.

\textbf{Postprocessing-Schritt}

Zunächst wird ein bipartiter Postprocessing-Graph $G_p = (V_{p1} \cup V_{p2}, E_p)$ generiert, welcher die zuvor bereits platzierten Items $i \in I$ in der Partition $V_{p1}$ und die verbleibenden freien Positionen in den Stacks innerhalb der Storage-Area in der Partition $V_{p2}$ enthält. Eine solche Position entspricht einem Level $l \in \{1, ..., b\}$ innerhalb eines Stacks $q \in Q$. Es existiert eine Kante $\{i, l\} \in E_p$ zwischen einem Item $i \in V_{p1}$ und einer freien Position $l \in V_{p2}$ innerhalb eines Stacks $q \in Q$, wenn das Item $i$ basierend auf den Placement-Constraints zu diesem Stack kompatibel ist, also wenn $t_{iq} = 1$ gilt. Zusätzlich muss das Item $i$ für den Fall, dass sich bereits ein Item $j \in I$ im Stack $q$ befindet, basierend auf den Stacking-Constraints mit diesem Item kompatibel sein, d.h. $s_{ij} = 1$ oder $s_{ji} = 1$.
Auf diese Weise kann nach der Zuweisung eines Items stets erneut eine den Stacking-Constraints entsprechende Anordnung innerhalb der Stacks erreicht werden. Die Kosten einer solchen Kante entsprechen der Kostenersparnis der Zuweisung des Items zum neuen Stack verglichen mit den Kosten der gegenwärtigen Zuweisung dieses Items. Die Kostenersparnis $r_i$, welche sich bei der Zuweisung des Items $i \in I$, welches sich gegenwärtig im Stack $q_1 \in Q$ befindet, zum Stack $q$ ergibt, ist durch $r_i = c_{iq_1} - c_{iq}$ definiert. Eine Erhöhung der Kosten entspricht demnach einer negativen Ersparnis.

Daraufhin wird in $G_p$ ein Maximum-Weight-Bipartite-Matching berechnet, dessen Kanten jene Items mit freien
Positionen in den Stacks verbinden, welche zur insgesamt größten Kostenreduktion führen.
Dabei kann es vorkommen, dass einem zuvor leeren Stack zwei Items zugeordnet werden. Angesichts der Tatsache,
dass nicht gewährleistet ist, dass diese Items bezüglich der Stacking-Constraints miteinander kompatibel sind,
darf jedem Stack nur ein neues Item zugewiesen werden.
Es wird in solchen Fällen jenes Item mit der größeren Kostenersparnis verschoben.
Nachdem die Items an die ihnen zugeordneten Positionen verschoben wurden,
wird die Reihenfolge der Items innerhalb der Stacks, welchen ein zusätzliches Item zugewiesen
wurde ggf. basierend auf den Stacking-Constraints angepasst.
Befand sich ein verschobenes Item zuvor auf dem Ground-Level eines Stacks,
so wird das darüber platzierte Item auf den Ground-Level heruntergezogen,
sodass sich anschließend erneut eine zulässige Stacking-Lösung ergibt.

Dieser Postprocessing-Prozess wird iterativ durchgeführt, bis keine weitere Verringerung des Kostenwerts erzielt wird.

\textbf{Beispiel}

Gegeben sei die Item-Menge $I = \{0, 1, 2, 3, 4, 5, 6\}$ sowie eine Stack-Kapazität von $b = 2$.
Die Anzahl der zur Verfügung stehenden Stacks wird nun, wie in Abschnitt \ref{sec:test_data} geschildert,
durch $m = \ceil{\ceil{\frac{n}{b}} + 0.2 \cdot \ceil{\frac{n}{b}}}$ berechnet und entspricht somit
$m = \ceil{\ceil{\frac{7}{2}} + 0.2 \cdot \ceil{\frac{7}{2}}} = 5$.
Die Stacking-Constraint-Matrix $S$ wird ebenfalls, wie in Abschnitt \ref{sec:test_data} beschrieben, generiert.
Basierend auf dieser wird der in Abb. \ref{fig:stacking_const_graph_example_b=2_a} dargestellte Stacking-Constraint-Graph
für die gegebenen Items eingeführt, auf welchem anschließend das in Abb. \ref{fig:stacking_const_graph_example_b=2_b} in grün dargestellte \textsc{MCM} berechnet wird. Das in rot dargestellte Item $4$ verbleibt dabei als einziges Unmatched-Item.
\begin{figure}[H]
  \centering
    \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.7, transform shape, node distance=3cm]
        \node[state] (A) [thick] {$\boldsymbol{0}$};
        \node[state] (D) [thick, right of=A] {$\boldsymbol{3}$};
        \node[state] (E) [thick, below of=D] {$\boldsymbol{4}$};
        \node[state] (B) [thick, above of=D] {$\boldsymbol{1}$};
        \node[state] (C) [thick, below left = 1cm of E] {$\boldsymbol{2}$};
        \node[state] (F) [thick, below right = 1cm of E] {$\boldsymbol{5}$};
        \node[state] (G) [thick, right of=D] {$\boldsymbol{6}$};

        \path
            (A) edge [thick] node {} (B)
            (A) edge [thick] node {} (D)
            (A) edge [thick, bend right = 35] node {} (G)
            (A) edge [thick] node {} (E)
            (B) edge [thick, bend right = 75, distance=5.5cm] node {} (E)
            (B) edge [thick] node {} (D)
            (B) edge [thick] node {} (G)
            (C) edge [thick] node {} (F)
            (D) edge [thick] node {} (G)
            (E) edge [thick] node {} (G);
    \end{tikzpicture}
    \caption{\textsc{Stacking-Constraint-Graph}}
    \label{fig:stacking_const_graph_example_b=2_a}
    \end{subfigure}
    % \hspace{0.075\textwidth}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.7, transform shape, node distance=3cm]
        \node[state] (A) [thick] {$\boldsymbol{\textcolor{mygreen}{0}}$};
        \node[state] (D) [thick, right of=A] {$\boldsymbol{\textcolor{mygreen}{3}}$};
        \node[state] (E) [thick, below of=D] {$\boldsymbol{\textcolor{red}{4}}$};
        \node[state] (B) [thick, above of=D] {$\boldsymbol{\textcolor{mygreen}{1}}$};
        \node[state] (C) [thick, below left = 1cm of E] {$\boldsymbol{\textcolor{mygreen}{2}}$};
        \node[state] (F) [thick, below right = 1cm of E] {$\boldsymbol{\textcolor{mygreen}{5}}$};
        \node[state] (G) [thick, right of=D] {$\boldsymbol{\textcolor{mygreen}{6}}$};

        \path
            (A) edge [thick, mygreen] node {} (B)
            (A) edge [thick] node {} (D)
            (A) edge [thick, bend right = 35] node {} (G)
            (A) edge [thick] node {} (E)
            (B) edge [thick, bend right = 75, distance=5.5cm] node {} (E)
            (B) edge [thick] node {} (D)
            (B) edge [thick] node {} (G)
            (C) edge [thick, mygreen] node {} (F)
            (D) edge [thick, mygreen] node {} (G)
            (E) edge [thick] node {} (G);
    \end{tikzpicture}
    \caption{\textsc{MCM (Item $4$ unmatched)}}
    \label{fig:stacking_const_graph_example_b=2_b}
    \end{subfigure}
\caption{\textsc{Generierung der Item-Paare}.}
\label{fig:stacking_const_graph_example_b=2}
\end{figure}
Die Kanten des in Abb. \ref{fig:stacking_const_graph_example_b=2_b} in grün dargestellten \textsc{MCM} werden nun,
wie in Abb. \ref{fig:item_pairs_b=2} dargestellt, als kompatible Item-Paare interpretiert. Das als unmatched verbleibende
Item $4$ wird im Folgenden dem Ground-Level eines eigenen Stacks zugewiesen.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.7, transform shape, node distance=3cm, state/.style={circle, draw, minimum size=1.25cm}]
        \node[state] (A) [thick] {\textcolor{mygreen}{$\boldsymbol{0, 1}$}};
        \node[state] (B) [thick, right of=A] {\textcolor{mygreen}{$\boldsymbol{2, 5}$}};
        \node[state] (C) [thick, right of=B] {\textcolor{mygreen}{$\boldsymbol{3, 6}$}};
        \node[state] (D) [thick, right of=C] {\textcolor{red}{$\boldsymbol{4}$}};
        \path ;
  \end{tikzpicture}
  \caption{\textsc{Resultierende Item-Paare.}}
  \label{fig:item_pairs_b=2}
\end{figure}

Nun geht es darum, eine möglichst günstige Zuweisung der Item-Paare und Unmatched-Items zu Stacks zu finden,
welche die Placement-Constraints respektiert. Dazu wird zunächst der vollständig bipartite Graph, welcher in
Abb. \ref{fig:bipartite_graph} dargestellt ist, generiert. Dieser enthält die Items bestehend aus Item-Paaren
und Unmatched-Items in der einen Partition und die Stacks in der anderen.
Da es lediglich $4$ Item-Tupel bei $5$ zur Verfügung stehenden Stacks gibt und damit die Partitionen nicht
dieselbe Größe besitzen, wird ein Dummy-Item $D_0$ eingeführt.
Die Kantengewichte sind der Übersichtlichkeit halber nicht in der Darstellung aufgetragen,
sondern stattdessen der Tabelle in Abb. \ref{fig:item_tuple_costs} zu entnehmen.

\pagebreak

\begin{figure}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{| c | c | c | c | c | c |}
    \hline
     & $\boldsymbol{S_0}$ & $\boldsymbol{S_1}$ & $\boldsymbol{S_2}$ & $\boldsymbol{S_3}$ & $\boldsymbol{S_4}$ \\ \hline
    $\boldsymbol{(0, 1)}$ & $\infty$ & $35.34$ & $47.46$ & $\infty$ & $71.7$ \\ \hline
    $\boldsymbol{(2, 5)}$ & $71.7$ & $59.58$ & $47.46$ & $47.46$ & $\infty$ \\ \hline
    $\boldsymbol{(3, 6)}$ & $\infty$ & $71.7$ & $59.58$ & $\infty$ & $47.46$ \\ \hline
    $\boldsymbol{4}$ & $38.88$ & $32.82$ & $26.76$ & $\infty$ & $\infty$ \\ \hline
    $\boldsymbol{D_0}$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ \\ \hline
\end{tabular}}
\caption{\textsc{Kosten der Stack-Zuweisungen}.}
\label{fig:item_tuple_costs}
\end{figure}

Anschließend wird im bipartiten Graphen aus Abb. \ref{fig:bipartite_graph} das \textsc{MWPM} berechnet,
welches in Abb. \ref{fig:mwpm_b=2} dargestellt ist. Jede Kante entspricht dabei der Zuweisung eines Item-Paars, Unmatched-Items
oder Dummy-Items zu einem Stack.

\begin{figure}[H]
\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.7, transform shape, node distance=1.5cm, state/.style={circle, draw, minimum size=1.25cm}]
        \node[state] (A) [thick] {$\boldsymbol{0, 1}$};
        \node[state] (B) [thick, below of=A] {$\boldsymbol{2, 5}$};
        \node[state] (C) [thick, below of=B] {$\boldsymbol{3, 6}$};
        \node[state] (D) [thick, below of=C] {$\boldsymbol{4}$};
        \node[state] (E) [thick, below of=D] {$\boldsymbol{D_0}$};

        \node[state] (F) [thick, right = 4cm of A] {$\boldsymbol{S_0}$};
        \node[state] (G) [thick, right = 4cm of B] {$\boldsymbol{S_1}$};
        \node[state] (H) [thick, right = 4cm of C] {$\boldsymbol{S_2}$};
        \node[state] (I) [thick, right = 4cm of D] {$\boldsymbol{S_3}$};
        \node[state] (J) [thick, right = 4cm of E] {$\boldsymbol{S_4}$};

        \path
          % from (0,1) to all stacks
          (A) edge [thick] node {} (F)
          (A) edge [thick] node {} (G)
          (A) edge [thick] node {} (H)
          (A) edge [thick] node {} (I)
          (A) edge [thick] node {} (J)

          % from (2,5) to all stacks
          (B) edge [thick] node {} (F)
          (B) edge [thick] node {} (G)
          (B) edge [thick] node {} (H)
          (B) edge [thick] node {} (I)
          (B) edge [thick] node {} (J)

          % from (3,6) to all stacks
          (C) edge [thick] node {} (F)
          (C) edge [thick] node {} (G)
          (C) edge [thick] node {} (H)
          (C) edge [thick] node {} (I)
          (C) edge [thick] node {} (J)

          % from 4 to all stacks
          (D) edge [thick] node {} (F)
          (D) edge [thick] node {} (G)
          (D) edge [thick] node {} (H)
          (D) edge [thick] node {} (I)
          (D) edge [thick] node {} (J)

          % from dummy to all stacks
          (E) edge [thick] node {} (F)
          (E) edge [thick] node {} (G)
          (E) edge [thick] node {} (H)
          (E) edge [thick] node {} (I)
          (E) edge [thick] node {} (J);
  \end{tikzpicture}
  \caption{\textsc{Vollständig bipartiter Graph}}
  \label{fig:bipartite_graph}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.7, transform shape, node distance=1.5cm, state/.style={circle, draw, minimum size=1.25cm}]
        \node[state] (A) [thick] {$\boldsymbol{0, 1}$};
        \node[state] (B) [thick, below of=A] {$\boldsymbol{2, 5}$};
        \node[state] (C) [thick, below of=B] {$\boldsymbol{3, 6}$};
        \node[state] (D) [thick, below of=C] {$\boldsymbol{4}$};
        \node[state] (E) [thick, below of=D] {$\boldsymbol{D_0}$};

        \node[state] (F) [thick, right = 4cm of A] {$\boldsymbol{S_0}$};
        \node[state] (G) [thick, right = 4cm of B] {$\boldsymbol{S_1}$};
        \node[state] (H) [thick, right = 4cm of C] {$\boldsymbol{S_2}$};
        \node[state] (I) [thick, right = 4cm of D] {$\boldsymbol{S_3}$};
        \node[state] (J) [thick, right = 4cm of E] {$\boldsymbol{S_4}$};

        \path
          (A) edge [thick, near start, above right] node {$35.34$} (G)
          (B) edge [thick, near start, above right] node {$47.46$} (I)
          (C) edge [thick, near end, below left] node {$47.46$} (J)
          (D) edge [thick, below right] node {$26.76$} (H)
          (E) edge [thick, near end, below right] node {$0.0$} (F);
  \end{tikzpicture}
  \caption{\textsc{MWPM}}
  \label{fig:mwpm_b=2}
\end{subfigure}
\caption{\textsc{Ermittlung der Stack-Zuweisungen minimaler Kosten}.}
\end{figure}

Sofern das in Abb. \ref{fig:mwpm_b=2} dargestellte \textsc{MWPM} keine Kante der Kosten $\infty$
enthält, handelt es sich um eine günstigste, zulässige Zuweisung sämtlicher Items und Item-Paare
zu Stacks. Andernfalls existiert keine solche Zuweisung für die gegebenen Paare und Unmatched-Items.
Im betrachteten Beispiel wurde demnach eine zulässige Lösung der Kosten $157.02$ gefunden.
Die entsprechende Interpretation des \textsc{MWPM} als zulässige Stack-Zuweisungen ist in Abb.
\ref{fig:stacking_solution} visualisiert.

\begin{figure}[H]
  \centering
  \resizebox{0.35\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|}
    \cline{2-6}
    $\boldsymbol{L_1}$ & $$ & $1$ & $$ & $5$ & $6$ \\ \cline{2-6}
    $\boldsymbol{L_0}$ & $$ & $0$ & $4$ & $2$ & $3$ \\ \cline{2-6}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\boldsymbol{S_0}$} & \multicolumn{1}{c}{$\boldsymbol{S_1}$}
    & \multicolumn{1}{c}{$\boldsymbol{S_2}$} & \multicolumn{1}{c}{$\boldsymbol{S_3}$} & \multicolumn{1}{c}{$\boldsymbol{S_4}$} \\
    \end{tabular}}
    \caption{\textsc{Zulässige Stack-Zuweisungen.}}
    \label{fig:stacking_solution}
\end{figure}

Zuletzt muss ggf. noch die Reihenfolge innerhalb der Stacks angepasst werden, da es sich bisher nur um zulässige Zuweisungen,
nicht aber um eine zulässige Ordnung der Items innerhalb der Stacks handelt.
Da jedoch sichergestellt ist, dass die Item-Paare kompatibel, d.h. in mindestens einer Richtung stackbar sind,
ist dies trivial in der in Abschnitt \ref{sec:two_cap_heuristic} beschriebenen Weise möglich.

\pagebreak

Ist der Postprocessing-Schritt aktiviert, so werden zuletzt ausgewählte Items an freie Positionen in anderen Stacks
verschoben, sofern sich dadurch eine Kostenersparnis ergibt. Dazu wird der bipartite Postprocessing-Graph generiert, welcher in
Abb. \ref{fig:post_processing_graph_two_cap} dargestellt ist. Dieser enthält die Items in der einen Partition
und die verbleibenden freien Positionen in der anderen Partition. Die Positionen sind jeweils in der Notation
$pos(stackIdx, level)$ gegeben.
In diesem Beispiel gibt es mit $pos(0, 0), pos(0, 1)$ und $pos(2, 1)$ drei verbleibende freie Positionen.
Da sämtliche Kantengewichte im Postprocessing-Graphen negativ oder neutral sind, kommt durch keine Item-Verschiebung
eine Kostenersparnis zustande und es ist nicht möglich, die zuvor generierte Lösung im Postprocessing-Schritt zu
verbessern, weshalb die Zuweisungen aus Abb. \ref{fig:stacking_solution} unverändert bestehen bleiben.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.5, transform shape, node distance=2.5cm, state/.style={circle, draw, minimum size=2cm}]
        \node[state] (A) [thick] {$\boldsymbol{0}$};
        \node[state] (B) [thick, below of=A] {$\boldsymbol{1}$};
        \node[state] (C) [thick, below of=B] {$\boldsymbol{2}$};
        \node[state] (D) [thick, below of=C] {$\boldsymbol{3}$};
        \node[state] (E) [thick, below of=D] {$\boldsymbol{4}$};
        \node[state] (F) [thick, below of=E] {$\boldsymbol{5}$};
        \node[state] (G) [thick, below of=F] {$\boldsymbol{6}$};

        \node[state] (H) [thick, right = 8cm of B] {$\boldsymbol{pos(0, 0)}$};
        \node[state] (I) [thick, right = 8cm of D] {$\boldsymbol{pos(0, 1)}$};
        \node[state] (J) [thick, right = 8cm of F] {$\boldsymbol{pos(2, 1)}$};

        \path

            (A) edge [thick, very near start, above right] node {$-6.06$} (J)
            (B) edge [thick, near end, above] node {$-6.06$} (H)
            (B) edge [thick, very near end, above right] node {$-6.06$} (I)
            (B) edge [thick, very near end, below left] node {$-6.06$} (J)
            (C) edge [thick, above] node {$-6.06$} (H)
            (C) edge [thick, very near start, below left] node {$-6.06$} (I)
            (D) edge [thick, very near start, above left] node {$-12.12$} (H)
            (D) edge [thick, very near start, above] node {$-12.12$} (I)
            (E) edge [thick, very near start, above left] node {$-12.12$} (H)
            (E) edge [thick, above] node {$-12.12$} (I)
            (E) edge [thick, near end, below] node {$0.0$} (J)
            (F) edge [thick, very near end, below right] node {$-18.18$} (H)
            (F) edge [thick, very near start, below right] node {$-18.18$} (I)
            (G) edge [thick, above] node {$-12.12$} (J);
\end{tikzpicture}

\caption{\textsc{Postprocessing-Graph}.}
\label{fig:post_processing_graph_two_cap}
\end{figure}




\subsection{Vergleich der $b = 2$ Solver}
\label{sec:solver_comp_b=2}

In diesem Kapitel werden die verschiedenen Solver für Instanzen von Stacking-Problemen mit einer Stack-Kapazität von $b=2$ miteinander verglichen. Die relevanten Kategorien sind dabei der im Folgenden als Instance-Coverage bezeichnete prozentuale Anteil an gelösten Instanzen pro Solver und Test-Instanz-Konfiguration, die Laufzeit und die jeweilige prozentuale Abweichung des Zielfunktionswerts vom Optimum.

Die in Kapitel \ref{sec:mip_formulations} vorgestellten MIP-Formulierungen wurden mit dem Java-Interface von CPLEX 12.8.0 implementiert
und gelöst. Dabei wurde CPLEX über den Parameter \textsc{CPX\_MIPEMPHASIS\_FEASIBILITY} so konfiguriert, dass die Zulässigkeit
Priorität vor der Optimalität hat.\cite{IBM_DOC}

Die in dieser Arbeit entwickelten Heuristiken wurden ebenfalls in der Programmiersprache Java implementiert
und sämtliche Experimente wurden auf einem Rechner mit Intel Core i5-8500 (6x 3.00GHz) Prozessor und 32 GB RAM durchgeführt.

\pagebreak

\textbf{Kleine Instanzen (s)}

Zunächst zum experimentellen Vergleich der Solver für in Kapitel \ref{sec:test_data} als klein definierte Instanzen,
bei denen es darum geht, $100$ eintreffende Items in die Storage-Area zu verladen.

Die Diagramme in Abb. \ref{fig:instance_cov_b=2_s} zeigen jeweils die Instance-Coverage der einzelnen Solver.
In \ref{fig:instance_cov_b=2_s_a} handelt es sich um den prozentualen Anteil der gelösten Instanzen pro Solver bei einem Zeitlimit
von $1s$ pro Instanz, in \ref{fig:instance_cov_b=2_s_b} bei einem Zeitlimit von $3s$ pro Instanz und in \ref{fig:instance_cov_b=2_s_c}
schließlich bei $5s$ pro Instanz.\newline
Auf der $y$-Achse ist jeweils der prozentuale Anteil der gelösten Instanzen aufgetragen und auf der $x$-Achse der jeweilige Solver.
Bei der Bin-Packing- und der 3-Index-Formulierung handelt es sich um keine Solver im eigentlichen Sinne,
sondern um MIP-Formulierungen, welche von CPLEX als Solver gelöst werden.
Im Folgenden wird der Einfachheit halber jedoch immer von drei Solvern gesprochen, womit die von
CPLEX gelösten MIP-Formulierungen und die Heuristik gemeint sind.
Die Solver werden in den Diagrammen und Abbildungen im Folgenden stets als \textsc{BinP} (Bin-Packing),
\textsc{3Idx} (3-Index) und \textsc{2Cap} (Heuristik für $b = 2$) abgekürzt.

Wie bereits in Kapitel \ref{sec:test_data} erläutert, wurden jeweils $20$ Instanzen pro Konfiguration betrachtet,
d.h. die im Folgenden betrachteten Ergebnisse basieren auf der Lösung von in diesem Fall $20$ kleinen Instanzen.

Wie man Abb. \ref{fig:instance_cov_b=2_s} entnehmen kann, löst die Heuristik bei allen betrachteten Zeitlimits sämtliche Instanzen,
während die 3-Index-Formulierung beispielsweise bei einem Zeitlimit von $1s$ nur $50 \%$ der Instanzen löst.
Die Bin-Packing-Formulierung löst bei diesem Zeitlimit sogar überhaupt keine Instanz.\newline
Die wesentliche Erkenntnis aus Abb. \ref{fig:instance_cov_b=2_s} ist allerdings, dass sämtliche Instanzen bereits bei einem Zeitlimit von $5s$
von allen Solvern gelöst werden, was darauf hindeutet, dass der Bedarf für eine Laufzeitverbesserung durch eine Heuristik hier
aus praktischer Perspektive eher gering sein dürfte.

\begin{figure}[H]
\centering

\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=2_s_1s.png}
\caption{\textsc{Zeitlimit 1s}}
\label{fig:instance_cov_b=2_s_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=2_s_3s.png}
\caption{\textsc{Zeitlimit 3s}}
\label{fig:instance_cov_b=2_s_b}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=2_s_5s.png}
\caption{\textsc{Zeitlimit 5s}}
\label{fig:instance_cov_b=2_s_c}
\end{subfigure}

\caption{\textsc{Instance-Coverage der $b=2$ Solver (s)}}
\label{fig:instance_cov_b=2_s}
\end{figure}

\pagebreak

In Abb. \ref{fig:b=2_s_runtimes} sind die Laufzeiten der Solver pro Instanz in Sekunden bei einem Zeitlimit von $5s$ dargestellt.
Man erkennt, dass die 3-Index-Formulierung deutlich schneller ist als die Bin-Packing-Formulierung und, dass die Heuristik beide MIP-Formulierungen noch einmal deutlich unterbietet. Da in der Abb. nur die absoluten Laufzeitdifferenzen zu sehen sind,
ist es wichtig, zu erwähnen, dass die Laufzeiten der Heuristik durchschnittlich um den Faktor $55.0$ kleiner sind,
als die der 3-Index-Formulierung. Zwischen der 3-Index- und der Bin-Packing-Formulierung liegt dagegen nur ein Faktor von $2.7$.

Aufgrund der Tatsache, dass CPLEX nur dann vor dem Zeitlimit terminiert, wenn die Instanz optimal gelöst wurde, ist klar
ersichtlich, dass die 3-Index-Formulierung sämtliche Instanzen nach etwa $1s$ optimal löst während die Bin-Packing-Formulierung in wenigen Fällen durch das Zeitlimit gestoppt wird.\newline
Bei den Instanzen $01$ und $08$, bei denen die Bin-Packing-Formulierung durch das Zeitlimit gestoppt wurde und dementsprechend nicht
notwendigerweise eine optimale Lösung gefunden hat, konnte die Heuristik, wie man Abb. \ref{fig:b=2_s_costs} entnehmen kann, sogar
bessere Zielfunktionswerte erzielen als die Bin-Packing-Formulierung.\newline
Die weiteren Zielfunktionswerte der Bin-Packing-Formulierung sind in Abb. \ref{fig:b=2_s_costs}  deshalb nicht zu erkennen,
weil sie durch die Zielfunktionswerte der 3-Index-Formulierung verdeckt werden. Da beide Formulierungen die entsprechenden
Instanzen optimal lösen, kommen sie jeweils zum selben Zielfunktionswert.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_time_b=2_s_5s.png}
\caption{\textsc{Laufzeiten}}
\label{fig:b=2_s_runtimes}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_cost_b=2_s_5s.png}
\caption{\textsc{Kosten}}
\label{fig:b=2_s_costs}
\end{subfigure}
\caption{\textsc{Ergebnisse bei $5s$ Zeitlimit}.}
\end{figure}

Die exakten Ergebnisse der MIP-Formulierungen sind den Tabellen in Abb. \ref{fig:table} zu entnehmen.
\textquote{Optimal} bezeichnet den prozentualen Anteil der Instanzen, die optimal gelöst wurden. Bei der Laufzeit handelt
es sich um die durchschnittliche Laufzeit pro Instanz in Sekunden und die Abweichung gibt die durchschnittliche prozentuale Abweichung
vom optimalen Zielfunktionswert pro Instanz an.

\begin{figure}[H]
\centering

\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{red}{0 \%}$ & $ \textcolor{mygreen}{35 \%}$ \\ \hline
    \textbf{Laufzeit} & $\textcolor{red}{---}$ & \O $\thinspace \textcolor{mygreen}{0.9 s}$ \\ \hline
    \textbf{Abweichung} & $\textcolor{red}{---}$ & \O $\thinspace \textcolor{mygreen}{2.0 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $1s$}
\label{fig:res_b=2_s_a}
\end{subfigure}
% $\quad\quad\quad\quad$
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{red}{10 \%}$ & $ \textcolor{mygreen}{100 \%}$ \\ \hline
    \textbf{Laufzeit} & \O $\thinspace \textcolor{red}{3.0 s}$ & \O $\thinspace \textcolor{mygreen}{1.1 s}$ \\ \hline
    \textbf{Abweichung} & \O $\thinspace \textcolor{red}{4.7 \%}$ & \O $\thinspace \textcolor{mygreen}{0.0 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $3s$}
\label{fig:res_b=2_s_b}
\end{subfigure}
% \end{figure}
% \begin{figure}[H]
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{red}{90 \%}$ & $ \textcolor{mygreen}{100 \%}$ \\ \hline
    \textbf{Laufzeit} & \O $\thinspace \textcolor{red}{3.9 s}$ & \O $\thinspace \textcolor{mygreen}{1.1 s}$ \\ \hline
    \textbf{Abweichung} & \O $\thinspace \textcolor{red}{0.4 \%}$ & \O $\thinspace \textcolor{mygreen}{0.0 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $5s$}
\label{fig:res_b=2_s_c}
\end{subfigure}

\caption{\textsc{MIP-Ergebnisse}}
\label{fig:table}
\end{figure}

Abbildung \ref{fig:table} zeigt, dass die 3-Index-Formulierung bei jedem betrachteten Zeitlimit eine geringere Laufzeit benötigt
als die Bin-Packing-Formulierung und somit zu besseren Ergebnissen kommt.

Da die Zeitlimits aufgrund der geringen Laufzeit für die Heuristik keine Rolle spielen, kommt diese stets zum selben Ergebnis.
Sie weicht durchschnittlich um $2.0 \%$ vom Optimum ab und benötigt dafür eine Laufzeit von nur $0.02s$.

Trotz dieser guten Ergebnisse der Heuristik, besteht aufgrund der Tatsache, dass die 3-Index-Formulierung sämtliche Instanzen
mit einer durchschnittlichen Laufzeit von $1.1s$ optimal löst, aus praktischer Perspektive vermutlich kein großer Bedarf für
eine Laufzeitverbesserung durch eine Heuristik.

\textbf{Mittelgroße Instanzen (m)}

Interessanter wird es bereits beim Vergleich der Solver für in Kapitel \ref{sec:test_data} als mittelgroß bezeichnete Instanzen,
bei denen es darum geht, $300$ eintreffende Items in die Storage-Area zu verladen.
In Abb. \ref{fig:instance_cov_b=2} ist erneut die Instance-Coverage der einzelnen Solver dargestelllt.

In Abb. \ref{fig:instance_cov_b=2_m_a} sieht man, dass die Bin-Packing-Formulierung nach einer Minute noch keine Instanz gelöst hat, während die
3-Index-Formulierung dort bereits $80 \%$ der Instanzen löst. Selbst bei einem Zeitlimit von $10$ Minuten pro Instanz löst die Bin-Packing-Formulierung nur $20 \%$ der Instanzen, 3-Index löst bei diesem Zeitlimit sämtliche Instanzen
(vgl. Abb. \ref{fig:instance_cov_b=2_m_b}).
Bei einem Zeitlimit von $20$ Minuten löst schließlich auch die Bin-Packing-Formulierung sämtliche Instanzen
(vgl. Abb. \ref{fig:instance_cov_b=2_m_c}). Die Heuristik löst, wie bereits in der Kategorie der kleinen Instanzen,
in jedem Fall sämtliche Instanzen.
\begin{figure}[H]
\centering

\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=2_m_60s.png}
\caption{\textsc{Zeitlimit} $1min$}
\label{fig:instance_cov_b=2_m_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=2_m_600s.png}
\caption{\textsc{Zeitlimit} $10min$}
\label{fig:instance_cov_b=2_m_b}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=2_m_1200s.png}
\caption{\textsc{Zeitlimit} $20min$}
\label{fig:instance_cov_b=2_m_c}
\end{subfigure}
\caption{\textsc{Instance-Coverage der $b=2$ Solver (m)}}
\label{fig:instance_cov_b=2}
\end{figure}

Da die Zeitlimits aufgrund der geringen Laufzeit der Heuristik für diese erneut keine Rolle spielen,
kommt sie stets zum selben Ergebnis. Sie weicht durchschnittlich um $0.8 \%$ vom
Optimum ab und benötigt dafür eine Laufzeit von $0.1s$.

\pagebreak

In Abb. \ref{fig:res_plots_b=2_m} sind die Laufzeiten und ermittelten Zielfunktionswerte der Solver bei einem Zeitlimit von $20$
Minuten dargestellt. Da die Bin-Packing- und die 3-Index-Formulierung bei diesem Zeitlimit sämtliche Instanzen optimal lösen,
kommen sie stets zum selben Kostenwert (vgl. Abb \ref{fig:b=2_m_costs}),
wobei die Bin-Packing-Einträge in der Visualisierung erneut durch die Einträge der 3-Index-Formulierung verdeckt werden.
In Abb. \ref{fig:b=2_m_runtimes} ist gut zu erkennen, dass die Bin-Packing-Formulierung deutlich längere Laufzeiten als die 3-Index-Formulierung
und die Heurisktik benötigt, wobei die Heuristik noch einmal deutlich schneller ist. Dabei handelt es sich allerdings um die absoluten Laufeiten.
Im Wesentlichen ist der Laufzeitunterschied zwischen der Heuristik und der 3-Index-Formulierung deutlich größer, als jener zwischen den beiden
MIP-Formulierungen. Zwischen der Heuristik, welche durchschnittlich $0.1s$ benötigt und der 3-Index-Formulierung, die durchschnittlich $107.8s$ benötigt (vgl. Abb. \ref{fig:res_b=2_m_c}), besteht ein Faktor von $1077.0$ während zwischen den beiden MIP-Formulierungen nur ein Faktor von
$7.1$ besteht. Die absolute Darstellung ist in diesem Fall also etwas irreführend, da die Heuristik deutlich schneller ist als beide MIP-Formulierungen. Trotzdem würde man in diesem Fall aus praktischer Perspektive die 3-Index-Formulierung klar der Bin-Packing-Formulierung vorziehen, dieser Sachverhalt ist in Abb. \ref{fig:b=2_m_runtimes} gut erkennbar.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_time_b=2_m_1200s.png}
\caption{\textsc{Laufzeiten}}
\label{fig:b=2_m_runtimes}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_cost_b=2_m_1200s.png}
\caption{\textsc{Kosten}}
\label{fig:b=2_m_costs}
\end{subfigure}
\caption{\textsc{Ergebnisse bei $20min$ Zeitlimit}.}
\label{fig:res_plots_b=2_m}
\end{figure}

In den Tabellen in Abb. \ref{fig:res_b=2_m} können die exakten Ergebnisse der MIP-Formulierungen nachvollzogen werden.
Wie bereits erwähnt, hat die Bin-Packing-Formulierung bei einem Zeitlimit von einer Minute keine Instanz gelöst, die 3-Index-Formulierung löst
$80 \%$ der Instanzen, allerdings nur $50 \%$ optimal (vgl. \ref{fig:res_b=2_m_a}). Die 3-Index-Formulierung benötigt dafür durchschnittlich
$55.4s$ und weicht, genau wie die Heuristik, um durchschnittlich $0.8 \%$ vom Optimum ab. Die Heuristik löst dabei allerdings sämtliche Instanzen
und benötigt nur durchschnittlich $0.1s$.

\begin{figure}[H]
% \end{figure}
% \begin{figure}[H]
% $\quad\quad\quad\quad$
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{red}{0 \%}$ & $ \textcolor{mygreen}{50 \%}$ \\ \hline
    \textbf{Laufzeit} & $\textcolor{red}{----}$ & \O $\thinspace \textcolor{mygreen}{55.4 s}$ \\ \hline
    \textbf{Abweichung} & $\textcolor{red}{----}$ & \O $\thinspace \textcolor{mygreen}{0.8 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $1min$}
\label{fig:res_b=2_m_a}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{red}{10 \%}$ & $ \textcolor{mygreen}{100 \%}$ \\ \hline
    \textbf{Laufzeit} & \O $\thinspace \textcolor{red}{581.6 s}$ & \O $\thinspace \textcolor{mygreen}{73.7 s}$ \\ \hline
    \textbf{Abweichung} & \O $\thinspace \textcolor{red}{0.6 \%}$ & \O $\thinspace \textcolor{mygreen}{0.0 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $10min$}
\label{fig:res_b=2_m_b}
\end{subfigure}
% \end{figure}
% \begin{figure}[H]
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ 100 \%$ & $ 100 \%$ \\ \hline
    \textbf{Laufzeit} & \O $\thinspace 869.6 s$ & \O $\thinspace \textcolor{mygreen}{107.8 s}$ \\ \hline
    \textbf{Abweichung} & \O $\thinspace 0.0 \%$ & \O $\thinspace 0.0 \%$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $20min$}
\label{fig:res_b=2_m_c}
\end{subfigure}
\caption{\textsc{MIP-Ergebnisse}}
\label{fig:res_b=2_m}
\end{figure}

Zusammenfassend ist die 3-Index-Formulierung in der Kategorie der mittelgroßen Instanzen der Bin-Packing-Formulierung vorzuziehen,
da sie bei allen betrachteten Zeitlimits die besseren Ergebnisse erzielt und auch bei einem Zeitlimit von $20$ Minuten, bei dem schließlich auch die Bin-Packing-Formulierung sämtliche Instanzen optimal löst, deutlich schneller ist als diese.

Die zu bevorzugende MIP-Formulierung 3-Index, benötigt, wie sich Abb. \ref{fig:res_b=2_m_b} entnehmen lässt, durchschnittlich
$73.7s$ um sämtliche Instanzen optimal zu lösen. Die Heuristik dagegen benötigt durchschnittlich $0.1s$ um sämtliche Instanzen
mit einer durchschnittlichen Abweichung vom Optimum von $0.8 \%$ zu lösen.

Dieser Laufzeitunterschied ist so erheblich, dass die gerine Abweichung vom Optimum in der Praxis vermutlich in Kauf genommen werden kann,
wenn sich dadurch eine derart große Laufzeitersparnis ergibt. In dieser Kategorie stellt sich die Heuristik demzufolge als sehr gute Alternative
heraus.

\textbf{Große Instanzen (l)}

Zuletzt werden die Solver für die in Kapitel \ref{sec:test_data} als groß bezeichneten Instanzen, bei denen es darum geht,
$500$ eintreffende Items in die Storage-Area zu verladen, verglichen. In dieser Kategorie wurden noch einmal größere Zeitlimits von
$15$, $30$ und $45$ Minuten betrachtet.

In Abb. \ref{fig:instance_cov_b=2_l} ist die Instance-Coverage der Solver für die jeweiligen Zeitlimits dargestellt und es fällt auf,
dass die Bin-Packing-Formulierung erst bei einem Zeitlimit von $45$ Minuten überhaupt eine Instanz löst (vgl. \ref{fig:instance_cov_b=2_l_c}).
In diesem Szenario ist die 3-Index-Formulierung der Bin-Packing-Formulierung folglich klar überlegen.

\begin{figure}[H]
\centering

\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=2_l_900s.png}
\caption{\textsc{Zeitlimit} $15min$}
\label{fig:instance_cov_b=2_l_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=2_l_1800s.png}
\caption{\textsc{Zeitlimit} $30min$}
\label{fig:instance_cov_b=2_l_b}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=2_l_2700s.png}
\caption{\textsc{Zeitlimit} $45min$}
\label{fig:instance_cov_b=2_l_c}
\end{subfigure}

\caption{\textsc{Instance-Coverage der $b=2$ Solver (l)}}
\label{fig:instance_cov_b=2_l}
\end{figure}

Überdies ist Abb. \ref{fig:instance_cov_b=2_l} zu enthnehmen, dass die 3-Index-Formulierung erst beim betrachteten
Zeitlimit von $45$ Minuten sämtliche Instanzen gelöst hat. Für die Heuristik sind die Zeitlimits erneut irrelevant,
da sie mit einer durchschnittlichen Laufzeit von $0.7s$ pro Instanz sämtliche Instanzen löst.

\pagebreak

In Abb. \ref{fig:res_plots_b=2_l} bestätigt sich die Tendenz aus der Darstellung der Instance-Coverage.
Die Heuristik ist deutlich schneller als die 3-Index-Formulierung (vgl. Abb \ref{fig:b=2_l_runtimes}) und weicht dabei nur
unwesentlich vom optimalen Zielfunktionswert ab (vgl. \ref{fig:b=2_l_costs}). Wie bereits zuvor gesehen, löst
die Bin-Packing-Formulierung mit Instanz $09$ nur eine einzige Instanz und reizt dafür, wie man Abb. \ref{fig:b=2_l_runtimes}
entnehmen kann, das Zeitlimit von $45$ Minuten vollständig aus. In Abb. \ref{fig:b=2_l_costs} ist zu erkennen, dass diese auch bei den
Zielfunktionswerten sehr weit von denen der 3-Index-Formulierung und der Heuristik abweicht und somit in dieser Kategorie in keinem Fall
konkurrenzfähig ist.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_time_b=2_l_2700s.png}
\caption{\textsc{Laufzeiten}}
\label{fig:b=2_l_runtimes}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_cost_b=2_l_2700s.png}
\caption{\textsc{Kosten}}
\label{fig:b=2_l_costs}
\end{subfigure}
\caption{\textsc{Ergebnisse bei $45min$ Zeitlimit}.}
\label{fig:res_plots_b=2_l}
\end{figure}

In Abb. \ref{fig:res_b=2_l} sind die exakten Ergebnisse der MIP-Formulierungen aufgeführt, denen man
entnehmen kann, dass die 3-Index-Formulierung mit einer durchschnittlichen Laufzeit von ca. $20$ Minuten sämtliche Instanzen
optimal löst (vgl. Abb. \ref{fig:res_b=2_l_c}). Die Heuristik benötigt allerdings nur durchschnittlich $0.7s$ pro Instanz und
weicht auch nur um im Durchschnitt $0.6 \%$ vom Optimalwert ab. Dieser, verglichen mit der Kategorie der mittelgroßen Instanzen,
noch gravierende Laufzeitunterschied und die nur sehr geringe Abweichung vom Optimum sorgen dafür, dass die Heuristik in der Praxis
klar bevorzugt werden sollte.

\begin{figure}[H]
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{red}{0 \%}$ & $ \textcolor{mygreen}{70 \%}$ \\ \hline
    \textbf{Laufzeit} & \textcolor{red}{$---$} & \O $\thinspace \textcolor{mygreen}{728.0 s}$ \\ \hline
    \textbf{Abweichung} & \textcolor{red}{$---$} & \O $\thinspace \textcolor{mygreen}{0.53 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $15min$}
\label{fig:res_b=2_l_a}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{red}{0 \%}$ & $ \textcolor{mygreen}{85 \%}$ \\ \hline
    \textbf{Laufzeit} & \textcolor{red}{$---$} & \O $\thinspace \textcolor{mygreen}{977.2 s}$ \\ \hline
    \textbf{Abweichung} & \textcolor{red}{$---$} & \O $\thinspace \textcolor{mygreen}{0.0 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $30min$}
\label{fig:res_b=2_l_b}
\end{subfigure}
% $\quad\quad\quad\quad$
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{red}{0 \%}$ & $ \textcolor{mygreen}{100 \%}$ \\ \hline
    \textbf{Laufzeit} & \textcolor{red}{$---$} & \O $\thinspace \textcolor{mygreen}{1185.9  s}$ \\ \hline
    \textbf{Abweichung} & \textcolor{red}{$---$} & \O $\thinspace \textcolor{mygreen}{0.0 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $45min$}
\label{fig:res_b=2_l_c}
\end{subfigure}

\caption{\textsc{MIP-Ergebnisse}}
\label{fig:res_b=2_l}
\end{figure}

\textbf{Fazit}

Insgesamt ist festzuhalten, dass bei Stacking-Problemen mit Stacks der Kapazität $b = 2$ in der Kategorie der kleinen Instanzen
aufgrund der geringen Laufzeit der MIP-Formulierungen kein wirklicher Bedarf für eine Laufzeitverbesserung besteht.
Anders in den Kategorien der mittelgroßen und großen Instanzen, bei welchen die Heuristik bei nur kleinen Abweichungen zu einer
derart großen Laufzeiteinparung führt, dass sie eine in der Praxis relevante Alternative darstellt.

\subsection{Konstruktive Heuristik ($b = 3$)}
\label{sec:three_cap_heuristic}

Die konstruktive Heuristik zur Lösung von Stacking-Problemen mit Stacks der Kapazität $b = 3$ funktioniert
an einigen Stellen analog zur Heuristik aus Abschnitt \ref{sec:two_cap_heuristic} für eine Stack-Kapazität von $b = 2$.
Um den zusätzlichen Möglichkeiten und Anforderungen der erhöhten Stack-Kapazität gerecht zu werden, werden überdies
erweiternde Ansätze präsentiert.

Zunächst geht es wie auch in der Heuristik für $b = 2$ darum, kompatible Item-Paare zu bilden. Dazu wird ein
Stacking-Constraint-Graph $G = (V, E)$ generiert, welcher die Items als Knoten enthält, d.h. $V = I$.
Dieser Graph besitzt eine Kante $\{i, j\} \in E$ zwischen zwei Knoten, wenn die entsprechenden Items $i, j \in I$
basierend auf den Stacking-Constraints in mindestens einer Richtung stackbar sind, d.h. wenn $s_{ij} + s_{ji} \geq 1$
und wenn mindestens ein Stack existiert, in welchem die beiden Items gemeinsam platziert werden können ohne die Placement-Constraints
zu verletzen, also wenn $c_{iq} < \infty$ und $c_{jq} < \infty$ für mindestens einen Stack $q \in Q$ gilt.

Berechnet man nun ein \textsc{MCM} $M \subseteq E$ in $G$ und interpretiert dessen Kanten $\{i, j\} \in M$ als Paare von kompatiblen Items
$i, j \in I$, so erhält man eine größtmögliche Anzahl von Item-Paaren, die in mindestens einer Richtung stackbar sind.
In der Regel ist anschließend die Mehrheit oder sogar die Gesamtheit der Items einer Instanz Teil eines Paares.
Dies hängt allerdings von den gegebenen Stacking-Constraints ab und gilt nur für die zweite Variante der Stacking-Constraint-Generierung, welche in Kapitel \ref{sec:test_data} beschrieben wird. Da diese jedoch realitätsnäher zu sein scheint und daher auch in sämtlichen Experimenten zum Einsatz kommt, ist davon auszugehen, dass an dieser Stelle keine oder wenige Unmatched-Items verbleiben.
Dieser Sachverhalt hat dazu geführt, dass sich die weitere Berechnung eines \textsc{MCM} zwischen Item-Paaren und Unmatched-Items,
um erste Tripel zu bilden, welche in einer früheren Version der Heuristik an dieser Stelle stattfand, als nicht zielführend
erwies.

Da die Stacks nun nicht nur Paare, sondern Tripel von Items beinhalten können, geht es im Folgenden darum,
ebensolche Tripel von kompatiblen Items zu generieren. Dazu werden die zuvor gebildeten Item-Paare zu Item-Tripeln gemergt.
Zunächst muss allerdings entschieden werden, welche dieser Paare sinnvollerweise wieder separiert werden sollten,
um beide Items einem jeweils anderen Paar zuzuweisen und zwei Tripel zu bilden. Dies geschieht mithilfe eines Rating-Systems, welches die Item-Paare mit dem Ziel bewertet, jene Paare zu ermitteln, die im Hinblick auf ein günstiges Ergebnis
und möglichst vollständig gefüllte Stacks getrennt werden sollten.

\textbf{Rating-System}

Initial wird jede Kante $\{i, j\} \in M$, also jedes Item-Paar, mit dem Basis-Rating $0$ bewertet.
Anschließend wird für jedes dieser Paare die Kompatibilität seiner Items $i, j \in I$ zu anderen Paaren aus $M \textbackslash \{i, j\}$ ermittelt.
Dementsprechend wird für jedes Item, welches Teil eines Paares ist, die Anzahl der verbleibenden Paare bestimmt, zu denen es kompatibel ist, denen es also zugewiesen werden kann, um ein Tripel zu bilden. Diese Kompatibilität bezieht sich auf die Stacking-Constraints.
Eine Menge von Items ist zueinander kompatibel, wenn diese basierend auf den Stacking-Constraints in mindestens einer
Reihenfolge gestapelt werden kann.

Geht es beispielsweise darum, zu überprüfen, ob ein Item $i \in I$ einem Paar $(s, t)$, welches aus einer Kante $\{s, t\} \in M$ resultiert,
zugewiesen werden kann, so ergeben sich die in Abb. \ref{fig:triple_permutations} dargestellten $3!$ möglichen Permutationen eines Tripels. Stellt sich eine dieser Permutationen als konform mit den Stacking-Constraints heraus, so kann das Item $i$ zusammen mit dem Paar $(s, t)$ ein zulässiges Tripel bilden.
\begin{figure}[H]
\[
\begin{bmatrix}
  \boldsymbol{i} & s & s & \boldsymbol{i} & t & t \\
  s & \boldsymbol{i} & t & t & \boldsymbol{i} & s \\
  t & t & \boldsymbol{i} & s & s & \boldsymbol{i}
\end{bmatrix}
\]
\caption{\textsc{Item-Tripel Permutationen.}}
\label{fig:triple_permutations}
\end{figure}
Sind die Items $s, t \in I$, welche das betrachtete Paar bilden, in beide Richtungen stackbar, also $s_{st} + s_{ts} = 2$, so sind sämtliche
Permutationen zu überprüfen.
Aufgrund der Transitivität der Stacking-Constraints ist das Item $i$ in diesem Fall allerdings bereits dann kompatibel zum Paar $(s, t)$,
wenn $s_{is} + s_{it} + s_{si} + s_{ti} \geq 1$ gilt.
Darf Item $s$ auf Item $t$ gestapelt werden, jedoch nicht andersherum, also $s_{st} = 1$ und $s_{ts} = 0$,
so müssen die ersten drei Permutationen überprüft werden. Item $i$ ist dann kompatibel zum Paar $(s, t)$,
wenn $s_{is} + s_{ti} \geq 1$ oder $s_{si} + s_{it} = 2$ gilt.
Gilt umgekehrt, dass lediglich Item $t$ auf Item $s$ gestapelt werden darf, also $s_{st} = 0$ und $s_{ts} = 1$, so
müssen die letzten drei Permutationen überprüft werden und Item $i$ ist kompatibel zum Paar $(s, t)$,
wenn $s_{it} + s_{si} \geq 1$ oder $s_{ti} + s_{is} = 2$ gilt.\newline
Auf diese Weise wird die Kompatibilität sämtlicher Items, welche Teil eines Paares sind, zu den verbleibenden Paaren bestimmt.
Ausschlaggebend ist dabei jeweils die Anzahl der kompatiblen Paare des weniger kompatiblen Items eines Paares,
da stets beide Items erneut zugewiesen werden müssen. Diese wird im Folgenden als \textquote{Worst-Compatibility}
$w_{ij}$ des Items $(i, j)$ bezeichnet.

Des Weiteren werden für jedes Item-Paar $(i, j)$ die durchschnittlichen Kosten einer gemeinsamen Stack-Zuweisung berechnet.
Zu diesem Zweck werden zunächst die durchschnittlichen Kosten $a$ einer zulässigen Zuweisung eines Items zu einem Stack berechnet,
also der Durchschnitt aller Einträge $c_{iq}$ für Items $i \in I$ und Stacks $q \in Q$ in der Transportkostenmatrix $C$, für
welche $t_{iq} = 1$ gilt. Dabei entspricht $c_{(i, j)q}$ jeweils den Kosten der gemeinsamen Zuweisung der Items $i$ und $j$ zu Stack $q$,
welche wie folgt definiert sind:
\[
    c_{(i, j)q} \quad := \quad
\begin{cases}
    c_{iq} + c_{jq}, & \text{wenn $t_{iq} = 1$ und $t_{jq} = 1$}\\
    5a, & \text{sonst}
\end{cases}
\]
D.h. die Kosten der Zuweisung entsprechen nur dann der Summe der beiden Einzelzuweisungen, wenn beide Items des Paares basierend
auf den Placement-Constraints kompatibel zum Stack sind. Ist dies nicht der Fall, so wird die Inkompatibilität durch
einen Kostenwert von $5a$, also dem $5$-fachen der Kosten einer durchschnittlichen Item-Stack-Zuweisung, bestraft. Der durchschnittliche Kostenwert $avgC_{ij}$ für das Item-Paar $(i, j)$ ergibt sich folgendermaßen:
\[ avgC_{ij} = \frac{\sum_{q \in Q} (c_{(i, j)q})}{|Q|}  \]

Anschließend wird über sämtliche Paare, die sich aus den Kanten $\{i, j\} \in M$ ergeben, hinweg eine durchschnittliche \textquote{Worst-Compatibility} $w$, also eine durchschnittliche Kompatibilität des jeweils weniger kompatiblen Items, sowie ein durchschnittlicher Kostenwert $avgC$ für Stackzuweisungen berechnet. Wenn ein Paar $(i, j)$ nun sowohl eine über dem Durchschnitt liegende \textquote{Worst-Compatibility} $w_{ij} > w$ als auch einen über dem Durchschnitt liegenden durchschnittlichen Kostenwert $avgC_{ij} > avgC$ besitzt, scheint es gut geeignet für eine Separation,
denn es ist überdurchschnittlich flexibel für eine erneute Zuweisung und seine gegenwärtige Zuweisung ist ohnehin überdurchschnittlich teuer.
Dabei ist selbstverständlich nicht garantiert, dass für diese Items nach der Trennung eine günstigere Zuweisung existiert.

Wenn die Summe der prozentualen Abweichungen $d_{ij}$ beider Werte des Paares $(i, j)$ vom Durchschnitt einen gewissen Schwellenwert,
welcher im Folgenden als \textquote{Deviation-Threshold} $d_t$ bezeichnet wird,
überschreitet, entspricht das Rating $r_{ij}$ des Paares ebendieser Summe.
Die Summe der prozentualen Abweichungen $d_{ij}$ ist wie folgt definiert:
\[ d_{ij} = \frac{|avgC - avgC_{ij}|}{avgC \cdot 100} + \frac{|w - w_{ij}|}{w \cdot 100}\]
Das Rating $r_{ij}$ des Paares $(i, j)$ ergibt sich dementsprechend folgendermaßen:
\[
    r_{ij} \quad := \quad
\begin{cases}
    d_{ij}, & \text{wenn $d_{ij} > d_t$}\\
    0, & \text{sonst}
\end{cases}
\]
Die Tatsache, dass das Rating $r_{ij}$ eines Paares $(i, j)$ direkt der Summe seiner Abweichungen $d_{ij}$
vom Durchschnitt entspricht, sofern diese den Schwellenwert $d_t$ überschreitet, hat zur Folge, dass die Höhe der Abweichungen
einen Einfluss auf die Bewertung der Paare hat.

Der \textquote{Deviation-Threshold} $d_t$ wird dabei in einem bestimmten Wertebereich variiert, d.h. der Prozess wird mit
unterschiedlichen Schwellenwerten durchgeführt, die zu unterschiedlich bewerteten Listen der Item-Paare führen.
Dieser Wertebereich wurde experimentell bestimmt und reicht von $d_t = 65 \%$ bis $d_t = 120 \%$ Abweichung in der Summe.
Dabei wird $d_t$ in jeder Iteration um $5 \%$ inkrementiert. Es entstehen demnach $12$ unterschiedlich bewertete Itemlisten zusätzlich zum ursprünglichen Ergebnis des \textsc{MCM}, welches unbeweret und unsortiert betrachtet wird. Diese Listen werden absteigend basierend auf ihrer Bewertung sortiert.

Da die Verwendung des beschriebenen Rating-Systems zu einer durchaus signifikanten Laufzeitverlängerung führt,
wird in der Implementation der Heuristik ein Parameter \textit{prioritizeRuntime} eingeführt, welcher ein Deaktivieren des
Rating-Systems ermöglicht, um eine möglichst geringe Laufzeit zu gewährleisten. Wird dieser Parameter auf \textit{true}
gesetzt, so wird das initiale Ergebnis des \textsc{MCM} als einzige Liste von Item-Paaren im Merge-Prozess verwendet und
resultiert schließlich auch nur in einer einzigen Lösung.
Wird das Rating-System verwendet, so erhöht sich die durchschnittliche Laufzeit pro Instanz in der Kategorie der großen
Instanzen, in welcher es die größte Auswirkung auf die Laufzeiten zeigt, von $0.5 s$ auf $4.2 s$. Dabei verringert
sich der Zielfunktionswert in den unterschiedlichen Kategorien um durchschnittlich $0.007 \% (l), 0.04 \% (m)$
und $0.13 \% (s)$. Trotz der vergleichsweise geringen Verbesserungen der Zielfunktionswerte im Durchschnitt,
ist diese im Einzelfall durchaus relevant. Gerade in Anbetracht der ohnehin bereits geringen Abweichung der Heuristik vom Optimum,
kann diese jenem durch Verwendung des Rating-Systems noch einmal näher kommen. Wenn also eine Laufzeit von wenigen Sekunden, welche
die MIP-Formulierungen weiterhin um Größenordnungen unterbietet, akzeptabel ist, sollte dies in Betracht gezogen werden.

Nachdem die unterschiedlich sortierten Listen generiert wurden, geht es um den tatsächlichen Merge-Prozess.
Die ersten $x$ Paare einer Liste werden separiert, was den ersten $x$ Paaren mit dem höchsten Rating, also der größten
prozentualen Abweichung vom Durchschnitt in der Summe $d_{ij}$ entspricht. Die daraus resultierenden einzelnen Items werden den verbleibenden
$|M| - x$ Paaren zugewiesen um kompatible Tripel zu bilden. Die größte Anzahl an Item-Paaren, die separiert und dessen Items theoretisch den verbleibenden Paaren zugewiesen werden könnten, beträgt $\floor{\frac{|M|}{3}}$. Aufgrund der Stacking Constraints ist es jedoch nicht sinnvoll, die Minimalanzahl verbleibender Paare zu wählen. Dementsprechend sollte $|M|$ durch einen Wert $> 3$ dividiert werden. Gleichzeitig sollte der Wert nicht zu groß gewählt werden, damit möglichst viele Tripel entstehen. Experimentell hat sich $3.4$ als guter Divisor herausgestellt, indem die Lösungen für unterschiedliche $x$ Werte verglichen wurden. Folglich werden die ersten $x = \floor{\frac{|M|}{3.4}}$ Paare separiert.

Nun sollen aus den dabei entstehenden Items zusammen mit den verbleibenden Paaren zulässige Tripel entstehen.
Dazu wird ein bipartiter Graph $G_1 = (V_1 \cup V_2, E_1)$ bestehend aus den Paaren und separierten Items generiert.
Dieser Graph enthält die Paare in der Partition $V_1$ und die separierten Items in der Partition $V_2$.
Es existiert eine Kante zwischen einem Paar $(i, j) \in V_1$ und einem Item $k \in V_2$, wenn das Item $k$ kompatibel zum Paar
$(i, j)$ ist. Die Überprüfung dieser Kompatibilität findet erneut in geschilderter Weise basierend auf den möglichen Permutationen
aus Abb. \ref{fig:triple_permutations} statt.

Anschließend wird in $G_1$ ein \textsc{MCM} berechnet, dessen Kanten einer größtmöglichen Menge an kompatiblen Item-Tripeln entspricht.
Dies geschieht für sämtliche Listen von Item-Paaren basierend auf den unterschiedlichen Sortierungen.
Daraufhin liegt also eine Menge von Listen von Item-Tripeln vor, die aus den unterschiedlich sortierten Listen von Item-Paaren resultiert.
Für jede dieser Listen wird nun der folgende Prozess durchgeführt.

Zunächst werden abermals durch ein \textsc{MCM} in dem für alle verbleibenden Items generierten Stacking-Constraint-Graphen
Paare gebildet, da im Zuge des Merge-Prozesses Unmatched-Items entstehen können, die nicht Teil eines Tripels sind und auch nicht notwendigerweise
sämtliche Items zuvor Teil eines Paares waren. Items, welche nicht inzident zu einer Kante dieses Matchings sind, werden im Folgenden als unmatched betrachtet und auf dem Ground-Level eines eigenen Stacks platziert.

Wie bereits in der Heuristik aus Abschnitt \ref{sec:two_cap_heuristic} werden auch hier auf diese Weise insgesamt möglichst wenige der zur Verfügung stehenden Stacks verwendet, denn wie zuvor ist das primäre Ziel, zu einer zulässigen Lösung zu gelangen, wofür sich die initiale Maximierung der Anzahl der möglichst weit gefüllten Stacks anbietet. Erst nachdem möglichst viele Tripel und Paare von Items generiert wurden, geht es darum, diese zusammen mit den verbleibenden Unmatched-Items möglichst günstig den vorhandenen Stacks zuzuweisen.

An dieser Stelle liegt also eine Menge an Item-Tripeln, eine Menge an Item-Paaren und eine Menge an Unmatched-Items vor. Diese müssen nun
noch in einer die Placement-Constraints respektierenden Weise den Stacks zugewiesen werden.
Dies ist nur dann möglich, wenn die Anzahl der Tripel, Paare und Unmatched-Items in der Summe kleiner oder gleich der Anzahl
der zur Verfügung stehenden Stacks ist.

Ist dies der Fall, so wird zu diesem Zweck ein bipartiter Graph $G_2 = (V_3 \cup V_4, E_2)$ eingeführt,
dessen erste Partition $V_3$ die Items bestehend aus Item-Tripeln, Item-Paaren und Unmatched-Items enthält während die zweite Partition $V_4$
die zur Verfügung stehenden Stacks beinhaltet. Da diese Partitionen nicht notwendigerweise dieselbe Kardinalität besitzen, der von \textit{JGrpahT} bereitgestellte Algorithmus, welcher zur Berechnung eines \textsc{MWPM} in $G_2$ verwendet wird, dies jedoch fordert, werden Dummy-Items
eingeführt, die die Item-Partition $V_3$ gegebenenfalls auffüllen.


Eine Kante $\{i, q\} \in E_2$ zwischen einem Knoten $i \in V_3$ und einem Knoten $q \in V_4$ gibt an, dass das entsprechende Item, Item-Paar oder Item-Tripel $i$ dem jeweiligen Stack $q$ zugeordnet werden kann. Wie zuvor gilt dabei
aus genannten Gründen, dass die Placement-Constraints indirekt durch ein hohes Gewicht einer solchen Kante und nicht durch ein Fehlen dieser
umgesetzt werden. Es existieren daher Kanten zwischen sämtlichen Knoten aus $V_3$ und denen aus $V_4$, d.h. $\forall i \in V_3, q \in V_4
\thinspace \exists \thinspace \{i, q\} \in E_2$, was dafür sorgt, dass es sich bei $G_2$ um einen vollständig bipartiten Graphen handelt.
Das Gewicht einer solchen Kante $\{i, q\} \in E_2$, die ein Tripel, Paar oder Unmatched-Item mit einem Stack verbindet, ist definiert
als die Summe der Kosten des Transports aller an der Zuweisung beteiligten Items zum jeweiligen Stack. Somit ergeben sich drei Fälle.
Der erste Fall behandelt Kanten $\{i, q\} \in E_2$, die ein Unmatched-Item $i \in V_3$ mit einem Stack $q \in V_4$ verbinden.
Das Gewicht einer solchen Kante entspricht den Kosten des Transports $c_{iq}$ des Items $i$ in den Stack $q$.
Fall zwei behandelt Kanten $\{(i, j), q\} \in E_2$, welche ein Item-Paar $(i, j) \in V_3$ mit einem Stack $q \in V_4$
verbinden. Die Kosten einer solchen Kante ergeben sich aus der Summe der Kosten des Transports beider Items,
also $c_{(i, j)q} = c_{iq} + c_{jq}$.
Der dritte und letzte Fall behandelt Kanten $\{(i, j, k), q\} \in E_2$, welche ein Item-Tripel $(i, j, k) \in V_3$
mit einem Stack $q \in V_4$ verbinden. Die Kosten einer solchen Kante ergeben sich aus der Summe der Kosten des Transports
der drei Items, also $c_{(i, j, k)q} = c_{iq} + c_{jq} + c_{kq}$.

Die Dummy-Items haben keinen Einfluss auf die Lösung, da sämtliche Kanten, die ein Dummy-Item mit einem Stack verbinden,
das Gewicht $0$ besitzen und somit keine günstigere Zuweisung eines echten Items, Item-Paars oder Item-Tripels durch ein Dummy-Item
verhindert wird. Diese Tatsache ergibt sich daraus, dass die Dummy-Items jedem Stack zu den Kosten $0$ zugewiesen werden können,
während eine Zuweisung echter Items, Item-Paare und Item-Tripel stets Kosten verursacht. Dementsprechend werden echte Items bei der
Zuweisung gewissermaßen bevorzugt während Dummy-Items schlicht den verbleibenden Stacks zugewiesen werden.

Nachdem beide Partitionen dieselbe Größe besitzen, wird ein \textsc{MWPM} in $G_2$ berechnet, dessen Kanten
als Stack-Zuweisungen interpretiert werden. Da es sich um ein Perfect-Matching handelt, werden sämtliche Items, Item-Paare
und Item-Tripel einem Stack zugewiesen. Die Minimum-Weight-Variante eines Perfect-Matchings sorgt dafür, dass es sich
jeweils um eine günstigste Zuweisung handelt, da die Kantenkosten jeweils den Transportkosten entsprechen.
Sollte das \textsc{MWPM} eine Kante der Kosten $\infty$ enthalten, so existiert keine zulässige Zuweisung
der generierten Tripel, Paare und Unmatched-Items zu den Stacks, welche die Placement-Constraints respektiert.

Auf diese Weise entstehen $13$ Lösungen, von denen schließlich die günstigste ausgewählt wird.
Zuletzt muss dann wiederum ggf. die Reihenfolge der Items innerhalb der Stacks so angepasst werden,
dass sie die Stacking-Constraints respektiert. Dazu werden sämtliche Stacks, denen mehr als
ein Item zugewiesen wurde, betrachtet. Für jeden dieser Stacks wird geprüft, ob die aufeinander
gestapelten Items jeweils paarweise aufeinander gestapelt sein dürfen.
Werden zwei Items innerhalb eines Stacks gefunden, welche die Stacking-Constraints verletzen, so werden diese
vertauscht und der Prozess startet für diesen Stack erneut. Ein vollständig gefüllter Stack ist mit dieser
Strategie nach spätestens drei Tausch-Operationen korrekt sortiert. Durch die bei der Zuweisung geprüfte
Kompatibilität ist sichergestellt, dass eine solche Reihenfolge existiert.

Wie bereits die Heuristik aus Abschnitt \ref{sec:two_cap_heuristic}, bietet auch diese Heuristik über den Parameter
\textit{postProcessing} die Möglichkeit, einen Postprocessing-Schritt zu aktivieren, welcher einen reinen Fokus
auf die Transportkostenminimierung ermöglicht. Wird dieser Wert auf \textit{true} gesetzt, so werden ausgewählte Items aus der zuvor generierten Lösung an andere, freie Positionen verschoben, sofern sich dadurch eine Kostenersparnis ergibt.

\textbf{Postprocessing-Schritt}

Zunächst wird ein bipartiter Postprocessing-Graph $G_p = (V_{p1} \cup V_{p2}, E_p)$ generiert,
welcher die zuvor bereits platzierten Items $i \in I$ in der Partition $V_{p1}$ und die verbleibenden
freien Positionen in den Stacks innerhalb der Storage-Area in der Partition $V_{p2}$ enthält.
Eine solche Position entspricht einem Level $l \in \{1, ..., b\}$ innerhalb eines Stacks $q \in Q$.
Es existiert eine Kante $\{i, l\} \in E_p$ zwischen einem Item $i \in V_{p1}$ und einer freien
Position $l \in V_{p2}$ innerhalb eines Stacks $q \in Q$, wenn das Item $i$ basierend auf den Placement-Constraints
zu diesem Stack kompatibel ist, also wenn $t_{iq} = 1$ gilt.
Zusätzlich muss das Item $i$ für den Fall, dass sich bereits ein Item $j \in I$ oder sogar ein Paar von Items $j, k \in I$
im Stack $q$ befindet, basierend auf den Stacking-Constraints mit diesem Item bzw. Item-Paar kompatibel sein.
Wenn sich also bereits ein einzelnes Item $j$ im Stack $q$ befindet, so muss $s_{ij} = 1$ oder $s_{ji} = 1$ gelten.
Befindet sich dagegen bereits ein Paar $j, k$ innerhalb des Stacks $q$, so ergeben sich erneut die $3!$ möglichen Permutationen
eines Tripels, welche in Abb. \ref{fig:triple_permutations} dargestellt sind. Ist eine der Permutationen konform mit den Stacking-Constraints, so kann das Item $i$ dem Stack $q$ zugewiesen werden. Auf diese Weise kann nach der Zuweisung eines Items stets erneut eine den Stacking-Constraints entsprechende Anordnung innerhalb der Stacks erreicht werden.
Die Kosten einer solchen Kante entsprechen der Kostenersparnis der Zuweisung des Items zum neuen Stack verglichen mit den Kosten der gegenwärtigen Zuweisung dieses Items. Die Kostenersparnis $r_i$ für ein Item $i \in I$ entspricht erneut der Definition aus dem Postprocessing-Schritt in Abschnitt \ref{sec:two_cap_heuristic}.

Daraufhin wird in $G_p$ ein Maximum-Weight-Bipartite-Matching berechnet, dessen Kanten jene Items
mit freien Positionen in den Stacks verbinden, welche zur insgesamt größten Kostenreduktion führen.
Je nach Anzahl der freien Positionen pro Stack, können einem Stack null bis drei Items zugeordnet werden.
Angesichts der Tatsache, dass nicht gewährleistet ist, dass diese Items bezüglich der Stacking-Constraints miteinander kompatibel sind, darf jedem Stack allerdings nur ein neues Item zugewiesen werden.
Es wird im Falle mehrerer Zuordnungen jenes Item mit der größten Kostenersparnis verschoben.
Nachdem die Items an die ihnen zugeordneten Positionen verschoben wurden, wird die Reihenfolge der Items
innerhalb der Stacks, welchen mindestens ein zusätzliches Item zugewiesen wurde ggf. basierend auf den Stacking-Constraints angepasst. Befand sich ein verschobenes Item zuvor unterhalb anderer Items innerhalb eines Stacks, so werden die darüber platzierten Items heruntergezogen, sodass sich anschließend erneut eine zulässige Stacking-Lösung ergibt.

Wie bereits in der Heuristik für eine Stack-Kapazität von $b = 2$, wird dieser Postprocessing-Prozess
iterativ durchgeführt, bis keine weitere Verringerung des Kostenwerts erzielt wird.

\textbf{Beispiel}

Gegeben sei die Item-Menge $I = \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\}$ sowie eine Stack-Kapazität von $b = 3$.
Die Anzahl der zur Verfügung stehenden Stacks ergibt sich, wie in Abschnitt \ref{sec:test_data} beschrieben,
durch $m = \ceil{\ceil{\frac{14}{3}} + 0.2 \cdot  \ceil{\frac{14}{3}}} = 6$.

Zunächst wird, ebenfalls basierend auf der Beschreibung aus Abschnitt \ref{sec:test_data}, ein Stacking-Constraint-Graph
für die Item-Menge $I$ generiert, auf welchem anschließend analog zum Beispiel aus Abschnitt \ref{sec:two_cap_heuristic} ein
\textsc{MCM} berechnet wird. Dieser Schritt kann dementsprechend in Abschnitt \ref{sec:two_cap_heuristic} nachvollzogen werden
und wird in diesem Beispiel übersprungen. In Abb. \ref{fig:item_pairs_example_b=3} ist das Ergebnis dargestellt,
welches die Kanten dieses \textsc{MCM} interpretiert als kompatible Item-Paare zeigt.

\begin{figure}[H]
\centering
\begin{tikzpicture}[auto, transform shape, node distance=2cm, state/.style={circle, draw, fill=gray!40, minimum size=1.5cm}]
        \node[state] (A) [thick] {$\boldsymbol{0, 11}$};
        \node[state] (B) [thick, right of=A] {$\boldsymbol{1, 2}$};
        \node[state] (C) [thick, right of=B] {$\boldsymbol{3, 12}$};
        \node[state] (D) [thick, right of=C] {$\boldsymbol{4, 5}$};
        \node[state] (E) [thick, right of=D] {$\boldsymbol{6, 7}$};
        \node[state] (F) [thick, right of=E] {$\boldsymbol{8, 13}$};
        \node[state] (G) [thick, right of=F] {$\boldsymbol{9, 10}$};
        \path;
\end{tikzpicture}
\caption{\textsc{Resultierende Item-Paare}.}
\label{fig:item_pairs_example_b=3}
\end{figure}

Nachdem die in Abb. \ref{fig:item_pairs_example_b=3} dargestellten Paare von Items generiert wurden, geht es darum,
möglichst viele dieser Paare zu Tripeln von Items zu mergen. Zunächst muss allerdings basierend auf dem entwickelten Rating-System
entschieden werden, welche der Paare dazu wieder separiert werden. Durch die Bewertung und anschließende Sortierung der Paare
ergeben sich die Abb. \ref{fig:lists_of_pairs} zu entnehmenden unterschiedlich sortierten Listen.
\begin{figure}[H]
\begin{gather*}
  [(1, 2), (3, 12), (9, 10), (8, 13), (6, 7), (4, 5), (0, 11)] \\
  [(9, 10), (8, 13), (6, 7), (4, 5), (3, 12), (1, 2), (0, 11)] \\
  [(1, 2), (9, 10), (8, 13), (6, 7), (4, 5), (3, 12), (0, 11)]
\end{gather*}
\caption{\textsc{Unterschiedlich sortierte Listen der Item-Paare}.}
\label{fig:lists_of_pairs}
\end{figure}
Es ergeben sich dabei in zuvor beschriebenr Weise $13$ potenziell unterschiedlich sortierte Listen von Item-Paaren.
In diesem Fall entstehen u.a. aufgrund der geringen Anzahl an Paaren jedoch lediglich drei voneinander unterschiedliche
Sortierungen (vgl. Abb. \ref{fig:lists_of_pairs}).

Anschließend geht es darum, die ersten $\floor{\frac{7}{3.4}} = 2$ Paare zu separieren und möglichst sämtliche der dabei entstehenden
Items den verbleibenden Paaren zuzuweisen. Für die erste der drei unterschiedlichen Sortierungen aus Abb. \ref{fig:lists_of_pairs}
ergibt sich dabei die in Abb. \ref{fig:pre_merge_step} dargestellte Aufteilung in Paare, welche separiert werden
(vgl. Abb. \ref{fig:split_pairs}) und jene, denen die daraus resultierenden Items zugewiesen werden sollen um Item-Tripel zu bilden
(vgl. Abb. \ref{fig:assign_pairs}).

\begin{figure}[H]
\centering
\begin{subfigure}[b]{\textwidth}
\centering
\begin{tikzpicture}[scale=0.85, transform shape, node distance=2cm, state/.style={circle, draw, fill=gray!40, minimum size=1.5cm}]
        \node[state] (A) [thick] {$\boldsymbol{1, 2}$};
        \node[state] (B) [thick, right of=A] {$\boldsymbol{3, 12}$};
        \path;
\end{tikzpicture}
\caption{\textsc{Split-Pairs}.}
\label{fig:split_pairs}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\centering
\begin{tikzpicture}[scale=0.85, transform shape, node distance=2cm, state/.style={circle, draw, fill=gray!40, minimum size=1.5cm}]
        \node[state] (A) [thick] {$\boldsymbol{9, 10}$};
        \node[state] (B) [thick, right of=A] {$\boldsymbol{8, 13}$};
        \node[state] (C) [thick, right of=B] {$\boldsymbol{6, 7}$};
        \node[state] (D) [thick, right of=C] {$\boldsymbol{4, 5}$};
        \node[state] (E) [thick, right of=D] {$\boldsymbol{0, 11}$};
        \path;
\end{tikzpicture}
\caption{\textsc{Assign-Pairs}.}
\label{fig:assign_pairs}
\end{subfigure}
\caption{\textsc{Merge Vorbereitung}.}
\label{fig:pre_merge_step}
\end{figure}

Zwecks Zuweisung der aus der Separation der Paare aus Abb. \ref{fig:split_pairs} hervorgehenden einzelnen Items $1, 2, 3$ und $12$
zu den verbleibenden Paaren aus Abb. \ref{fig:assign_pairs}, um Tripel zu bilden, wird der in Abb.
\ref{fig:bipartite_graph_pairs_and_unmatched} dargestellte bipartite Graph generiert, welcher die Paare in der einen Partition
und die einzelnen Items in der anderen Partition enthält. Eine Kante zwischen einem einzelnen Item und einem Paar bedeutet,
dass dieses Item dem Paar zugeordnet werden kann, sodass ein kompatibles Tripel entsteht.
In Abb. \ref{fig:item_triple_mcm} ist das auf diesem Graphen berechnete \textsc{MCM} in grün dargestellt, in welchem
sämtliche einzelnen Items einem Paar zugewiesen werden. Das Paar $(4, 5)$ wird dabei als einziges Paar nicht verwendet
um ein Tripel zu bilden und verbleibt dementsprechend als Item-Paar.

\vfill

\pagebreak

\begin{figure}[H]
\centering

\begin{subfigure}[b]{0.4\textwidth}
\centering
\begin{tikzpicture}[scale=0.7, transform shape, node distance=2cm, state/.style={circle, draw, minimum size=1.25cm}]
        \node[state] (A) [thick] {$\boldsymbol{0, 11}$};
        \node[state] (B) [thick, below of=A] {$\boldsymbol{4, 5}$};
        \node[state] (C) [thick, below of=B] {$\boldsymbol{6, 7}$};
        \node[state] (D) [thick, below of =C] {$\boldsymbol{8, 13}$};
        \node[state] (E) [thick, below of=D] {$\boldsymbol{9, 10}$};
        \node[state] (F) [thick, right = 4cm of A] {$\boldsymbol{1}$};
        \node[state] (G) [thick, below of=F] {$\boldsymbol{2}$};
        \node[state] (H) [thick, below of=G] {$\boldsymbol{3}$};
        \node[state] (I) [thick, below of=H] {$\boldsymbol{12}$};
        \path
          (A) edge [thick] node {} (H)

          (C) edge [thick] node {} (F)
          (C) edge [thick] node {} (G)
          (C) edge [thick] node {} (H)

          (D) edge [thick] node {} (F)
          (D) edge [thick] node {} (H)

          (E) edge [thick] node {} (F)
          (E) edge [thick] node {} (G)
          (E) edge [thick] node {} (H)
          (E) edge [thick] node {} (I);
\end{tikzpicture}
\caption{\textsc{Bipartiter Graph}.}
\label{fig:bipartite_graph_pairs_and_unmatched}
\end{subfigure}
\hspace{10pt}
\begin{subfigure}[b]{0.4\textwidth}
\centering
\begin{tikzpicture}[scale=0.7, transform shape, node distance=2cm, state/.style={circle, draw, minimum size=1.25cm}]
        \node[state] (A) [thick] {$\boldsymbol{\textcolor{mygreen}{0, 11}}$};
        \node[state] (B) [thick, below of=A] {$\boldsymbol{\textcolor{red}{4, 5}}$};
        \node[state] (C) [thick, below of=B] {$\boldsymbol{\textcolor{mygreen}{6, 7}}$};
        \node[state] (D) [thick, below of =C] {$\boldsymbol{\textcolor{mygreen}{8, 13}}$};
        \node[state] (E) [thick, below of=D] {$\boldsymbol{\textcolor{mygreen}{9, 10}}$};
        \node[state] (F) [thick, right = 4cm of A] {$\boldsymbol{\textcolor{mygreen}{1}}$};
        \node[state] (G) [thick, below of=F] {$\boldsymbol{\textcolor{mygreen}{2}}$};
        \node[state] (H) [thick, below of=G] {$\boldsymbol{\textcolor{mygreen}{3}}$};
        \node[state] (I) [thick, below of=H] {$\boldsymbol{\textcolor{mygreen}{12}}$};
        \path
          (A) edge [thick, mygreen] node {} (H)

          (C) edge [thick] node {} (F)
          (C) edge [thick, mygreen] node {} (G)
          (C) edge [thick] node {} (H)

          (D) edge [thick, mygreen] node {} (F)
          (D) edge [thick] node {} (H)

          (E) edge [thick] node {} (F)
          (E) edge [thick] node {} (G)
          (E) edge [thick] node {} (H)
          (E) edge [thick, mygreen] node {} (I);
\end{tikzpicture}
\caption{\textsc{MCM}.}
\label{fig:item_triple_mcm}
\end{subfigure}

\caption{\textsc{Item-Tripel Generierung.}}
\label{fig:item_triple_generation}
\end{figure}

Abbildung \ref{fig:merge_result} zeigt das Ergebnis des Merge-Prozesses, in welchem durch das
\textsc{MCM} zusätzlich zum verbleibenden Item-Paar vier Item-Tripel generiert wurden.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.85, transform shape, node distance=2.5cm, state/.style={circle, draw, fill=gray!40, minimum size=2cm}]
        \node[state] (A) [thick] {$\boldsymbol{9, 10, 12}$};
        \node[state] (B) [thick, right of=A] {$\boldsymbol{1, 8, 13}$};
        \node[state] (C) [thick, right of=B] {$\boldsymbol{2, 6, 7}$};
        \node[state] (D) [thick, right of=C] {$\boldsymbol{0, 3, 11}$};
        \node[state] (E) [thick, right of=D] {$\boldsymbol{4, 5}$};
        \path;
\end{tikzpicture}
\caption{\textsc{Merge-Ergebnis.}}
\label{fig:merge_result}
\end{figure}

Die in Abb. \ref{fig:merge_result} dargestellten Item-Tripel, sowie das Item-Paar, werden im Folgenden
möglichst günstig und in einer Weise, welche die Placement-Constraints respektiert, einem Stack zugewiesen.
Dazu wird zunächst der vollständig bipartite Graph, welcher in Abb. \ref{fig:complete_bipartite} dargestellt ist,
generiert. Dieser Graph enthält die Items bestehend aus vier Tripeln und einem Paar in der einen Partition
und die Stacks in der anderen Partition.
Da es lediglich $5$ Item-Tupel bei $6$ zur Verfügung stehenden Stacks gibt und somit die Partitionen nicht dieselbe
Größe besitzen, wird ein Dummy-Item $D_0$ eingeführt.
Die Kantengewichte sind der Übersichtlichkeit halber nicht in der Darstellung aufgetragen, sondern stattdessen
der Tabelle in Abb. \ref{fig:item_tuple_costs_three_cap} zu entnehmen.
\begin{figure}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{| c | c | c | c | c | c | c |}
    \hline
     & $\boldsymbol{S_0}$ & $\boldsymbol{S_1}$ & $\boldsymbol{S_2}$ & $\boldsymbol{S_3}$ & $\boldsymbol{S_4}$ & $\boldsymbol{S_5}$ \\ \hline
    $\boldsymbol{(0, 11, 3)}$ & $128.76$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ \hline
    $\boldsymbol{(6, 7, 2)}$ & $134.82$ & $\infty$ & $98.46$ & $\infty$ & $\infty$ & $80.28$ \\ \hline
    $\boldsymbol{(8, 13, 1)}$ & $\infty$ & $159.06$ & $\infty$ & $146.94$ & $\infty$ & $134.82$ \\ \hline
    $\boldsymbol{(9, 10, 12)}$ & $231.78$ & $\infty$ & $195.42$ & $\infty$ & $159.06$ & $140.88$ \\ \hline
    $\boldsymbol{(4, 5)}$ & $83.82$ & $71.7$ & $59.58$ & $\infty$ & $35.34$ & $\infty$ \\ \hline
    $\boldsymbol{D_0}$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ \\ \hline
\end{tabular}}
\caption{\textsc{Kosten der Stack-Zuweisungen}.}
\label{fig:item_tuple_costs_three_cap}
\end{figure}

Anschließend wird im bipartiten Graphen aus Abb. \ref{fig:complete_bipartite} das \textsc{MWPM} berechnet, welches
in Abb. \ref{fig:mwpm} dargestellt ist und einer günstigsten Zuweisung der zuvor generierten Item-Tupel zu den Stacks entspricht.

\begin{figure}[H]
% \begin{figure}[H]
\begin{subfigure}[b]{0.4\textwidth}
\centering
\begin{tikzpicture}[scale=0.55, transform shape, node distance=2.25cm, state/.style={circle, draw, minimum size=2cm}]
        \node[state] (A) [thick] {$\boldsymbol{0, 11, 3}$};
        \node[state] (B) [thick, below of=A] {$\boldsymbol{6, 7, 2}$};
        \node[state] (C) [thick, below of=B] {$\boldsymbol{8, 13, 1}$};
        \node[state] (D) [thick, below of=C] {$\boldsymbol{9, 10, 12}$};
        \node[state] (E) [thick, below of=D] {$\boldsymbol{4, 5}$};
        \node[state] (F) [thick, below of=E] {$\boldsymbol{D_0}$};

        \node[state] (G) [thick, right = 4cm of A] {$\boldsymbol{S_0}$};
        \node[state] (H) [thick, below of=G] {$\boldsymbol{S_1}$};
        \node[state] (I) [thick, below of=H] {$\boldsymbol{S_2}$};
        \node[state] (J) [thick, below of=I] {$\boldsymbol{S_3}$};
        \node[state] (K) [thick, below of=J] {$\boldsymbol{S_4}$};
        \node[state] (L) [thick, below of=K] {$\boldsymbol{S_5}$};

        \path
          (A) edge [thick] node {} (G)
          (A) edge [thick] node {} (H)
          (A) edge [thick] node {} (I)
          (A) edge [thick] node {} (J)
          (A) edge [thick] node {} (K)
          (A) edge [thick] node {} (L)

          (B) edge [thick] node {} (G)
          (B) edge [thick] node {} (H)
          (B) edge [thick] node {} (I)
          (B) edge [thick] node {} (J)
          (B) edge [thick] node {} (K)
          (B) edge [thick] node {} (L)

          (C) edge [thick] node {} (G)
          (C) edge [thick] node {} (H)
          (C) edge [thick] node {} (I)
          (C) edge [thick] node {} (J)
          (C) edge [thick] node {} (K)
          (C) edge [thick] node {} (L)

          (D) edge [thick] node {} (G)
          (D) edge [thick] node {} (H)
          (D) edge [thick] node {} (I)
          (D) edge [thick] node {} (J)
          (D) edge [thick] node {} (K)
          (D) edge [thick] node {} (L)

          (E) edge [thick] node {} (G)
          (E) edge [thick] node {} (H)
          (E) edge [thick] node {} (I)
          (E) edge [thick] node {} (J)
          (E) edge [thick] node {} (K)
          (E) edge [thick] node {} (L)

          (F) edge [thick] node {} (G)
          (F) edge [thick] node {} (H)
          (F) edge [thick] node {} (I)
          (F) edge [thick] node {} (J)
          (F) edge [thick] node {} (K)
          (F) edge [thick] node {} (L);
\end{tikzpicture}
\caption{\textsc{Vollständig Bipartit}.}
\label{fig:complete_bipartite}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
\centering
\begin{tikzpicture}[scale=0.55, transform shape, node distance=2.25cm, state/.style={circle, draw, minimum size=2cm}]
        \node[state] (A) [thick] {$\boldsymbol{0, 11, 3}$};
        \node[state] (B) [thick, below of=A] {$\boldsymbol{6, 7, 2}$};
        \node[state] (C) [thick, below of=B] {$\boldsymbol{8, 13, 1}$};
        \node[state] (D) [thick, below of=C] {$\boldsymbol{9, 10, 12}$};
        \node[state] (E) [thick, below of=D] {$\boldsymbol{4, 5}$};
        \node[state] (F) [thick, below of=E] {$\boldsymbol{D_0}$};

        \node[state] (G) [thick, right = 4cm of A] {$\boldsymbol{S_0}$};
        \node[state] (H) [thick, below of=G] {$\boldsymbol{S_1}$};
        \node[state] (I) [thick, below of=H] {$\boldsymbol{S_2}$};
        \node[state] (J) [thick, below of=I] {$\boldsymbol{S_3}$};
        \node[state] (K) [thick, below of=J] {$\boldsymbol{S_4}$};
        \node[state] (L) [thick, below of=K] {$\boldsymbol{S_5}$};

        \path
          (A) edge [thick, above] node {$128.76$} (G)
          (B) edge [thick, above right, near start] node {$98.46$} (I)
          (C) edge [thick, near start, above right] node {$146.94$} (J)
          (D) edge [thick, below left, near end] node {$140.88$} (L)
          (E) edge [thick, near end, above] node {$35.34$} (K)
          (F) edge [thick, below right] node {$0.0$} (H);
\end{tikzpicture}
\caption{\textsc{MWPM}.}
\label{fig:mwpm}
\end{subfigure}
\caption{\textsc{Stack-Zuweisung}.}
\label{}
\end{figure}

Da keine Kante im \textsc{MWPM} das Gewicht $\infty$ besitzt, handelt es sich um eine günstigste, zulässige Zuweisung
sämtlicher Item-Tupel zu Stacks, welche insgesamt die Kosten $550.38$ verursacht.
Die daraus resultierende Stacking-Lösung ist in Abb. \ref{fig:valid_solution} abgebildet, wobei die Reihenfolge
innerhalb der Stacks ggf. in zuvor beschriebener Weise angepasst werden muss.
\begin{figure}[H]
  \centering
  \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|}
    \cline{2-7}
    $\boldsymbol{L_2}$ & $3$ & $$ & $2$ & $1$ & $$ & $12$ \\ \cline{2-7}
    $\boldsymbol{L_1}$ & $11$ & $$ & $7$ & $13$ & $5$ & $10$\\ \cline{2-7}
    $\boldsymbol{L_0}$ & $0$ & $$ & $6$ & $8$ & $4$ & $9$ \\ \cline{2-7}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\boldsymbol{S_0}$} & \multicolumn{1}{c}{$\boldsymbol{S_1}$}
    & \multicolumn{1}{c}{$\boldsymbol{S_2}$} & \multicolumn{1}{c}{$\boldsymbol{S_3}$} & \multicolumn{1}{c}{$\boldsymbol{S_4}$}
    & \multicolumn{1}{c}{$\boldsymbol{S_5}$} \\
    \end{tabular}}
    \caption{\textsc{Zulässige Zuweisungen.}}
    \label{fig:valid_solution}
\end{figure}

Diese Lösung basiert auf der ersten der drei unterschiedlichen Sortierungen der Item-Paare aus Abb. \ref{fig:lists_of_pairs}.
Um die bestmögliche Lösung zu finden, wird der geschilderte Prozess für die beiden übrigen Sortierungen
ebenfalls durchgeführt und schließlich die beste Lösung in Bezug auf die Kosten ausgewählt.

Ist der Postprocessing-Schritt aktiviert, so wird zuletzt die beste generierte Lösung erneut betrachtet
und es werden ausgewählte Items an freie Positionen in anderen Stacks verschoben, sofern sich dadurch eine
Kostenersparnis ergibt.
Dazu wird der bipartite Postprocessing-Graph generiert, welcher in Abb. \ref{fig:post_processing_graph_b=3} dargestellt ist.
Dieser enthält die Items in der einen Partition und die freien Positionen innerhalb der Stacks in der anderen Partition.

Die Positionen sind erneut jeweils in der Notation $pos(stackIdx, level)$ gegeben. In diesem Beispiel gibt es,
wie man Abb. \ref{fig:valid_solution} entnehmen kann, mit $pos(1, 0), pos(1, 1), pos(1, 2)$ und $pos(4, 2)$
vier verbleibende freie Positionen in den Stacks. Die Kantengewichte des Postprocessing-Graphen sind der Übersichtlichkeit halber nicht an den Kanten notiert, sondern in der Tabelle in Abb. \ref{fig:post_processing_costs} aufgeführt.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.5, transform shape, node distance=1.5cm, state/.style={circle, draw, minimum size=1.25cm}]
        \node[state] (A) [thick] {$\boldsymbol{0}$};
        \node[state] (B) [thick, right of=A] {$\boldsymbol{1}$};
        \node[state] (C) [thick, right of=B] {$\boldsymbol{2}$};
        \node[state] (D) [thick, right of=C] {$\boldsymbol{3}$};
        \node[state] (E) [thick, right of=D] {$\boldsymbol{4}$};
        \node[state] (F) [thick, right of=E] {$\boldsymbol{5}$};
        \node[state] (G) [thick, right of=F] {$\boldsymbol{6}$};
        \node[state] (H) [thick, right of=G] {$\boldsymbol{7}$};
        \node[state] (I) [thick, right of=H] {$\boldsymbol{8}$};
        \node[state] (J) [thick, right of=I] {$\boldsymbol{9}$};
        \node[state] (K) [thick, right of=J] {$\boldsymbol{10}$};
        \node[state] (L) [thick, right of=K] {$\boldsymbol{11}$};
        \node[state] (M) [thick, right of=L] {$\boldsymbol{12}$};
        \node[state] (N) [thick, right of=M] {$\boldsymbol{13}$};

        \node[state] (O) [thick, below = 5cm of B] {$\boldsymbol{pos(1, 0)}$};
        \node[state] (P) [thick, below = 5cm of F] {$\boldsymbol{pos(1, 1)}$};
        \node[state] (Q) [thick, below = 5cm of I] {$\boldsymbol{pos(1, 2)}$};
        \node[state] (R) [thick, below = 5cm of M] {$\boldsymbol{pos(4, 0)}$};

        \path
            (B) edge [thick] node {$$} (O)
            (B) edge [thick] node {$$} (P)
            (B) edge [thick] node {$$} (Q)
            (D) edge [thick] node {$$} (O)
            (D) edge [thick] node {$$} (P)
            (D) edge [thick] node {$$} (Q)
            (E) edge [thick] node {$$} (O)
            (E) edge [thick] node {$$} (P)
            (E) edge [thick] node {$$} (Q)
            (E) edge [thick] node {$$} (R)
            (F) edge [thick] node {$$} (O)
            (F) edge [thick] node {$$} (P)
            (F) edge [thick] node {$$} (Q)
            (F) edge [thick] node {$$} (R)
            (H) edge [thick] node {$$} (O)
            (H) edge [thick] node {$$} (P)
            (H) edge [thick] node {$$} (Q)
            (I) edge [thick] node {$$} (O)
            (I) edge [thick] node {$$} (P)
            (I) edge [thick] node {$$} (Q)
            (J) edge [thick] node {$$} (O)
            (J) edge [thick] node {$$} (P)
            (J) edge [thick] node {$$} (Q)
            (K) edge [thick] node {$$} (O)
            (K) edge [thick] node {$$} (P)
            (K) edge [thick] node {$$} (Q)
            (L) edge [thick] node {$$} (O)
            (L) edge [thick] node {$$} (P)
            (L) edge [thick] node {$$} (Q)
            (L) edge [thick] node {$$} (R)
            (N) edge [thick] node {$$} (O)
            (N) edge [thick] node {$$} (P)
            (N) edge [thick] node {$$} (Q);
\end{tikzpicture}
\caption{\textsc{Postprocessing-Graph}.}
\label{fig:post_processing_graph_b=3}
\end{figure}

Wie man der Tabelle in Abb. \ref{fig:post_processing_costs} entnehmen kann, sind im Gegensatz zum Beispiel
der Heuristik aus Abschnitt \ref{sec:two_cap_heuristic} in diesem Fall positive Kantengewichte vorhanden,
was bedeutet, dass sich durch Verschieben ausgewählter Items an freie Positionen Kosten einsparen lassen.

\begin{figure}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{| c | c | c | c | c |}
    \hline
     & $\boldsymbol{pos(1, 0)}$ & $\boldsymbol{pos(1, 1)}$ & $\boldsymbol{pos(1, 2)}$ & $\boldsymbol{pos(4, 0)}$ \\ \hline
     $1$ & $12.12$ & $12.12$ & $12.12$ & $-$ \\ \hline
     $3$ & $6.06$ & $6.06$ & $6.06$ & $-$ \\ \hline
     $4$ & $-18.18$ & $-18.18$ & $-18.18$ & $0.0$ \\ \hline
     $5$ & $-18.18$ & $-18.18$ & $-18.18$ & $0.0$ \\ \hline
     $7$ & $-6.06$ & $-6.06$ & $-6.06$ & $-$ \\ \hline
     $8$ & $-12.12$ & $-12.12$ & $-12.12$ & $-$ \\ \hline
     $9$ & $-24.24$ & $-24.24$ & $-24.24$ & $-$ \\ \hline
     $10$ & $-24.24$ & $-24.24$ & $-24.24$ & $-$ \\ \hline
     $11$ & $6.06$ & $6.06$ & $6.06$ & $24.24$ \\ \hline
     $13$ & $-12.12$ & $-12.12$ & $-12.12$ & $-$ \\ \hline
\end{tabular}}
\caption{\textsc{Kantengewichte des Postprocessing-Graphen}.}
\label{fig:post_processing_costs}
\end{figure}

Jene Zurordnungen von Items zu freien Positionen, welche zur insgesamt größten Kostenreduktion führen, werden durch
das Maximum-Weight-Bipartite-Matching auf dem Graphen aus Abb. \ref{fig:post_processing_graph_b=3} ermittelt,
welches in Abb. \ref{fig:mwbm} dargestellt ist.
Diese Zuordnungen sorgen für eine Gesamtersparnis von $36.36$, welche sich wie folgt ergibt.
Item $11$ wird mit einer Ersparnis von $24.24$ in $S_4$ platziert. Da die Items $1$ und $3$ beide $S_1$ zugeordnet werden,
wird lediglich das Item mit der größeren Ersparnis verschoben. Somit wird Item $1$ mit einer Ersparnis von $12.12$
in $S_1$ platziert. Insgesamt verringern sich die Kosten der Lösung von $550.38$ auf $514.02$.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.5, transform shape, node distance=1.5cm, state/.style={circle, draw, minimum size=1.25cm}]
        \node[state] (A) [thick] {$\boldsymbol{0}$};
        \node[state] (B) [thick, right of=A] {$\boldsymbol{1}$};
        \node[state] (C) [thick, right of=B] {$\boldsymbol{2}$};
        \node[state] (D) [thick, right of=C] {$\boldsymbol{3}$};
        \node[state] (E) [thick, right of=D] {$\boldsymbol{4}$};
        \node[state] (F) [thick, right of=E] {$\boldsymbol{5}$};
        \node[state] (G) [thick, right of=F] {$\boldsymbol{6}$};
        \node[state] (H) [thick, right of=G] {$\boldsymbol{7}$};
        \node[state] (I) [thick, right of=H] {$\boldsymbol{8}$};
        \node[state] (J) [thick, right of=I] {$\boldsymbol{9}$};
        \node[state] (K) [thick, right of=J] {$\boldsymbol{10}$};
        \node[state] (L) [thick, right of=K] {$\boldsymbol{11}$};
        \node[state] (M) [thick, right of=L] {$\boldsymbol{12}$};
        \node[state] (N) [thick, right of=M] {$\boldsymbol{13}$};

        \node[state] (O) [thick, below = 2cm of B] {$\boldsymbol{pos(1, 0)}$};
        \node[state] (P) [thick, below = 2cm of F] {$\boldsymbol{pos(1, 1)}$};
        \node[state] (Q) [thick, below = 2cm of I] {$\boldsymbol{pos(1, 2)}$};
        \node[state] (R) [thick, below = 2cm of M] {$\boldsymbol{pos(4, 0)}$};

        \path
            (B) edge [thick, left] node {$12.12$} (O)
            (D) edge [thick, right] node {$6.06$} (P)
            (L) edge [thick, right] node {$24.24$} (R);
\end{tikzpicture}
\caption{\textsc{Maximum-Weight-Bipartite-Matching}.}
\label{fig:mwbm}
\end{figure}

In Abb. \ref{fig:valid_solution_post_processing} ist das finale Ergebnis zu sehen, in welchem
die im Zuge des Postprocessings verschobenen Items in grün dargestellt sind.
Zuletzt muss ggf. in zuvor beschriebener Weise die Reihenfolge innerhalb der Stacks angepasst werden,
sodass sie den Stacking-Constraints entspricht. Da es sich beim Postprocessing um ein iteratives Verfahren handelt,
würde dieser Prozess nun wiederholt, bis sich keine Verbesserung der Lösung mehr erzielen lässt.
In diesem Beispiel wird bereits nach der ersten Iteration das beste Ergebnis erzielt und der Postprocessing-Prozess terminiert.

\begin{figure}[H]
  \centering
  \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|}
    \cline{2-7}
    $\boldsymbol{L_2}$ & $$ & $$ & $2$ & $$ & $\textcolor{mygreen}{11}$ & $12$ \\ \cline{2-7}
    $\boldsymbol{L_1}$ & $3$ & $$ & $7$ & $13$ & $5$ & $10$\\ \cline{2-7}
    $\boldsymbol{L_0}$ & $0$ & $\textcolor{mygreen}{1}$ & $6$ & $8$ & $4$ & $9$ \\ \cline{2-7}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\boldsymbol{S_0}$} & \multicolumn{1}{c}{$\boldsymbol{S_1}$}
    & \multicolumn{1}{c}{$\boldsymbol{S_2}$} & \multicolumn{1}{c}{$\boldsymbol{S_3}$} & \multicolumn{1}{c}{$\boldsymbol{S_4}$}
    & \multicolumn{1}{c}{$\boldsymbol{S_5}$} \\
    \end{tabular}}
    \caption{\textsc{Postprocessing-Ergebnis.}}
    \label{fig:valid_solution_post_processing}
\end{figure}

\subsection{Vergleich der $b = 3$ Solver}
\label{sec:solver_comp_b=3}

In diesem Kapitel werden die verschiedenen Solver für Instanzen von Stacking-Problemen mit einer Stack-Kapazität
von $b = 3$ miteinander verglichen. Dabei gilt weiterhin das in Kapitel \ref{sec:solver_comp_b=2} vorgestellte Setup
bezüglich der Vergleichskategorien, der \textsc{CPLEX}-Konfiguration sowie der verwendeten Hardware.

Der in Abschnitt \ref{sec:three_cap_heuristic} eingeführte Parameter \textit{prioritizeRuntime} zur Konfiguration der Heuristik
wird in den folgenden Experimenten stets auf \textit{false} gesetzt, weil die durch Verwendung des Rating-Systems erhöhten Laufzeiten
aus praktischer Perspektive vermutlich in jedem Fall vertretbar sind. Aufgrunddessen liegt der Fokus hier auf dem Erreichen
möglichst geringer Zielfunktionswerte. Liegt der Fokus in der Praxis hingegen auf einer möglichst geringen Laufzeit, so kann dementsprechend
\textit{prioritizeRuntime} auf \textit{true} gesetzt werden, was zu erheblich kürzeren Laufzeiten bei nur geringfügig schlechteren
Zielfunktionswerten führt.

\pagebreak

\textbf{Kleine Instanzen (s)}

Zunächst zum experimentellen Vergleich der Solver für in Kapitel \ref{sec:test_data} als klein definierte
Instanzen, bei denen es darum geht, 100 eintreffende Items in die Storage-Area zu verladen.

In Abb. \ref{fig:instance_coverage_b=3_s} ist die Instance-Coverage der einzelnen Solver bei einem Zeitlimit von $3, 5$
und $10$ Sekunden pro Instanz dargestellt. Die Bin-Packing-Formulierung ist hier offensichtlich schneller als die 3-Index-Formulierung.
Diese hat bereits bei einem Zeitlimit von $3s$ sämtliche Instanzen gelöst, während die 3-Index-Formulierung
bei diesem Zeitlimit lediglich drei Instanzen löst. Die Ergebnisse aus Abschnitt \ref{sec:solver_comp_b=2} in der Kategorie der kleinen
Instanzen haben sich also bezüglich der MIP-Formulierungen für eine Stack-Kapazität von $b = 3$ umgekehrt, denn in diesem Fall
ist die Bin-Packing-Formulierung der 3-Index-Formulierung vorzuziehen.

Grundsätzlich gilt, wie Abb. \ref{fig:instance_coverage_b=3_s_c} zu entnehmen ist, dass sämtliche Solver
bei einem vergleichsweise geringen Zeitlimit von $10s$ alle Instanzen lösen. Für die Heuristik ist das Zeitlimit irrelevant,
da diese mit einer durchschnittlichen Laufzeit von $0.09s$ sämtliche Instanzen löst.

\begin{figure}[H]
\centering

\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=3_s_3s.png}
\caption{\textsc{Zeitlimit} $3s$}
\label{fig:instance_coverage_b=3_s_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=3_s_5s.png}
\caption{\textsc{Zeitlimit} $5s$}
\label{fig:instance_coverage_b=3_s_b}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=3_s_10s.png}
\caption{\textsc{Zeitlimit} $10s$}
\label{fig:instance_coverage_b=3_s_c}
\end{subfigure}

\caption{\textsc{Instance-Coverage der $b = 3$ Solver (s)}.}
\label{fig:instance_coverage_b=3_s}
\end{figure}

In der Darstellung der Laufzeiten in Abb. \ref{fig:b=3_s_runtimes} ist zu erkennen, dass die Bin-Packing-
und die 3-Index-Formulierung, obwohl die Bin-Packing-Formulierunge in der Regel schneller ist,
häufig ähnliche Laufzeiten von ca. $2.5$ bis $5$ Sekunden besitzen.
Da die 3-Index-Formulierung mit den Instanzen $03, 09, 10, 11$ und $13$ allerdings einige Ausreißer,
welche eine erheblich längere Laufzeit erfordern, zeigt, ist die Bin-Packing-Formulierung im Durchschnitt klar schneller.
Die Heuristik unterbietet diese mit einer durchschnittlichen Laufzeit von nur $0.09s$ allerdings noch einmal deutlich
und weicht dabei um nur durchschnittlich $0.94 \%$ vom Optimum ab.

\pagebreak

Da die Bin-Packing-Formulierung, wie in Abb. \ref{fig:b=3_s_runtimes} zu erkennen ist, stets vor dem Zeitlimit terminiert,
löst diese sämtliche Instanzen optimal. Die 3-Index-Formulierung wird basierend auf der Abbildung in drei Fällen
durch das Zeitlimit gestoppt. Wie man in Abb. \ref{fig:b=3_s_costs} erkennen kann, wurden mit den
Instanzen $03$ und $11$ zwei jener drei Instanzen nicht optimal gelöst.
In diesen Fällen gelangt die Heuristik sogar zu besseren Zielfunktionswerten.
Im dritten Fall, in welchem die 3-Index-Formulierung durch das Zeitlimit gestoppt wurde, der Instanz $09$, hat \textsc{CPLEX}
offenkundig lediglich noch nicht bewiesen, dass es sich um das Optimum handelt, der entsprechende Wert wurde allerdings,
wie man Abb. \ref{fig:b=3_s_costs} entnehmen kann, bereits ermittelt. Sofern beide MIP-Formulierungen den Optimalwert ermittelt haben,
verdecken die Einträge der 3-Index-Formulierung in der Abb. jeweils die Einträge der Bin-Packing-Formulierung.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_time_b=3_s_10s.png}
\caption{\textsc{Laufzeiten}}
\label{fig:b=3_s_runtimes}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_cost_b=3_s_10s.png}
\caption{\textsc{Kosten}}
\label{fig:b=3_s_costs}
\end{subfigure}
\caption{\textsc{Ergebnisse bei $10s$ Zeitlimit}.}
\end{figure}

Abbildung \ref{fig:mip_results_b=3_s_b} ist zu entnehmen, dass die Bin-Packing-Formulierung nach durchschnittlich
$3.1s$ sämtliche Instanzen optimal löst. Bei einem Zeitlimit von $10s$ löst die 3-Index-Formulierung zwar ebenfalls
sämtliche Instanzen (vgl. \ref{fig:instance_coverage_b=3_s_c}), allerdings nur $90 \%$ optimal (vgl. \ref{fig:mip_results_b=3_s_c}). Dafür benötigt diese eine durchschnittliche Laufzeit von $5.9s$.
Wie bereits einleitend anhand der Instance-Coverage deutlich wurde, ist die Bin-Packing-Formulierung der 3-Index-Formulierung in dieser Kategorie vorzuziehen.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $\textcolor{mygreen}{50 \%}$ & $\textcolor{red}{5 \%}$ \\ \hline
    \textbf{Laufzeit} & \O $\thinspace 2.9 s$ & \O $\thinspace 2.9 s$ \\ \hline
    \textbf{Abweichung} & \O $\thinspace \textcolor{mygreen}{1.7 \%}$ & \O $\thinspace \textcolor{red}{7.8 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $3s$}
\label{fig:mip_results_b=3_s_a}
\end{subfigure}
% \end{figure}
% \begin{figure}[H]
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $\textcolor{mygreen}{100 \%}$ & $\textcolor{red}{50 \%}$ \\ \hline
    \textbf{Laufzeit} & \O $\thinspace \textcolor{mygreen}{3.1 s}$ & \O $\thinspace \textcolor{red}{4.0 s}$ \\ \hline
    \textbf{Abweichung} & \O $\thinspace \textcolor{mygreen}{0 \%}$ & \O $\thinspace \textcolor{red}{1.2 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $5s$}
\label{fig:mip_results_b=3_s_b}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{mygreen}{100 \%}$ & $ \textcolor{red}{90 \%}$ \\ \hline
    \textbf{Laufzeit} & \O $\thinspace \textcolor{mygreen}{3.1 s}$ & \O $\thinspace \textcolor{red}{5.9 s}$ \\ \hline
    \textbf{Abweichung} & \O $\thinspace \textcolor{mygreen}{0.0 \%}$ & \O $\thinspace \textcolor{red}{0.5 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $10s$}
\label{fig:mip_results_b=3_s_c}
\end{subfigure}
\caption{\textsc{MIP-Ergebnisse.}}
\label{fig:mip_results_b=3_s}
\end{figure}

Bei der durchschnittlichen Abweichung der Heuristik von $0.94 \%$ handelt es sich sicher in Anbetracht
der enormen Laufzeitverbesserung um einen akzeptablen Wert, allerdings gilt, wie schon in der Kategorie
der kleinen Instanzen für eine Stack-Kapazität von $b = 2$ auch hier, dass die durchschnittlich benötigten $3.1s$
für die exakte Lösung seitens der Bin-Packing-Formulierung aus praktischer Perspektive vermutlich vertretbar sind
und kein wirklicher Bedarf für eine Laufzeitvebessrung durch eine Heuristik herrscht.

\textbf{Mittelgroße Instanzen (m)}

In der Kategorie der mittelgroßen Instanzen werden Zeitlimits von $10$, $20$ und $30$ Minuten pro Instanz betrachtet.
In Abb. \ref{fig:instance_coverage_b=3_m} ist klar zu erkennen, dass die Bin-Packing-Formulierung in dieser Kategorie erfolgsversprechender ist, als die 3-Index-Formulierung, welche selbst bei einem Zeitlimit von $30$ Minuten nur $40 \%$ der Instanzen löst (vgl. Abb. \ref{fig:instance_coverage_b=3_m_c}).
Wie man den Abbildungen \ref{fig:instance_coverage_b=3_m_a} und \ref{fig:instance_coverage_b=3_m_b} entnehmen kann,
ändert sich die Instance-Coverage der 3-Index-Formulierung zwischen dem betrachteten Zeitlimit von $10$ Minuten pro
Instanz und jenem von $20$ Minuten pro Instanz nur marginal.
Die Erhöhung des Zeitlimits von $20$ auf $30$ Minuten pro Instanz bringt sogar überhaupt keine Erhöhung der Instance-Coverage
mit sich. Dies deutet daraufhin, dass die 3-Index-Formulierung noch erheblich mehr Zeit zur Lösung sämtlicher Instanzen benötigt.

Die Bin-Packing-Formulierung löst, wie in Abb. \ref{fig:instance_coverage_b=3_m_b} zu sehen
ist, bereits bei einem Zeitlimit von $20$ Minuten sämtliche Instanzen.
Die Heuristik löst sämtliche Instanzen mit einer durchschnittlichen Laufzeit von $1.01s$ und ist erneut
in keinem Fall vom Zeitlimit betroffen.

\begin{figure}[H]
\centering

\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=3_m_600s.png}
\caption{\textsc{Zeitlimit} $10min$}
\label{fig:instance_coverage_b=3_m_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=3_m_1200s.png}
\caption{\textsc{Zeitlimit} $20min$}
\label{fig:instance_coverage_b=3_m_b}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=3_m_1800s.png}
\caption{\textsc{Zeitlimit} $30min$}
\label{fig:instance_coverage_b=3_m_c}
\end{subfigure}
\caption{\textsc{Instance-Coverage der $b = 3$ Solver (m)}.}
\label{fig:instance_coverage_b=3_m}
\end{figure}

In Abb. \ref{fig:b=3_m_runtimes} ist zu sehen, dass sich die Laufzeiten der 3-Index-Formulierung bei jenen Instanzen,
welche von dieser gelöst werden, in der Regel nur marginal von denen der Bin-Packing-Formulierung unterscheiden,
wenngleich diese aufgrund der Instanzen $10$ und $13$ im Durchschnitt trotzdem um ca. $12 \%$ schneller ist.\newline
Die Heuristik ist, wie Abb. \ref{fig:b=3_m_runtimes} zu entnehmen, in Bezug auf die Laufzeit erneut konkurrenzlos.
Nach durchschnittlich $1.01s$ löst diese sämtliche Instanzen und unterbietet die MIP-Formulierungen damit um mehrere Größenordnungen.

Für die Kosten gilt, wie man Abb. \ref{fig:b=3_m_costs} entnehmen kann, dass die Ergebnisse der Heuristik,
wie in vorherigen Experimenten, eine Abweichung von den Optimalwerten der MIP-Formulierungen von durchschnittlich
unter einem Prozent besitzen. Beim dort betrachteten Zeitlimit von $10$ Minuten ermittelt die Heuristik mit den
Instanzen $00$ und $16$ in zwei Fällen sogar einen besseren Zielfunktionswert als die Bin-Packing-Formulierung,
obwohl diese dort auch zu zulässigen Lösungen gelangt.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_time_b=3_m_1800s.png}
\caption{\textsc{Laufzeiten ($30min$ Zeitlimit)}}
\label{fig:b=3_m_runtimes}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_cost_b=3_m_600s.png}
\caption{\textsc{Kosten ($10min$ Zeitlimit)}}
\label{fig:b=3_m_costs}
\end{subfigure}
\caption{\textsc{Ergebnisse}.}
\end{figure}

Abbildung \ref{fig:mip_results_b=3_m_a} bestätigt die Tendenz der Visualisierungen der Instance-Coverage und belegt,
dass beide MIP-Formulierungen bei einem Zeitlimit von $10$ Minuten nur wenige Instanzen überhaupt und noch weniger
Instanzen optimal lösen.
Bei einem Zeitlimit von $20$ Minuten hingegen, löst die Bin-Packing-Formulierung mit einer durchschnittlichen
Laufzeit von ca. $13$ Minuten pro Instanz sämtliche Instanzen optimal (vlg. Abb. \ref{fig:mip_results_b=3_m_b}).
Die 3-Index-Formulierung löst im Gegensatz dazu selbst bei einem Zeitlimit von $30$ Minuten nur $40 \%$ der Instanzen
optimal (vlg. Abb. \ref{fig:mip_results_b=3_m_c}), weshalb die Bin-Packing-Formulierung jener in dieser Kategorie erneut
klar vorzuziehen ist.

Die Heuristik benötigt durchschnittlich eine Laufzeit von nur $1.01s$ und weicht dabei um durchschnittlich
$0.35 \%$ vom Optimum ab. Wie bereits in der Kategorie der mittelgroßen Instanzen im $b = 2$ Szenario, ist dieser erhebliche
Laufzeitunterschied um mehrere Größenordnungen wohl hinreichende Motivation, um diese jeweils sehr geringe Abweichung
vom Optimum in Kauf zu nehmen.

\begin{figure}[H]
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{mygreen}{50 \%}$ & $ \textcolor{red}{25 \%}$ \\ \hline
    \textbf{Laufzeit} & \O $\thinspace \textcolor{red}{583.7 s}$ & \O $\thinspace \textcolor{mygreen}{554.4 s}$ \\ \hline
    \textbf{Abweichung} & \O $\thinspace \textcolor{red}{0.2 \%}$ & \O $\thinspace \textcolor{mygreen}{0.01 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $10min$}
\label{fig:mip_results_b=3_m_a}
\end{subfigure}
% $\quad\quad\quad\quad$
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{mygreen}{100 \%}$ & $ \textcolor{red}{40 \%}$ \\ \hline
    \textbf{Laufzeit} & \O $\thinspace \textcolor{mygreen}{791.4 s}$ & \O $\thinspace \textcolor{red}{795.4 s}$ \\ \hline
    \textbf{Abweichung} & \O $\thinspace 0.0 \%$ & \O $\thinspace 0.0 \%$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $20min$}
\label{fig:mip_results_b=3_m_b}
\end{subfigure}
% \end{figure}
% \begin{figure}[H]
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{mygreen}{100 \%}$ & $ \textcolor{red}{40 \%}$ \\ \hline
    \textbf{Laufzeit} & \O $\thinspace \textcolor{mygreen}{766.4 s}$ & \O $\thinspace \textcolor{red}{874.0 s}$ \\ \hline
    \textbf{Abweichung} & \O $\thinspace 0.0 \%$ & \O $\thinspace 0.0 \%$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $30min$}
\label{fig:mip_results_b=3_m_c}
\end{subfigure}

\caption{\textsc{MIP-Ergebnisse.}}
\label{fig:mip_results_b=3_m}
\end{figure}

\pagebreak

\textbf{Große Instanzen (l)}

Zuletzt werden die Solver für die in Kapitel \ref{sec:test_data} als groß bezeichneten Instanzen,
bei denen es darum geht, $500$ eintreffende Items in die Storage-Area zu verladen, verglichen.
In dieser Kategorie werden Zeitlimits von $45, 90$ und $120$ Minuten pro Instanz betrachtet.

Wie man Abb. \ref{fig:instance_coverage_b=3_l_a} entnehmen kann, findet keine der beiden MIP-Formulierungen
bei einem Zeitlimit von $45$ Minuten pro Instanz eine zulässige Lösung für eine der Instanzen.
Beim verdoppelten Zeitlimit von $90$ Minuten pro Instanz wird erkennbar, dass die Bin-Packing-Formulierung
mit einer Instance-Coverage von $80 \%$ auch in dieser Kategorie besser geeignet scheint, als die 3-Index-Formulierung,
welche selbst bei diesem Zeitlimit noch keine Instanz zulässig löst (vgl. \ref{fig:instance_coverage_b=3_l_b}).
Bei einem Zeitlimit von $120$ Minuten pro Instanz löst die Bin-Packing-Formulierung schließlich sämtliche Instanzen,
während die 3-Index-Formulierung sogar dort nur $20 \%$ der Instanzen löst (vgl. \ref{fig:instance_coverage_b=3_l_c}).

Die Heuristik ist mit einer durchschnittlichen Laufzeit von $3.9s$ pro Instanz erneut in keinem Fall
vom Zeitlimit betroffen.

\begin{figure}[H]
\centering

\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=3_l_2700s.png}
\caption{\textsc{Zeitlimit} $45min$}
\label{fig:instance_coverage_b=3_l_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=3_l_5400s.png}
\caption{\textsc{Zeitlimit} $90min$}
\label{fig:instance_coverage_b=3_l_b}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=1.2\textwidth]{img/solver_instance_coverage_b=3_l_7200s.png}
\caption{\textsc{Zeitlimit} $120min$}
\label{fig:instance_coverage_b=3_l_c}
\end{subfigure}
\caption{\textsc{Instance-Coverage der $b = 3$ Solver (l)}.}
\label{}
\end{figure}

Wie man Abb. \ref{fig:b=3_l_runtimes} entnehmen kann, ist die Bin-Packing-Formulierung in den wenigen
Fällen, in welchen die 3-Index-Formulierung zu einer zulässigen Lösung gelangt, stets schneller
als diese. Des Weiteren erkennt man, dass die Heuristik wie erwartet auch hier konkurrenzlos ist.

In Abb. \ref{fig:b=3_l_costs} ist zu erkennen, dass die Heuristik in einem der drei Fälle, in welchen die
3-Index-Formulierung zu einer zulässigen Lösung gelangt, einen besseren Zielfunktionswert als diese erzielt.
Außerdem ist zu sehen, dass die Abweichungen der Zielfunktionswerte der Heuristik von denen der Bin-Packing-Formulierung
erneut nur sehr gering sind.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_time_b=3_l_7200s.png}
\caption{\textsc{Laufzeiten}}
\label{fig:b=3_l_runtimes}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[width=1.3\textwidth]{img/solver_instance_cost_b=3_l_7200s.png}
\caption{\textsc{Kosten}}
\label{fig:b=3_l_costs}
\end{subfigure}
\caption{\textsc{Ergebnisse bei $120min$ Zeitlimit}.}
\label{fig:res_plots_b=3_l}
\end{figure}

Wie bereits erwähnt, ist die 3-Index-Formulierung in dieser Kategorie erneut der Bin-Packing-Formulierung
unterlegen, welche, wie man in Abb. \ref{fig:mip_results_b=3_l_c} erkennt, nach durchschnittlich
etwas mehr als einer Stunde sämtliche Instanzen optimal löst.

% TODO: ggf. später noch einmal durchführen
Die Verringerung der durchschnittlichen Laufzeit der Bin-Packing-Formulierung zwischen den betrachteten
Zeitlimits von $90$ und $120$ Minuten um mehrere Minuten ist überraschend und lässt sich vermutlich
lediglich durch unterschiedliche Auslastungen des Systems, auf welchem die Experimente durchgeführt wurden,
zu unterschiedlichen Zeiten erklären.

\begin{figure}[H]
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ 0 \%$ & $ 0 \%$ \\ \hline
    \textbf{Laufzeit} & $---$ & $---$ \\ \hline
    \textbf{Abweichung} & $---$ & $---$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $45min$}
\label{fig:mip_results_b=3_l_a}
\end{subfigure}
% $\quad\quad\quad\quad$
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{mygreen}{80 \%}$ & $ \textcolor{red}{0 \%}$ \\ \hline
    \textbf{Laufzeit} & $\textcolor{mygreen}{4511.5}$ & $\textcolor{red}{---}$ \\ \hline
    \textbf{Abweichung} & $\textcolor{mygreen}{0 \%}$ &$\textcolor{red}{---}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $90min$}
\label{fig:mip_results_b=3_l_b}
\end{subfigure}
% \end{figure}
% \begin{figure}[H]
\begin{subfigure}[b]{0.3\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ | l | l | l |}
    \hline
     & \textbf{BinP} & \textbf{3Idx} \\ \hline
    \textbf{Optimal} & $ \textcolor{mygreen}{100 \%}$ & $ \textcolor{red}{15 \%}$ \\ \hline
    \textbf{Laufzeit} & $\textcolor{mygreen}{4022.9}$ & $\textcolor{red}{6339.3}$ \\ \hline
    \textbf{Abweichung} & $\textcolor{mygreen}{0 \%}$ &$\textcolor{red}{0.13 \%}$ \\ \hline
\end{tabular}}
\caption{\textsc{Zeitlimit} $120min$}
\label{fig:mip_results_b=3_l_c}
\end{subfigure}
\caption{\textsc{MIP-Ergebnisse.}}
\label{}
\end{figure}

Die Heuristik löst mit einer durchschnittlichen Laufzeit von $3.9s$ pro Instanz sämtliche Instanzen
und weicht dabei im Durchschnitt nur um $0.17 \%$ vom Optimum ab.

\textbf{Fazit}\newline

Bei Stacking-Problemen mit Stacks der Kapazität $b = 3$ besteht in der Kategorie der kleinen Instanzen
aufgrund der relativ geringen Laufzeit der Bin-Packing-Formulierung nicht zwingend ein Bedarf für eine
Laufzeitverbesserung durch eine Heuristik. Nichtsdestotrotz ist die Heuristik bei einer nur geringen
durchschnittlichen Abweichung vom Optimum noch einmal erheblich schneller als diese.

In der Kategorie der mittelgroßen Instanzen ist die Bin-Packing-Formulierung der 3-Index-Formulierung sogar
sehr deutlich vorzuziehen. Da jedoch auch die Bin-Packing-Formulierung durschnittlich $13$ Minuten zur exakten Lösung benötigt,
ermöglicht die Heuristik hier mit einer durchschnittlichen Laufzeit von nur ca. einer Sekunde eine derart große Laufzeitersparnis,
dass die sehr geringe Abweichung von $0.35 \%$ in Kauf genommen werden sollte.

Noch deutlicher äußert sich dieser Trend in der Kategorie der großen Instanzen, in welcher die 3-Index-Formulierung
selbst bei einem Zeitlimit von zwei Stunden nur $20 \%$ der Instanzen löst. Die Bin-Packing-Formulierung, welche dort
mit einer durchschnittlichen Laufzeit von etwas mehr als einer Stunde sämtliche Instanzen optimal löst,
ist demnach bei einer Stack-Kapazität von $b = 3$ in jeder Kategorie der 3-Index-Formulierung vorzuziehen.
Die Heuristik benötigt hier durschnittlich $3.9s$ pro Instanz und weicht um durchschnittlich $0.17 \%$ vom Optimum ab.
Der daraus resultierende noch gravierendere Unterschied der Laufzeiten zwischen der Bin-Packing-Formulierung und
der Heuristik sorgt in Verbindung mit der sehr geringen Abweichung vom Optimum dafür, dass die Heuristik in der Praxis
auch in dieser Kategorie bevorzugt werden sollte.

Insgesamt ist festzuhalten, dass die Heuristik in jeder betrachteten Kategorie bei sehr geringen Abweichungen
vom Optimum deutlich kürzere Laufzeiten als die von \textsc{CPLEX} gelösten MIP-Formulierungen bietet und somit in
jedem Fall eine gute Alternative darstellt. Je größer die Instanzen werden, desto besser schneidet die Heuristik
verglichen mit den MIP-Formulierungen ab und sollte bereits ab der Kategorie der mittelgroßen Instanzen in der Regel
den MIP-Formulierungen vorgezogen werden.

% TODO: insert observation

An jener Stelle in der Heuristik, an welcher das Rating-System zum Einsatz kommt, um unterschiedlich sortierte
Listen der Item-Paare zu bilden, hätten zusätzlich zufällige \textquote{Shuffles} genutzt werden können,
um weitere Permutationen zu generieren. Besonders bei einer hohen Anzahl solcher Shuffles,
kann dabei mitunter ein besserer Zielfunktionswert erzielt werden. Dieser Ansatz wird jedoch aus zwei Gründen in
dieser Arbeit nicht verfolgt. Zunächst ist eine verhältnismäßig große Anzahl solcher Shuffles vonnöten um in
manchen Fällen zu bessren Ergebnissen zu gelangen, was eine erhebliche Verlängerung der Laufzeit zur Folge hätte.
Des Weiteren führt ein solcher stochastischer Ansatz dazu, dass für eine Instanz nicht mehr stets dasselbe Ergebnis
generiert wird. Da es sich bei den auf diese Weise erreichbaren Verbesserungen jedoch in der Regel nur um minimale
Verbesserungen handelt, die zum Preis einer extrem erhöhten Laufzeit kommen, wird dies nicht in Kauf genommen.
Stattdessen handelt es sich bei der präsentierten Heuristik, wie auch bei der Heuristik für $b = 2$ aus Abschnitt
\ref{sec:two_cap_heuristic}, um ein deterministisches Verfahren, welches bei denselben Rahmenbedingungen für eine Instanz
stets zum selben Ergebnis kommt.

\pagebreak

\section{Verbesserungsverfahren}
\label{sec:post_optimization}

Bei Verbesserungsverfahren handelt es sich um lokale Suchverfahren, welche bereits mit einer Lösung des Problems starten,
die z.B. durch ein konstruktives Eröffnungsverfahren, wie die Heuristiken aus den Kapiteln \ref{sec:two_cap_heuristic}
und \ref{sec:three_cap_heuristic}, bestimmt wurde und versuchen diese im Laufe des Verfahrens zu verbessern.
Dabei wird zwischen deterministischen Verfahren, die bei gleichen Startlösungen und gleichen Rahmenbedingungen stets zum gleichen Ergebnis kommen und stochastischen Verfahren, bei denen durch den Einsatz einer Zufallskomponente unterschiedliche Lösungen generiert werden, unterschieden \cite{Knust2017}.

Zunächst werden in Abschnitt \ref{sec:local_search} die Grundlagen zu lokalen Suchverfahren eingeführt.
Anschließend geht es in Abschnitt \ref{sec:tabu_search} um das allgemeine Verfahren einer Tabu-Suche,
bevor in Abschnitt \ref{sec:tabu_search_for_sp} konkret die entwickelte Tabu-Suche zur Lösung von Stacking-Problemen
präsentiert wird.

\subsection{Lokale Suchverfahren}
\label{sec:local_search}

Beim in dieser Arbeit betrachteten lokalen Suchverfahren handelt es sich um eine sogenannte Metaheuristik,
weil dieses nicht auf ein spezielles Problem abzielt, sondern auf grundsätzlichen Suchprinzipien beruht,
die auf eine Vielzahl von Optimierungsproblemen anwendbar sind. Aus diesem Grund erfreuen sich diese Heuristiken einer sehr großen Beliebtheit, sie sind allgemein anwendbar und sehr flexibel an konkrete Problemstellungen anpassbar.
Eine lokale Suche ist ein iterativer Prozess, bei welchem von einer Lösung zur nächsten, verbessernden Lösung vorangeschritten wird, bis ein bestimmtes Abbruchkriterium erfüllt ist. Um den Lösungsraum systematisch zu durchsuchen, wird die
sogenannte Nachbarschaft einer Lösung definiert, die der Menge der Lösungen entspricht, welche im nächsten Schritt
von der gegenwärtigen Lösung erreichbar sind \cite{Brucker2006}.
Die Idee eines lokalen Suchverfahrens ist es also, ausgehend von einer Initiallösung durch wohldefinierte Regeln zu anderen,
verbessernden Lösungen innerhalb der Nachbarschaft zu gelangen. Dabei wird häufig solange fortgeschritten, bis keine verbessernde Lösung in der Nachbarschaft gefunden wird, also bis ein lokales Minimum erreicht ist.
Der Erfolg dieses Ansatzes und somit die Lösungsqualität hängt primär von der definierten Struktur
der Nachbarschaft ab \cite{Pirlot1996}.
Die größte Schwäche dieses Ansatzes ist die Unfähigkeit, lokale Minima zu verlassen. Dieses Problem ist sehr
anschaulich in Abb. \ref{fig:local_search_weakness} visualisiert. Darin sind sämtliche Lösungen in der
Nachbarschaft $V(x_n)$ schlechter als die gegenwärtige Lösung $x_n$, obwohl weiter entfernt ein globales Minimum
existiert, welches aufgrund der Tatsache, dass ausschließlich verbessernde Lösungen gewählt werden,
nicht erreicht werden kann.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{img/local_minimum.png}
\caption{\textsc{Schwäche lokaler Suchverfahren. \cite{Pirlot1996}}}
\label{fig:local_search_weakness}
\end{figure}

\textbf{Formaler Ablauf einer lokalen Suche}

Eine lokale Suche startet mit einer beliebigen Lösung $x_1 \in X$ und in jedem Schritt $n$
wird eine neue Lösung $x_{n+1}$ aus der Nachbarschaft $V(x_n)$ der gegenwärtigen Lösung $x_n$ gewählt.
Für jedes $x \in X$ existiert eine solche Nachbarschaft $V(x) \subseteq X$.
Typischerweise entspricht das Kriterium, anhand welchem die nächste Lösung $x_{n+1}$ gewählt wird,
die beste Lösung in der Nachbarschaft von $x_n$ zu wählen, also eine Lösung $x_{n+1} \in V(x_n)$
mit $F(x_{n+1}) \leq F(x) \thinspace \forall \thinspace x \in V(x_n)$ bei Betrachtung eines
Minimierungsproblems \cite{Pirlot1996}.

Sollte keine verbessernde Lösung in der Nachbarschaft gefunden werden, so terminiert die Suche.
Diese Strategie wird \textquote{Steepest Descent} genannt, also Strategie des steilsten Abstiegs.
Dieser Name ergibt sich daraus, dass bei einem Minimierungsproblem in der Richtung des steilsten Abstiegs
von der Initiallösung vorangeschritten wird, bis keine Verbesserung mehr erzielt wird,
was zum bereits erwähnten Problem aus Abb. \ref{fig:local_search_weakness} führt.

\textbf{Beispiel}

Ein simples Beispiel für eine solche Nachbarschaft lässt sich anhand eines Bitvektors demonstrieren.
Wenn $X$ eine Menge von Bitvektoren ist und $x \in X$, dann kann eine Nachbarschaft $V(x)$ als die Menge
sämtlicher Lösungen $x \in X$ definiert werden, bei denen in $x$ ein einzelnes Bit invertiert wird \cite{Pirlot1996}.

Sei $x = (1, 0, 1, 0, 1, 0)$ eine als Bitvektor kodierte Lösung.
Die Nachbarschaft dieser Lösung ergibt sich gemäß der beschriebenen Struktur wie folgt:
\[
V(x) =
  \begin{bmatrix}
    \boldsymbol{0} & 0 & 1 & 0 & 1 & 0 \\
    1 & \boldsymbol{1} & 1 & 0 & 1 & 0 \\
    1 & 0 & \boldsymbol{0} & 0 & 1 & 0 \\
    1 & 0 & 1 & \boldsymbol{1} & 1 & 0 \\
    1 & 0 & 1 & 0 & \boldsymbol{0} & 0 \\
    1 & 0 & 1 & 0 & 1 & \boldsymbol{1}
  \end{bmatrix}
\]

\pagebreak

\subsection{Tabu-Suche}
\label{sec:tabu_search}

Die größte Schwäche des in Kapitel \ref{sec:local_search} vorgestellten allgemeinen lokalen Suchverfahrens
ist, dass die generierte Lösung nur einem lokalen Optimum entspricht und daher möglicherweise stark vom globalen
Optimum abweicht. Eine gute Qualität der Lösung ist somit nicht garantiert.
Eine Strategie um diesem Problem entgegenzutreten wäre beispielsweise, das Verfahren mehrfach mit unterschiedlichen
Initiallösungen zu starten und anschließend das beste generierte lokale Optimum zu wählen.
Ein weiterer Ansatz ist, Verschlechterungen des Zielfunktionswerts während des Suchprozesses zu erlauben.
Dies führt allerdings dazu, dass Lösungen mehrfach besucht werden können und somit Zyklen entstehen.
Daher müssen in solchen Ansätzen zusätzliche Strategien umgesetzt werden, die Zyklen vermeiden \cite{Brucker2006}.
Genau dort setzt das Verfahren der sogenannten Tabu-Suche an, welches zum Ziel hat, Situationen, wie jene aus
Abb. \ref{fig:local_search_weakness}, zu vermeiden.

Die Strategie, mit welcher lokale Optima verlassen werden können, ist die folgende.
Sogar wenn in der Nachbarschaft $V(x_n)$ keine bessere Lösung als die gegenwärtige Lösung $x_n$ existiert,
wird zur besten Lösung $x \in V(x_n)$ vorangeschritten.
Da die gesamte Nachbarschaft zu groß sein kann, um diese effizient zu durchsuchen,
wird dabei häufig nur ein Teil der Nachbarschaft $V'(x_n) \subseteq V(x_n)$ betrachtet \cite{Pirlot1996}.
Dieses Vorgehen führt dazu, dass es zu Zyklen kommen kann. Entspricht die gegenwärtige Lösung $x$,
so wird anschließend die beste Lösung $x_n \in V(x)$ gewählt. Daraufhin besteht die Chance,
dass die beste Lösung in $V(x_n)$ erneut $x$ ist. D.h. es entsteht ein Zyklus, in welchem wiederkehrend
zwischen den Lösungen $x$ und $x_n$ gewechselt wird.
Unter anderem zur Vermeidung solcher Situationen, wird eine der Tabu-Suche ihren Namen gebende Tabu-Liste verwendet,
welche bereits besuchte Lösungen speichert. Ist eine Lösung $x$ Teil der Tabu-Liste, so ist der Übergang
von $x_n$ zu $x$ verboten bzw. \textquote{tabu}.
In der Regel werden in der Tabu-Liste aus Effizienzgründen jedoch nicht vollständige Lösungen gespeichert, sondern
bestimmte Attribute der Lösungen oder auch die Operation, welche durchgeführt wurde, um zur jeweiligen Lösung zu gelangen. Im Fall des Bitvektor-Beispiels wäre es z.B. die Information, dass Bit $i$ von $0$ auf $1$ gesetzt wurde.
Die primäre Funktion der Tabu-Liste besteht allerdings darin, die Suche zu diversifizieren,
indem in zuvor undurchsuchten Regionen des \textquote{Search Space}\footnote{Menge, welche nach den zu findenden Objekten durchsucht werden soll.} gesucht wird und lokale Optima verlassen werden \cite{Pirlot1996}.

Der allgemeine Ablauf einer Tabu-Suche ist in Abb. \ref{fig:tabu_search_algo}
dargestellt. Zunächst wird eine initiale Lösung $s$ aus der Menge der zulässigen Lösungen $S$ generiert.
Anschließend wird die beste Lösung $s^*$ mit der Lösung $s$ initialisiert, ebenso wie der beste Zielfunktionswert
$c^*$, welcher mit dem Zielfunktionswert von $s$ initialisiert wird. Außerdem wird eine leere Tabu-Liste $TL$
angelegt. Daraufhin wird in einer Schleife jeweils die Nachbarschaft der gegenwärtigen Lösung $s$ durchsucht. Dazu wird eine
Kandidatenmenge $Cand(s)$ der Lösung $s$ generiert, bei welcher es sich um eine Teilmenge der Nachbarschaft von $s$
handelt, wobei eine zu $s$ benachbarte Lösung nur dann akzeptiert wird, wenn diese nicht Teil der Tabu-Liste ist,
oder diese ein Aspirationskriterium\footnote{Kriterium, welches bei Erfüllung zur Missachtung des Tabu-Status führt.} erfüllt. Aus dieser wird anhand einer Selektionsstrategie eine Nachbarlösung $s'$ gewählt und die Tabu-Liste wird aktualisiert, sodass sämtliche Lösungen, welche besucht werden, als zukünftig \textquote{tabu} gespeichert werden. In jeder Iteration wird die gegenwärtige Lösung $s$ entsprechend durch eine benachbarte Lösung $s'$ aktualisiert.
Sollte $s'$ einen besseren Zielfunktionswert besitzen als $s^*$, so wird die beste Lösung $s^*$
sowie der beste Zielfunktionswert $c^*$ aktualisiert. Dieser Prozess terminiert, sobald ein definiertes
Abbruckriterium erfüllt ist.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{img/tabu_search_algo_placeholder.png}
\caption{\textsc{Tabu-Suche. \cite{Knust2017}}}
\label{fig:tabu_search_algo}
\end{figure}

Die Qualität der von einer Tabu-Suche generierten Lösungen hängt stark von den vielen
Konfigurations- und Spezifikationsmöglichkeiten ab, welche bei der Entwicklung einer Tabu-Suche gegeben sind.
Die wohl wichtigste Spezifikation ist jene der Nachbarschaftsstruktur. Da in der Regel nicht die vollständige Nachbarschaft einer Lösung besucht wird, wird häufig eine feste Anzahl zufälliger Nachbarn innerhalb der Nachbarschaft besucht. Wie bereits erwähnt, werden zumeist nicht vollständige Lösungen in der Tabu-Liste
gespeichert, weshalb auch die Entscheidung darüber, welche Informationen dort gespeichert werden,
von Relevanz ist. Außerdem wird über ein Aspirationskriterium die Möglichkeit gegeben, dass in bestimmten Fällen
der Tabu-Status einer Lösung bzw. eines Übergangs zu einer anderen Lösung ignoriert wird,
z.B. wenn dieser zu einer bisher besten Lösung führt. Schließlich muss ein sinnvolles
Abbruchkriterium definiert werden, welches die Suche beendet. Im Folgenden wird die in dieser Arbeit entwickelte
Tabu-Suche zur Lösung von Stacking-Problemen vorgestellt.

\vfill
\pagebreak

\subsection{Tabu-Suche zur Lösung von Stacking-Problemen}
\label{sec:tabu_search_for_sp}

Im allgemeinen Tabu-Suche Algorithmus aus Abb. \ref{fig:tabu_search_algo} wird eine initiale Lösung generiert.
Die in dieser Arbeit entwickelte Tabu-Suche generiert die Initiallösung dagegen nicht, sondern bekommt eine
von den konstruktiven Heuristiken aus den Abschnitten \ref{sec:two_cap_heuristic}
und \ref{sec:three_cap_heuristic} generierte Lösung als Eingabe. Bevor die Nachbarschaftsstruktur etc. definiert werden kann, muss zunächst die Repräsentation einer solchen Lösung festgelegt werden, welche genau den Zuweisungen von Item-Tupeln zu Stacks, die von den konstruktiven Heuristiken generiert werden, entspricht. Es handelt sich daher um ein zweidimensionales Array, welches sämtliche Zellen der Storage-Area repräsentiert und entsprechend die zugewiesenen Item-Tupel enthält.\newline
Eine allgemeine Tabu-Suche besteht aus drei Hauptstrategien, welche von ihrem Erfinder \citet{Glover90} als Forbidding-Strategy,
Freeing-Strategy und Short-Term-Strategy bezeichnet werden. Diese werden im Folgenden anhand der in dieser Arbeit entwickelten Tabu-Suche erläutert.

Zunächst zur betrachteten Nachbarschaftsstruktur. Um Nachbarlösungen einer gegebenen Lösung zu generieren,
wird eine kombinierte \textquote{Shift}- und \textquote{Swap}-Nachbarschaft verwendet.
Jedes Item $i \in I$ einer gegebenen Lösung befindet sich in einem Stack $q \in Q$.
Eine \textquote{Shift}-Operation verschiebt das Item $i$ in einen Stack $r \in Q$ mit $r \neq q$.
Das Item-Tupel $(i_k, ..., i_1)$, welches sich in Stack $r$ befindet, muss nach der Zuweisung des Items $i$
weiterhin zulässig sein. Dementsprechend muss die Stack-Kapazität durch $k \leq b$ eingehalten werden.
Außerdem müssen sowohl die Stacking-Constraints mit $s_{i_{\lambda + 1}i_\lambda} = 1 \thinspace \forall \thinspace
\lambda = 1, ..., k - 1$, als auch die Placement-Constraints mit $t_{i_\lambda q} = 1 \thinspace \forall
\thinspace i_\lambda$ respektiert werden.
Ein \textquote{Swap} entspricht einer Tausch-Operation zwischen zwei unterschiedlichen Items $i, j \in I$,
welche sich in unterschiedlichen Stacks $q, r \in Q$ befinden, d.h. $q \neq r$, $i \in q$ und $j \in r$.
Sind beide Items in Bezug auf die Placement- und Stacking-Constraints kompatibel zum jeweils anderen Stack,
so wird Item $i$ an der Stelle von Item $j$ in Stack $r$ platziert und umgekehrt.

\textbf{Forbidding-Strategy}

Die Forbidding-Strategy spezifiziert die Art der Einträge, also die Art der gespeicherten Information in der
Tabu-Liste. Wie bereits erwähnt, werden aus Effizienzgründen typischerweise nicht die Lösungen selbst gespeichert,
sondern bestimmte Attribute der Lösungen oder Operatoren, welche zur Generierung der Lösung geführt haben.
Dabei ist es wichtig, Attribute bzw. Informationen zu speichern, welche eine Lösung möglichst genau identifizieren,
um zu verhindern, dass weitere Lösungen mit ähnlichen Eigenschaften ausgeschlossen werden.

Die entwickelte Tabu-Suche verwaltet aufgrund der kombinierten Shift- und Swap-Nachbarschaft zwei unterschiedliche
Tabu-Listen. Die Shift-Tabu-Liste speichert durchgeführte Shift-Operationen, also eine Kombination aus einem Item und einer Position innerhalb eines Stacks in der Storage-Area, an welche dieses verschoben wird. Eine Shift-Operation entspricht daher dem folgenden $2$-Tupel.
\begin{gather*}
shift := (item, shiftPos)
\end{gather*}
Anschließend kann ein Item nicht erneut an dieselbe Position verschoben werden, sehr wohl jedoch an eine andere.
In der Swap-Tabu-Liste werden entsprechend Swap-Operationen gespeichert. Dabei handelt es sich um eine Kombination aus zwei Positionen innerhalb der Stacks in der Storage-Area verknüpft mit den Items, welche sie beinhalten.
D.h. je zwei Positionen tauschen lediglich einmal ihren Inhalt, solange sich dieser nicht verändert.
Swap-Operationen sind durch das folgende $4$-Tupel definiert.
\begin{gather*}
swap := (pos_1, pos_2, item_1, item_2)
\end{gather*}
Eine Position zeichnet sich sowohl im Shift- als auch im Swap-Operator durch ein $2$-Tupel bestehend aus einem
Stack-Index und einem Level innerhalb dieses Stacks aus.
\begin{gather*}
 pos = (stackIdx, level)
\end{gather*}

\textbf{Freeing-Strategy}

Die Freeing-Strategy entspricht jener Strategie, welche spezifiziert, zu welchem Zeitpunkt Einträge aus den
Tabu-Listen wieder entfernt werden. Typischerweise wird eine Maximallänge der Tabu-Liste definiert,
welche dafür sorgt, dass diese nicht beliebig lang wird und effizient durchsuchbar und speicherbar bleibt.
Somit werden die Einträge der Tabu-Liste nur für eine gewisse Zeit verboten, denn bei Erreichen der Maximallänge
wird der jeweils älteste Eintrag gelöscht, um den aktuellen Eintrag aufzunehmen.
Daher sind lediglich Zyklen, welche kleiner oder gleich der Maximallänge der Tabu-Liste sind,
ausgeschlossen, was bei der Definition dieser Länge berücksichtigt werden sollte.

\textbf{Short-Term-Strategy}

Die Short-Term-Strategy gibt im Wesentlichen an, wie zulässige Nachbarlösungen ausgewählt werden.
Eine Operation, welche eine Nachbarlösung generiert, ist dann zulässig, wenn diese nicht tabu ist,
oder, wenn sie das Aspirationskriterium erfüllt. Das Aspirationskriterium hat die Funktion bestimmte
Operationen zu akzeptieren, obwohl diese tabu sind, sofern das Kriterium erfüllt wird.
In der entwickelten Tabu-Suche enspricht dieses Kriterium der Generierung einer bisher besten Lösung.
Es wäre nicht sinnvoll, eine solche Lösung zu verbieten.\newline
In der Regel wird nicht nur eine Nachbarlösung generiert, sondern entweder sämtliche Nachbarlösungen,
sofern dies effizient möglich ist, oder, wie in diesem Fall, eine Teilmenge der Nachbarlösungen,
welche eine definierte, feste Kardinalität besitzt. Es muss also jeweils eine Nachbarlösung aus dieser
Teilmenge ausgewählt werden, mit welcher die Tabu-Suche fortgesetzt wird.
Für die Selektion der Nachbarlösungen wurden zwei Strategien implementiert, welche im Folgenden vorgestellt
werden.

\textbf{First-Fit}

Bei der First-Fit-Strategie wird die Nachbarschaft einer Lösung lediglich solange durchsucht, bis eine bessere
Lösung gefunden wird, also eine Lösung, welche einen besseren Zielfunktionswert besitzt, als die gegenwärtige Lösung.
Diese wird direkt ausgewählt und die Nachbarschaft wird nicht weiter durchsucht. Sollte keine bessere Lösung gefunden werden,
so wird die beste Nachbarlösung aus der Teilmenge ausgewählt.

\textbf{Best-Fit}

Die Best-Fit-Strategie generiert die gesamte Teilmenge fester Kardinalität und liefert schließlich die
beste Nachbarlösung aus dieser Teilmenge.\newline

Als Abbruchkriterien stehen drei verschiedene Optionen bereit, welche über entsprechende Parameter konfiguriert werden können.
Die simpelste Variante besteht darin, eine feste Anzahl an Iterationen zu definieren, nach welcher die Tabu-Suche terminiert.
Die zweite Variante beendet die Tabu-Suche nach einer gewissen Anzahl an Zurücksetzungen der Tabu-Listen. Wenn innerhalb
der Short-Term-Strategy eine definierte Anzahl von Nachbarlösungen hintereinander allesamt tabu sind, so wird die
entsprechende Tabu-Liste zurückgesetzt. In dieser Variante kann die Tabu-Suche nach einer bestimmten Anzahl solcher Zurücksetzungen terminiert werden. Bei der letzten Variante handelt es sich um ein gängiges Abbruchkriterium, welches häufig zu guten Ergebnissen führt und die Tabu-Suche nach einer bestimmten Anzahl nichtverbessernder Iterationen terminiert.

Eine wichtige Eigenschaft einer Nachbarschaft ist der sogenannte Zusammenhang. Eine Nachbarschaft wird als zusammenhängend bezeichnet, wenn von einer Lösung $s$ durch Anwendung einer endlichen Folge von Operatoren jede andere Lösung $s'$ erreicht
werden kann \cite{Brucker2006}. Bei einer zusammenhängenden Nachbarschaft existieren Konvergenzaussagen für Tabu-Suchen.
Auch praktische Experimente haben gezeigt, dass lokale Suchverfahren im Allgemeinen bessere Ergebnisse
liefern, wenn die betrachteten Nachbarschaften mindestens opt-zusammenhängend\footnote{Analog zum Zusammenhang, allerdings müssen nur sämtliche optimalen Lösungen erreichbar sein.} sind \cite{Knust2017}.
Dementsprechend ist ein solcher Zusammenhang bei der Nachbarschaftsstruktur erstrebenswert.
Die zuvor beschriebene kombinierte Shift- und Swap-Nachbarschaft ist allerdings noch nicht zusammenhängend,
was an folgendem Gegenbeispiel nachvollzogen werden kann.

Es sei der Graph aus Abb. \ref{fig:counter_example_stacking_graph} für die Items $I = \{1, 2, 3, 4, 5, 6\}$ gegeben,
dessen transitiver Abschluss dem Stacking-Graphen der Item-Menge $I$ entspricht. Außerdem seien sämtliche
Items mit den Stacks $S_0$ und $S_1$ kompatibel.

\begin{figure}[H]
  \centering
    \begin{tikzpicture}[->, scale=0.7, transform shape, node distance=2cm]
    \node[state] (A) {1};
    \node[state] (B) [below of=A] {2};
    \node[state] (C) [below of=B] {3};
    \node[state] (D) [right of=A] {4};
    \node[state] (E) [right of=B] {5};
    \node[state] (F) [right of=C] {6};

    \path (A) edge node {} (B)
          (B) edge node {} (C)

          (D) edge node {} (E)
          (E) edge node {} (F)

          (B) edge node {} (F)
          (E) edge node {} (C);
  \end{tikzpicture}
  \caption{\textsc{Stacking-Graph}.}
  \label{fig:counter_example_stacking_graph}
\end{figure}

In Abb. \ref{fig:counter_example_stacks} sind zwei zulässige Lösungen dargestellt. Dementsprechend muss
sich die Lösung $s$ aus Abb. \ref{fig:counter_a} in einer endlichen Anzahl von Schritten in die Lösung $s'$ aus
Abb. \ref{fig:counter_b} transformieren lassen, wenn die Nachbarschaft zusammenhängend ist.

Da lediglich zulässige Nachbarlösungen betrachtet werden und beide Stacks vollständig gefüllt sind,
sind Shift-Operationen nicht gestattet, weil sonst die Stack-Kapazität verletzt würde.
Die Transformation muss daher durch Anwendung einer endliche Anzahl von Swap-Operationen auf $s$ gelingen.
Dabei können zwei Items innerhalb eines Stacks basierend auf dem Graphen aus Abb. \ref{fig:counter_example_stacking_graph}  nicht vertauscht werden ohne die Stacking-Constraints zu verletzen.
Demzufolge können nur Items zwischen den beiden Stacks vertauscht werden. Es gibt also initial drei mögliche
Swap-Operationen: $swap_1 = ((0, 2), (1, 2), 1, 4), swap_2 = ((0, 1), (1, 1), 2, 5)$ und $swap_3 = ((0, 0), (1, 0), 3, 6)$.
Wird $swap_1$ durchgeführt, so wird Item $4$ auf Item $2$ platziert, was gegen die Stacking-Constraints vertößt.
Somit ist die erste Swap-Operation nicht zielführend. Die zweite Swap-Operation $swap_2$ sorgt ebenfalls dafür,
dass sich Item $4$ oberhalb von Item $2$ befindet und ist somit gleichermaßen unzulässig.
Die letzte Swap-Operation $swap_3$ führt zunächst zu einer zulässigen Zwischenlösung mit den Stack-Zuweisungen
$S_0 = (1, 2, 6)$ und $S_1 = (4, 5, 3)$. Diese entspricht allerdings noch nicht der Ziellösung $s'$,
weshalb anschließend die Swap-Operation $((0, 2), (1, 2), 1, 4)$ oder $((0, 1), (1, 1), 2, 5)$
durchgeführt werden muss, welche beide zu unzulässigen Lösungen führen. Demnach ist Lösung $s'$ nicht durch Anwendung einer endlichen Folge von Operationen innerhalb der definierten Nachbarschaft von Lösung $s$ erreichbar, was bedeutet, dass die Nachbarschaft nicht zusammenhängend ist.

\begin{figure}[H]
  \begin{subfigure}[b]{0.5\textwidth}
  \centering
  \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{c|c|c|}
    \cline{2-3}
    $\boldsymbol{L_2}$ & $1$ & $4$ \\ \cline{2-3}
    $\boldsymbol{L_1}$ & $2$ & $5$ \\ \cline{2-3}
    $\boldsymbol{L_0}$ & $3$ & $6$ \\ \cline{2-3}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\boldsymbol{S_0}$} & \multicolumn{1}{c}{$\boldsymbol{S_1}$} \\
    \end{tabular}}
    \caption{\textsc{Lösung $s$}}
    \label{fig:counter_a}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
  \centering
  \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{c|c|c|}
    \cline{2-3}
    $\boldsymbol{L_2}$ & $4$ & $1$ \\ \cline{2-3}
    $\boldsymbol{L_1}$ & $5$ & $2$ \\ \cline{2-3}
    $\boldsymbol{L_0}$ & $3$ & $6$ \\ \cline{2-3}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\boldsymbol{S_0}$} & \multicolumn{1}{c}{$\boldsymbol{S_1}$} \\
    \end{tabular}}
    \caption{\textsc{Lösung $s'$}}
    \label{fig:counter_b}
  \end{subfigure}
  \caption{\textsc{Gegenbeispiel - Nachbarschaft nicht zusammenhängend.}}
  \label{fig:counter_example_stacks}
\end{figure}

Da sämtliche Lösungen, welche basierend auf der definierten Nachbarschaftsstruktur nicht ineinander überführbar sind,
analog zum Beispiel aus Abb. \ref{fig:counter_example_stacks} aufgebaut sind und einen oder mehrere unzulässige
Zwischenschritte erfordern, um zur zulässigen Ziellösung zu gelangen, muss die Nachbarschaft dahingehend erweitert werden,
dass solche Situationen aufgelöst werden können.
Dazu wird die Liste der Operatoren um einen Mehrfach-Swap erweitert, welcher mehrere simultane Swap-Operationen
ermöglicht. Es können maximal $b - 1$ Swaps durchgeführt werden, denn sonst würde der gesamte Stack-Inhalt
vertauscht. Dies ist nicht sinnvoll, denn wenn sich auf diese Weise eine günstigere Zuweisung erzielen ließe,
wäre diese bereits Teil des Matchings in der konstruktiven Heuristik gewesen. Da in dieser Arbeit lediglich
Stacking-Probleme mit Stacks der Kapazitäten $b = 2$ und $b = 3$ betrachtet werden, wird höchstens ein $2$-Swap
benötigt. Bei $b = 2$ ist sogar bereits die einfache Swap-Operation ausreichend.

Mithilfe des $2$-Swaps lässt sich das Beispiel aus Abb. \ref{fig:counter_example_stacks} lösen,
indem auf Lösung $s$ gleichzeitig die zuvor eingeführten Swap-Operationen $swap_1$ und $swap_2$ durchgeführt werden.
Anschließend ergibt sich die zulässige Lösung $s'$.

Letztlich ist die Nachbarschaft einer Lösung in der entwickelten Tabu-Suche also durch die drei Operatoren
Shift, Swap und $2$-Swap definiert, wobei $2$-Swap lediglich bei $b = 3$ zum Einsatz kommt.
Bei dieser Nachbarschaft handelt es sich um eine zusammenhängende Nachbarschaft,
weil jede zulässige Lösung von jeder anderen zulässigen Lösung durch eine endliche Folge von Anwendungen
dieser Operatoren erreicht werden kann.\newline

\textbf{Konfiguration der Tabu-Suche}\newline

% TODO: Complete Section

Die Tabu-Suche bietet viele Konfigurationsmöglichkeiten, welche zu qualitativ unterschiedlichen
Ergebnissen führen. In diesem Abschnitt wird die Konfiguration der Tabu-Suche vorgestellt,
mit welcher die im Folgenden vorgestellten Experimente durchgeführt wurden.

%  und zumeist experimentell bestimmt wurden.
% Zunächst muss zwischen First-Fit und Best-Fit gewählt werden. First-Fit sollte in Betracht gezogen werden,
% wenn möglichst geringe Laufzeiten angestrebt werden. Ist dagegen die Qualität der Lösung wichtiger,
% so sollte Best-Fit bevorzugt werden. Als Abbruchkriterium wird in den Experimenten die Anzahl nichtverbessernder
% Iterationen betrachtet.

\vfill

\pagebreak

\section{Fazit}
\label{sec:conclusion}
